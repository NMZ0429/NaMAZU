
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <meta name="viewport" content="width=device-width,initial-scale=1">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <meta name="lang:clipboard.copy" content="Copy to clipboard">
  <meta name="lang:clipboard.copied" content="Copied to clipboard">
  <meta name="lang:search.language" content="en">
  <meta name="lang:search.pipeline.stopwords" content="True">
  <meta name="lang:search.pipeline.trimmer" content="True">
  <meta name="lang:search.result.none" content="No matching documents">
  <meta name="lang:search.result.one" content="1 matching document">
  <meta name="lang:search.result.other" content="# matching documents">
  <meta name="lang:search.tokenizer" content="[\s\-]+">

  
    <link href="https://fonts.gstatic.com/" rel="preconnect" crossorigin>
    <link href="https://fonts.googleapis.com/css?family=Roboto+Mono:400,500,700|Roboto:300,400,400i,700&display=fallback" rel="stylesheet">

    <style>
      body,
      input {
        font-family: "Roboto", "Helvetica Neue", Helvetica, Arial, sans-serif
      }

      code,
      kbd,
      pre {
        font-family: "Roboto Mono", "Courier New", Courier, monospace
      }
    </style>
  

  <link rel="stylesheet" href="_static/stylesheets/application.css"/>
  <link rel="stylesheet" href="_static/stylesheets/application-palette.css"/>
  <link rel="stylesheet" href="_static/stylesheets/application-fixes.css"/>
  
  <link rel="stylesheet" href="_static/fonts/material-icons.css"/>
  
  <meta name="theme-color" content="#3f51b5">
  <script src="_static/javascripts/modernizr.js"></script>
  
  
  
    <title>NaMAZU.lightning_wingman package &#8212; NaMAZU 0.0.58 documentation</title>
    <link rel="stylesheet" type="text/css" href="_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="_static/material.css" />
    <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/doctools.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
  
   

  </head>
  <body dir=ltr
        data-md-color-primary=blue data-md-color-accent=light-blue>
  
  <svg class="md-svg">
    <defs data-children-count="0">
      
      <svg xmlns="http://www.w3.org/2000/svg" width="416" height="448" viewBox="0 0 416 448" id="__github"><path fill="currentColor" d="M160 304q0 10-3.125 20.5t-10.75 19T128 352t-18.125-8.5-10.75-19T96 304t3.125-20.5 10.75-19T128 256t18.125 8.5 10.75 19T160 304zm160 0q0 10-3.125 20.5t-10.75 19T288 352t-18.125-8.5-10.75-19T256 304t3.125-20.5 10.75-19T288 256t18.125 8.5 10.75 19T320 304zm40 0q0-30-17.25-51T296 232q-10.25 0-48.75 5.25Q229.5 240 208 240t-39.25-2.75Q130.75 232 120 232q-29.5 0-46.75 21T56 304q0 22 8 38.375t20.25 25.75 30.5 15 35 7.375 37.25 1.75h42q20.5 0 37.25-1.75t35-7.375 30.5-15 20.25-25.75T360 304zm56-44q0 51.75-15.25 82.75-9.5 19.25-26.375 33.25t-35.25 21.5-42.5 11.875-42.875 5.5T212 416q-19.5 0-35.5-.75t-36.875-3.125-38.125-7.5-34.25-12.875T37 371.5t-21.5-28.75Q0 312 0 260q0-59.25 34-99-6.75-20.5-6.75-42.5 0-29 12.75-54.5 27 0 47.5 9.875t47.25 30.875Q171.5 96 212 96q37 0 70 8 26.25-20.5 46.75-30.25T376 64q12.75 25.5 12.75 54.5 0 21.75-6.75 42 34 40 34 99.5z"/></svg>
      
    </defs>
  </svg>
  
  <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer">
  <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search">
  <label class="md-overlay" data-md-component="overlay" for="__drawer"></label>
  <a href="#NaMAZU.lightning_wingman" tabindex="1" class="md-skip"> Skip to content </a>
  <header class="md-header" data-md-component="header">
  <nav class="md-header-nav md-grid">
    <div class="md-flex navheader">
      <div class="md-flex__cell md-flex__cell--shrink">
        <a href="index.html" title="NaMAZU 0.0.58 documentation"
           class="md-header-nav__button md-logo">
          
            &nbsp;
          
        </a>
      </div>
      <div class="md-flex__cell md-flex__cell--shrink">
        <label class="md-icon md-icon--menu md-header-nav__button" for="__drawer"></label>
      </div>
      <div class="md-flex__cell md-flex__cell--stretch">
        <div class="md-flex__ellipsis md-header-nav__title" data-md-component="title">
          <span class="md-header-nav__topic">NaMAZU</span>
          <span class="md-header-nav__topic"> NaMAZU.lightning_wingman package </span>
        </div>
      </div>
      <div class="md-flex__cell md-flex__cell--shrink">
        <label class="md-icon md-icon--search md-header-nav__button" for="__search"></label>
        
<div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" action="search.html" method="get" name="search">
      <input type="text" class="md-search__input" name="q" placeholder="Search"
             autocapitalize="off" autocomplete="off" spellcheck="false"
             data-md-component="query" data-md-state="active">
      <label class="md-icon md-search__icon" for="__search"></label>
      <button type="reset" class="md-icon md-search__icon" data-md-component="reset" tabindex="-1">
        &#xE5CD;
      </button>
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" data-md-scrollfix>
        <div class="md-search-result" data-md-component="result">
          <div class="md-search-result__meta">
            Type to start searching
          </div>
          <ol class="md-search-result__list"></ol>
        </div>
      </div>
    </div>
  </div>
</div>

      </div>
      
        <div class="md-flex__cell md-flex__cell--shrink">
          <div class="md-header-nav__source">
            <a href="https://github.com/NMZ0429/NaMAZU" title="Go to repository" class="md-source" data-md-source="github">

    <div class="md-source__icon">
      <svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" viewBox="0 0 24 24" width="28" height="28">
        <use xlink:href="#__github" width="24" height="24"></use>
      </svg>
    </div>
  
  <div class="md-source__repository">
    Project
  </div>
</a>
          </div>
        </div>
      
      
  
  <script src="_static/javascripts/version_dropdown.js"></script>
  <script>
    var json_loc = ""versions.json"",
        target_loc = "../",
        text = "Versions";
    $( document ).ready( add_version_dropdown(json_loc, target_loc, text));
  </script>
  

    </div>
  </nav>
</header>

  
  <div class="md-container">
    
    
    
  <nav class="md-tabs" data-md-component="tabs">
    <div class="md-tabs__inner md-grid">
      <ul class="md-tabs__list">
          <li class="md-tabs__item"><a href="index.html" class="md-tabs__link">NaMAZU 0.0.58 documentation</a></li>
      </ul>
    </div>
  </nav>
    <main class="md-main">
      <div class="md-main__inner md-grid" data-md-component="container">
        
          <div class="md-sidebar md-sidebar--primary" data-md-component="navigation">
            <div class="md-sidebar__scrollwrap">
              <div class="md-sidebar__inner">
                <nav class="md-nav md-nav--primary" data-md-level="0">
  <label class="md-nav__title md-nav__title--site" for="__drawer">
    <a href="index.html" title="NaMAZU 0.0.58 documentation" class="md-nav__button md-logo">
      
        <img src="_static/" alt=" logo" width="48" height="48">
      
    </a>
    <a href="index.html"
       title="NaMAZU 0.0.58 documentation">NaMAZU</a>
  </label>
    <div class="md-nav__source">
      <a href="https://github.com/NMZ0429/NaMAZU" title="Go to repository" class="md-source" data-md-source="github">

    <div class="md-source__icon">
      <svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" viewBox="0 0 24 24" width="28" height="28">
        <use xlink:href="#__github" width="24" height="24"></use>
      </svg>
    </div>
  
  <div class="md-source__repository">
    Project
  </div>
</a>
    </div>
  
  

</nav>
              </div>
            </div>
          </div>
          <div class="md-sidebar md-sidebar--secondary" data-md-component="toc">
            <div class="md-sidebar__scrollwrap">
              <div class="md-sidebar__inner">
                
<nav class="md-nav md-nav--secondary">
    <label class="md-nav__title" for="__toc">Contents</label>
  <ul class="md-nav__list" data-md-scrollfix="">
        <li class="md-nav__item"><a href="#namazu-lightning-wingman--page-root" class="md-nav__link">NaMAZU.lightning_wingman package</a><nav class="md-nav">
              <ul class="md-nav__list">
        <li class="md-nav__item"><a href="#submodules" class="md-nav__link">Submodules</a>
        </li>
        <li class="md-nav__item"><a href="#module-NaMAZU.lightning_wingman.ani_label" class="md-nav__link">NaMAZU.lightning_wingman.ani_label module</a>
        </li>
        <li class="md-nav__item"><a href="#module-NaMAZU.lightning_wingman.aninet" class="md-nav__link">NaMAZU.lightning_wingman.aninet module</a>
        </li>
        <li class="md-nav__item"><a href="#module-NaMAZU.lightning_wingman.inference_helper" class="md-nav__link">NaMAZU.lightning_wingman.inference_helper module</a>
        </li>
        <li class="md-nav__item"><a href="#module-NaMAZU.lightning_wingman.instance" class="md-nav__link">NaMAZU.lightning_wingman.instance module</a>
        </li>
        <li class="md-nav__item"><a href="#module-NaMAZU.lightning_wingman.lit_aninet" class="md-nav__link">NaMAZU.lightning_wingman.lit_aninet module</a>
        </li>
        <li class="md-nav__item"><a href="#module-NaMAZU.lightning_wingman.lit_multimodal" class="md-nav__link">NaMAZU.lightning_wingman.lit_multimodal module</a>
        </li>
        <li class="md-nav__item"><a href="#module-NaMAZU.lightning_wingman.lit_u2net" class="md-nav__link">NaMAZU.lightning_wingman.lit_u2net module</a>
        </li>
        <li class="md-nav__item"><a href="#module-NaMAZU.lightning_wingman.lit_video_clf" class="md-nav__link">NaMAZU.lightning_wingman.lit_video_clf module</a>
        </li>
        <li class="md-nav__item"><a href="#module-NaMAZU.lightning_wingman.multi_modal" class="md-nav__link">NaMAZU.lightning_wingman.multi_modal module</a>
        </li>
        <li class="md-nav__item"><a href="#module-NaMAZU.lightning_wingman.pretrainer" class="md-nav__link">NaMAZU.lightning_wingman.pretrainer module</a>
        </li>
        <li class="md-nav__item"><a href="#module-NaMAZU.lightning_wingman.torch_gmm" class="md-nav__link">NaMAZU.lightning_wingman.torch_gmm module</a>
        </li>
        <li class="md-nav__item"><a href="#module-NaMAZU.lightning_wingman.torch_knn" class="md-nav__link">NaMAZU.lightning_wingman.torch_knn module</a>
        </li>
        <li class="md-nav__item"><a href="#module-NaMAZU.lightning_wingman.torch_nbc" class="md-nav__link">NaMAZU.lightning_wingman.torch_nbc module</a>
        </li>
        <li class="md-nav__item"><a href="#module-NaMAZU.lightning_wingman.u2net" class="md-nav__link">NaMAZU.lightning_wingman.u2net module</a>
        </li>
        <li class="md-nav__item"><a href="#module-NaMAZU.lightning_wingman.video_clf" class="md-nav__link">NaMAZU.lightning_wingman.video_clf module</a>
        </li>
        <li class="md-nav__item"><a href="#module-NaMAZU.lightning_wingman" class="md-nav__link">Module contents</a>
        </li></ul>
            </nav>
        </li>
    
<li class="md-nav__item"><a class="md-nav__extra_link" href="_sources/NaMAZU.lightning_wingman.rst.txt">Show Source</a> </li>

<li id="searchbox" class="md-nav__item"></li>

  </ul>
</nav>
              </div>
            </div>
          </div>
        
        <div class="md-content">
          <article class="md-content__inner md-typeset" role="main">
            
  
<h1 id="namazu-lightning-wingman--page-root">NaMAZU.lightning_wingman package<a class="headerlink" href="#namazu-lightning-wingman--page-root" title="Permalink to this headline">¶</a></h1>

<h2 id="submodules">Submodules<a class="headerlink" href="#submodules" title="Permalink to this headline">¶</a></h2>


<span id="namazu-lightning-wingman-ani-label-module"></span><h2 id="module-NaMAZU.lightning_wingman.ani_label">NaMAZU.lightning_wingman.ani_label module<a class="headerlink" href="#module-NaMAZU.lightning_wingman.ani_label" title="Permalink to this headline">¶</a></h2>


<span id="namazu-lightning-wingman-aninet-module"></span><h2 id="module-NaMAZU.lightning_wingman.aninet">NaMAZU.lightning_wingman.aninet module<a class="headerlink" href="#module-NaMAZU.lightning_wingman.aninet" title="Permalink to this headline">¶</a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="NaMAZU.lightning_wingman.aninet.AdaptiveConcatPool2d">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">NaMAZU.lightning_wingman.aninet.</span></span><span class="sig-name descname"><span class="pre">AdaptiveConcatPool2d</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">sz</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/NaMAZU/lightning_wingman/aninet.html#AdaptiveConcatPool2d"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#NaMAZU.lightning_wingman.aninet.AdaptiveConcatPool2d" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.module.Module</span></code></p>
<p>Layer that concats <cite>AdaptiveAvgPool2d</cite> and <cite>AdaptiveMaxPool2d</cite>.
Source: Fastai. This code was taken from the fastai library at url
<a class="reference external" href="https://github.com/fastai/fastai/blob/master/fastai/layers.py#L176">https://github.com/fastai/fastai/blob/master/fastai/layers.py#L176</a></p>
<dl class="py method">
<dt class="sig sig-object py" id="NaMAZU.lightning_wingman.aninet.AdaptiveConcatPool2d.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/NaMAZU/lightning_wingman/aninet.html#AdaptiveConcatPool2d.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#NaMAZU.lightning_wingman.aninet.AdaptiveConcatPool2d.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>
<dl class="py attribute">
<dt class="sig sig-object py" id="NaMAZU.lightning_wingman.aninet.AdaptiveConcatPool2d.training">
<span class="sig-name descname"><span class="pre">training</span></span><em class="property"><span class="pre">:</span> <span class="pre">bool</span></em><a class="headerlink" href="#NaMAZU.lightning_wingman.aninet.AdaptiveConcatPool2d.training" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>
</dd></dl>
<dl class="py class">
<dt class="sig sig-object py" id="NaMAZU.lightning_wingman.aninet.Flatten">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">NaMAZU.lightning_wingman.aninet.</span></span><span class="sig-name descname"><span class="pre">Flatten</span></span><a class="reference internal" href="_modules/NaMAZU/lightning_wingman/aninet.html#Flatten"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#NaMAZU.lightning_wingman.aninet.Flatten" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.module.Module</span></code></p>
<p>Flatten <cite>x</cite> to a single dimension. Adapted from fastai’s Flatten() layer,
at <a class="reference external" href="https://github.com/fastai/fastai/blob/master/fastai/layers.py#L25">https://github.com/fastai/fastai/blob/master/fastai/layers.py#L25</a></p>
<dl class="py method">
<dt class="sig sig-object py" id="NaMAZU.lightning_wingman.aninet.Flatten.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/NaMAZU/lightning_wingman/aninet.html#Flatten.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#NaMAZU.lightning_wingman.aninet.Flatten.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>
<dl class="py attribute">
<dt class="sig sig-object py" id="NaMAZU.lightning_wingman.aninet.Flatten.training">
<span class="sig-name descname"><span class="pre">training</span></span><em class="property"><span class="pre">:</span> <span class="pre">bool</span></em><a class="headerlink" href="#NaMAZU.lightning_wingman.aninet.Flatten.training" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>
</dd></dl>
<dl class="py function">
<dt class="sig sig-object py" id="NaMAZU.lightning_wingman.aninet.aninet18">
<span class="sig-prename descclassname"><span class="pre">NaMAZU.lightning_wingman.aninet.</span></span><span class="sig-name descname"><span class="pre">aninet18</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">pretrained</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">progress</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">top_n</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">100</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/NaMAZU/lightning_wingman/aninet.html#aninet18"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#NaMAZU.lightning_wingman.aninet.aninet18" title="Permalink to this definition">¶</a></dt>
<dd><p>Resnet18 model trained on the Danbooru2018 dataset tags</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>pretrained</strong> (<em>bool</em>) – kwargs, load pretrained weights into the model</p></li>
<li><p><strong>top_n</strong> (<em>int</em>) – kwargs, pick to load the model for predicting the top <cite>n</cite> tags.
currently only supports top_n=100.</p></li>
</ul>
</dd>
</dl>
</dd></dl>
<dl class="py function">
<dt class="sig sig-object py" id="NaMAZU.lightning_wingman.aninet.aninet34">
<span class="sig-prename descclassname"><span class="pre">NaMAZU.lightning_wingman.aninet.</span></span><span class="sig-name descname"><span class="pre">aninet34</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">pretrained</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">progress</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">top_n</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">500</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/NaMAZU/lightning_wingman/aninet.html#aninet34"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#NaMAZU.lightning_wingman.aninet.aninet34" title="Permalink to this definition">¶</a></dt>
<dd><p>Resnet34 model trained on the Danbooru2018 dataset tags</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>pretrained</strong> (<em>bool</em>) – kwargs, load pretrained weights into the model</p></li>
<li><p><strong>top_n</strong> (<em>int</em>) – kwargs, pick to load the model for predicting the top <cite>n</cite> tags,
currently only supports top_n=500.</p></li>
</ul>
</dd>
</dl>
</dd></dl>
<dl class="py function">
<dt class="sig sig-object py" id="NaMAZU.lightning_wingman.aninet.aninet50">
<span class="sig-prename descclassname"><span class="pre">NaMAZU.lightning_wingman.aninet.</span></span><span class="sig-name descname"><span class="pre">aninet50</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">pretrained</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">progress</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">top_n</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">6000</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/NaMAZU/lightning_wingman/aninet.html#aninet50"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#NaMAZU.lightning_wingman.aninet.aninet50" title="Permalink to this definition">¶</a></dt>
<dd><p>Resnet50 model trained on the full Danbooru2018 dataset’s top 6000 tags</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>pretrained</strong> (<em>bool</em>) – kwargs, load pretrained weights into the model.</p></li>
<li><p><strong>top_n</strong> (<em>int</em>) – kwargs, pick to load the model for predicting the top <cite>n</cite> tags,
currently only supports top_n=6000.</p></li>
</ul>
</dd>
</dl>
</dd></dl>
<dl class="py function">
<dt class="sig sig-object py" id="NaMAZU.lightning_wingman.aninet.bn_drop_lin">
<span class="sig-prename descclassname"><span class="pre">NaMAZU.lightning_wingman.aninet.</span></span><span class="sig-name descname"><span class="pre">bn_drop_lin</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">n_in</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_out</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">bn</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">bool</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">p</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">float</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">0.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">actn</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/NaMAZU/lightning_wingman/aninet.html#bn_drop_lin"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#NaMAZU.lightning_wingman.aninet.bn_drop_lin" title="Permalink to this definition">¶</a></dt>
<dd><p>Sequence of batchnorm (if <cite>bn</cite>), dropout (with <cite>p</cite>) and linear (<cite>n_in</cite>,`n_out`) layers followed by <cite>actn</cite>.
Adapted from Fastai at <a class="reference external" href="https://github.com/fastai/fastai/blob/master/fastai/layers.py#L44">https://github.com/fastai/fastai/blob/master/fastai/layers.py#L44</a></p>
</dd></dl>
<dl class="py function">
<dt class="sig sig-object py" id="NaMAZU.lightning_wingman.aninet.create_head">
<span class="sig-prename descclassname"><span class="pre">NaMAZU.lightning_wingman.aninet.</span></span><span class="sig-name descname"><span class="pre">create_head</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">top_n_tags</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">nf</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">ps</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.5</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/NaMAZU/lightning_wingman/aninet.html#create_head"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#NaMAZU.lightning_wingman.aninet.create_head" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>


<span id="namazu-lightning-wingman-inference-helper-module"></span><h2 id="module-NaMAZU.lightning_wingman.inference_helper">NaMAZU.lightning_wingman.inference_helper module<a class="headerlink" href="#module-NaMAZU.lightning_wingman.inference_helper" title="Permalink to this headline">¶</a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="NaMAZU.lightning_wingman.inference_helper.PredictionAssistant">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">NaMAZU.lightning_wingman.inference_helper.</span></span><span class="sig-name descname"><span class="pre">PredictionAssistant</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">models</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span> </span><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">pytorch_lightning.core.lightning.LightningModule</span><span class="p"><span class="pre">,</span> </span><span class="pre">str</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/NaMAZU/lightning_wingman/inference_helper.html#PredictionAssistant"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#NaMAZU.lightning_wingman.inference_helper.PredictionAssistant" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">abc.ABC</span></code></p>
<dl class="py method">
<dt class="sig sig-object py" id="NaMAZU.lightning_wingman.inference_helper.PredictionAssistant.predict">
<em class="property"><span class="pre">abstract</span> </em><span class="sig-name descname"><span class="pre">predict</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">query</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><a class="reference internal" href="#NaMAZU.lightning_wingman.instance.Query" title="NaMAZU.lightning_wingman.instance.Query"><span class="pre">NaMAZU.lightning_wingman.instance.Query</span></a></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">→</span> <span class="sig-return-typehint"><a class="reference internal" href="#NaMAZU.lightning_wingman.instance.Result" title="NaMAZU.lightning_wingman.instance.Result"><span class="pre">NaMAZU.lightning_wingman.instance.Result</span></a></span></span><a class="reference internal" href="_modules/NaMAZU/lightning_wingman/inference_helper.html#PredictionAssistant.predict"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#NaMAZU.lightning_wingman.inference_helper.PredictionAssistant.predict" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>
</dd></dl>


<span id="namazu-lightning-wingman-instance-module"></span><h2 id="module-NaMAZU.lightning_wingman.instance">NaMAZU.lightning_wingman.instance module<a class="headerlink" href="#module-NaMAZU.lightning_wingman.instance" title="Permalink to this headline">¶</a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="NaMAZU.lightning_wingman.instance.Query">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">NaMAZU.lightning_wingman.instance.</span></span><span class="sig-name descname"><span class="pre">Query</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">_Query__query</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span> </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/NaMAZU/lightning_wingman/instance.html#Query"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#NaMAZU.lightning_wingman.instance.Query" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></p>
</dd></dl>
<dl class="py class">
<dt class="sig sig-object py" id="NaMAZU.lightning_wingman.instance.Result">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">NaMAZU.lightning_wingman.instance.</span></span><span class="sig-name descname"><span class="pre">Result</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">_Result__results</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span> </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/NaMAZU/lightning_wingman/instance.html#Result"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#NaMAZU.lightning_wingman.instance.Result" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></p>
</dd></dl>


<span id="namazu-lightning-wingman-lit-aninet-module"></span><h2 id="module-NaMAZU.lightning_wingman.lit_aninet">NaMAZU.lightning_wingman.lit_aninet module<a class="headerlink" href="#module-NaMAZU.lightning_wingman.lit_aninet" title="Permalink to this headline">¶</a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="NaMAZU.lightning_wingman.lit_aninet.AniNet">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">NaMAZU.lightning_wingman.lit_aninet.</span></span><span class="sig-name descname"><span class="pre">AniNet</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">choice</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">str</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">'50'</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/NaMAZU/lightning_wingman/lit_aninet.html#AniNet"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#NaMAZU.lightning_wingman.lit_aninet.AniNet" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">pytorch_lightning.core.lightning.LightningModule</span></code></p>
<dl class="py method">
<dt class="sig sig-object py" id="NaMAZU.lightning_wingman.lit_aninet.AniNet.calc_result">
<span class="sig-name descname"><span class="pre">calc_result</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">probs</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">torch.Tensor</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">thresh</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.3</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">→</span> <span class="sig-return-typehint"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span></span></span><a class="reference internal" href="_modules/NaMAZU/lightning_wingman/lit_aninet.html#AniNet.calc_result"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#NaMAZU.lightning_wingman.lit_aninet.AniNet.calc_result" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>
<dl class="py method">
<dt class="sig sig-object py" id="NaMAZU.lightning_wingman.lit_aninet.AniNet.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">torch.Tensor</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">→</span> <span class="sig-return-typehint"><span class="pre">torch.Tensor</span></span></span><a class="reference internal" href="_modules/NaMAZU/lightning_wingman/lit_aninet.html#AniNet.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#NaMAZU.lightning_wingman.lit_aninet.AniNet.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Same as <code class="xref py py-meth docutils literal notranslate"><span class="pre">torch.nn.Module.forward()</span></code>, however in Lightning you want this to define
the operations you want to use for prediction (i.e.: on a server or as a feature extractor).</p>
<p>Normally you’d call <code class="docutils literal notranslate"><span class="pre">self()</span></code> from your <code class="xref py py-meth docutils literal notranslate"><span class="pre">training_step()</span></code> method.
This makes it easy to write a complex system for training with the outputs
you’d want in a prediction setting.</p>
<p>You may also find the <code class="xref py py-func docutils literal notranslate"><span class="pre">auto_move_data()</span></code> decorator useful
when using the module outside Lightning in a production setting.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>*args</strong> – Whatever you decide to pass into the forward method.</p></li>
<li><p><strong>**kwargs</strong> – Keyword arguments are also possible.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Predicted output</p>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># example if we were using this model as a feature extractor</span>
<span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
    <span class="n">feature_maps</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">convnet</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">feature_maps</span>

<span class="k">def</span> <span class="nf">training_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">):</span>
    <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">batch</span>
    <span class="n">feature_maps</span> <span class="o">=</span> <span class="bp">self</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">logits</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">classifier</span><span class="p">(</span><span class="n">feature_maps</span><span class="p">)</span>

    <span class="c1"># ...</span>
    <span class="k">return</span> <span class="n">loss</span>

<span class="c1"># splitting it this way allows model to be used a feature extractor</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">MyModelAbove</span><span class="p">()</span>

<span class="n">inputs</span> <span class="o">=</span> <span class="n">server</span><span class="o">.</span><span class="n">get_request</span><span class="p">()</span>
<span class="n">results</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
<span class="n">server</span><span class="o">.</span><span class="n">write_results</span><span class="p">(</span><span class="n">results</span><span class="p">)</span>

<span class="c1"># -------------</span>
<span class="c1"># This is in stark contrast to torch.nn.Module where normally you would have this:</span>
<span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">):</span>
    <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">batch</span>
    <span class="n">feature_maps</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">convnet</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">logits</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">classifier</span><span class="p">(</span><span class="n">feature_maps</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">logits</span>
</pre></div>
</div>
</dd></dl>
<dl class="py method">
<dt class="sig sig-object py" id="NaMAZU.lightning_wingman.lit_aninet.AniNet.predict">
<span class="sig-name descname"><span class="pre">predict</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">image_path</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span> </span><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">→</span> <span class="sig-return-typehint"><span class="pre">torch.Tensor</span></span></span><a class="reference internal" href="_modules/NaMAZU/lightning_wingman/lit_aninet.html#AniNet.predict"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#NaMAZU.lightning_wingman.lit_aninet.AniNet.predict" title="Permalink to this definition">¶</a></dt>
<dd><p>Use this function with trainer.predict(…). Override if you need to add any processing logic.</p>
</dd></dl>
<dl class="py method">
<dt class="sig sig-object py" id="NaMAZU.lightning_wingman.lit_aninet.AniNet.predict_probs">
<span class="sig-name descname"><span class="pre">predict_probs</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">outputs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/NaMAZU/lightning_wingman/lit_aninet.html#AniNet.predict_probs"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#NaMAZU.lightning_wingman.lit_aninet.AniNet.predict_probs" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>
<dl class="py method">
<dt class="sig sig-object py" id="NaMAZU.lightning_wingman.lit_aninet.AniNet.run_batch_prediction">
<span class="sig-name descname"><span class="pre">run_batch_prediction</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">image_dir</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">str</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/NaMAZU/lightning_wingman/lit_aninet.html#AniNet.run_batch_prediction"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#NaMAZU.lightning_wingman.lit_aninet.AniNet.run_batch_prediction" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>
<dl class="py attribute">
<dt class="sig sig-object py" id="NaMAZU.lightning_wingman.lit_aninet.AniNet.training">
<span class="sig-name descname"><span class="pre">training</span></span><em class="property"><span class="pre">:</span> <span class="pre">bool</span></em><a class="headerlink" href="#NaMAZU.lightning_wingman.lit_aninet.AniNet.training" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>
</dd></dl>


<span id="namazu-lightning-wingman-lit-multimodal-module"></span><h2 id="module-NaMAZU.lightning_wingman.lit_multimodal">NaMAZU.lightning_wingman.lit_multimodal module<a class="headerlink" href="#module-NaMAZU.lightning_wingman.lit_multimodal" title="Permalink to this headline">¶</a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="NaMAZU.lightning_wingman.lit_multimodal.MultiModalNet">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">NaMAZU.lightning_wingman.lit_multimodal.</span></span><span class="sig-name descname"><span class="pre">MultiModalNet</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">modality_dimensions</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">modality_encoders</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">torch.nn.modules.module.Module</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">latent_dim</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">int</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">512</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">direct_fusion</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">bool</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_classes</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">int</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">is_regression</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">bool</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">train_encoders</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">bool</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">use_modality_dropout</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">bool</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">md_prob</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">float</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">0.5</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">use_stochastic_fusion</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">bool</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/NaMAZU/lightning_wingman/lit_multimodal.html#MultiModalNet"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#NaMAZU.lightning_wingman.lit_multimodal.MultiModalNet" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">pytorch_lightning.core.lightning.LightningModule</span></code></p>
<dl class="py method">
<dt class="sig sig-object py" id="NaMAZU.lightning_wingman.lit_multimodal.MultiModalNet.configure_optimizers">
<span class="sig-name descname"><span class="pre">configure_optimizers</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">→</span> <span class="sig-return-typehint"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span> </span><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">torch.optim.adam.Adam</span><span class="p"><span class="pre">,</span> </span><span class="pre">torch.optim.lr_scheduler.StepLR</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></span><a class="reference internal" href="_modules/NaMAZU/lightning_wingman/lit_multimodal.html#MultiModalNet.configure_optimizers"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#NaMAZU.lightning_wingman.lit_multimodal.MultiModalNet.configure_optimizers" title="Permalink to this definition">¶</a></dt>
<dd><p>Choose what optimizers and learning-rate schedulers to use in your optimization.
Normally you’d need one. But in the case of GANs or similar you might have multiple.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p><p>Any of these 6 options.</p>
<ul class="simple">
<li><p>Single optimizer.</p></li>
<li><p>List or Tuple - List of optimizers.</p></li>
<li><p>Two lists - The first list has multiple optimizers, the second a list of LR schedulers (or lr_dict).</p></li>
<li><p>Dictionary, with an ‘optimizer’ key, and (optionally) a ‘lr_scheduler’
key whose value is a single LR scheduler or lr_dict.</p></li>
<li><p>Tuple of dictionaries as described, with an optional ‘frequency’ key.</p></li>
<li><p>None - Fit will run without any optimizer.</p></li>
</ul>
</p>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The ‘frequency’ value is an int corresponding to the number of sequential batches
optimized with the specific optimizer. It should be given to none or to all of the optimizers.
There is a difference between passing multiple optimizers in a list,
and passing multiple optimizers in dictionaries with a frequency of 1:
In the former case, all optimizers will operate on the given batch in each optimization step.
In the latter, only one optimizer will operate on the given batch at every step.</p>
<p>The lr_dict is a dictionary which contains the scheduler and its associated configuration.
The default configuration is shown below.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="p">{</span>
    <span class="s1">'scheduler'</span><span class="p">:</span> <span class="n">lr_scheduler</span><span class="p">,</span> <span class="c1"># The LR scheduler instance (required)</span>
    <span class="s1">'interval'</span><span class="p">:</span> <span class="s1">'epoch'</span><span class="p">,</span> <span class="c1"># The unit of the scheduler's step size</span>
    <span class="s1">'frequency'</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span> <span class="c1"># The frequency of the scheduler</span>
    <span class="s1">'reduce_on_plateau'</span><span class="p">:</span> <span class="kc">False</span><span class="p">,</span> <span class="c1"># For ReduceLROnPlateau scheduler</span>
    <span class="s1">'monitor'</span><span class="p">:</span> <span class="s1">'val_loss'</span><span class="p">,</span> <span class="c1"># Metric for ReduceLROnPlateau to monitor</span>
    <span class="s1">'strict'</span><span class="p">:</span> <span class="kc">True</span><span class="p">,</span> <span class="c1"># Whether to crash the training if `monitor` is not found</span>
    <span class="s1">'name'</span><span class="p">:</span> <span class="kc">None</span><span class="p">,</span> <span class="c1"># Custom name for LearningRateMonitor to use</span>
<span class="p">}</span>
</pre></div>
</div>
<p>Only the <code class="docutils literal notranslate"><span class="pre">scheduler</span></code> key is required, the rest will be set to the defaults above.</p>
</div>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># most cases</span>
<span class="k">def</span> <span class="nf">configure_optimizers</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="n">opt</span> <span class="o">=</span> <span class="n">Adam</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">1e-3</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">opt</span>

<span class="c1"># multiple optimizer case (e.g.: GAN)</span>
<span class="k">def</span> <span class="nf">configure_optimizers</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="n">generator_opt</span> <span class="o">=</span> <span class="n">Adam</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model_gen</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span>
    <span class="n">disriminator_opt</span> <span class="o">=</span> <span class="n">Adam</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model_disc</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.02</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">generator_opt</span><span class="p">,</span> <span class="n">disriminator_opt</span>

<span class="c1"># example with learning rate schedulers</span>
<span class="k">def</span> <span class="nf">configure_optimizers</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="n">generator_opt</span> <span class="o">=</span> <span class="n">Adam</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model_gen</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span>
    <span class="n">disriminator_opt</span> <span class="o">=</span> <span class="n">Adam</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model_disc</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.02</span><span class="p">)</span>
    <span class="n">discriminator_sched</span> <span class="o">=</span> <span class="n">CosineAnnealing</span><span class="p">(</span><span class="n">discriminator_opt</span><span class="p">,</span> <span class="n">T_max</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
    <span class="k">return</span> <span class="p">[</span><span class="n">generator_opt</span><span class="p">,</span> <span class="n">disriminator_opt</span><span class="p">],</span> <span class="p">[</span><span class="n">discriminator_sched</span><span class="p">]</span>

<span class="c1"># example with step-based learning rate schedulers</span>
<span class="k">def</span> <span class="nf">configure_optimizers</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="n">gen_opt</span> <span class="o">=</span> <span class="n">Adam</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model_gen</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span>
    <span class="n">dis_opt</span> <span class="o">=</span> <span class="n">Adam</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model_disc</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.02</span><span class="p">)</span>
    <span class="n">gen_sched</span> <span class="o">=</span> <span class="p">{</span><span class="s1">'scheduler'</span><span class="p">:</span> <span class="n">ExponentialLR</span><span class="p">(</span><span class="n">gen_opt</span><span class="p">,</span> <span class="mf">0.99</span><span class="p">),</span>
                 <span class="s1">'interval'</span><span class="p">:</span> <span class="s1">'step'</span><span class="p">}</span>  <span class="c1"># called after each training step</span>
    <span class="n">dis_sched</span> <span class="o">=</span> <span class="n">CosineAnnealing</span><span class="p">(</span><span class="n">discriminator_opt</span><span class="p">,</span> <span class="n">T_max</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span> <span class="c1"># called every epoch</span>
    <span class="k">return</span> <span class="p">[</span><span class="n">gen_opt</span><span class="p">,</span> <span class="n">dis_opt</span><span class="p">],</span> <span class="p">[</span><span class="n">gen_sched</span><span class="p">,</span> <span class="n">dis_sched</span><span class="p">]</span>

<span class="c1"># example with optimizer frequencies</span>
<span class="c1"># see training procedure in `Improved Training of Wasserstein GANs`, Algorithm 1</span>
<span class="c1"># https://arxiv.org/abs/1704.00028</span>
<span class="k">def</span> <span class="nf">configure_optimizers</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="n">gen_opt</span> <span class="o">=</span> <span class="n">Adam</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model_gen</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span>
    <span class="n">dis_opt</span> <span class="o">=</span> <span class="n">Adam</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model_disc</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.02</span><span class="p">)</span>
    <span class="n">n_critic</span> <span class="o">=</span> <span class="mi">5</span>
    <span class="k">return</span> <span class="p">(</span>
        <span class="p">{</span><span class="s1">'optimizer'</span><span class="p">:</span> <span class="n">dis_opt</span><span class="p">,</span> <span class="s1">'frequency'</span><span class="p">:</span> <span class="n">n_critic</span><span class="p">},</span>
        <span class="p">{</span><span class="s1">'optimizer'</span><span class="p">:</span> <span class="n">gen_opt</span><span class="p">,</span> <span class="s1">'frequency'</span><span class="p">:</span> <span class="mi">1</span><span class="p">}</span>
    <span class="p">)</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Some things to know:</p>
<ul>
<li><p>Lightning calls <code class="docutils literal notranslate"><span class="pre">.backward()</span></code> and <code class="docutils literal notranslate"><span class="pre">.step()</span></code> on each optimizer
and learning rate scheduler as needed.</p></li>
<li><p>If you use 16-bit precision (<code class="docutils literal notranslate"><span class="pre">precision=16</span></code>), Lightning will automatically
handle the optimizers for you.</p></li>
<li><p>If you use multiple optimizers, <a class="reference internal" href="#NaMAZU.lightning_wingman.lit_multimodal.MultiModalNet.training_step" title="NaMAZU.lightning_wingman.lit_multimodal.MultiModalNet.training_step"><code class="xref py py-meth docutils literal notranslate"><span class="pre">training_step()</span></code></a> will have an additional
<code class="docutils literal notranslate"><span class="pre">optimizer_idx</span></code> parameter.</p></li>
<li><p>If you use LBFGS Lightning handles the closure function automatically for you.</p></li>
<li><p>If you use multiple optimizers, gradients will be calculated only
for the parameters of current optimizer at each training step.</p></li>
<li><p>If you need to control how often those optimizers step or override the
default <code class="docutils literal notranslate"><span class="pre">.step()</span></code> schedule, override the <code class="xref py py-meth docutils literal notranslate"><span class="pre">optimizer_step()</span></code> hook.</p></li>
<li><p>If you only want to call a learning rate scheduler every <code class="docutils literal notranslate"><span class="pre">x</span></code> step or epoch,
or want to monitor a custom metric, you can specify these in a lr_dict:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="p">{</span>
    <span class="s1">'scheduler'</span><span class="p">:</span> <span class="n">lr_scheduler</span><span class="p">,</span>
    <span class="s1">'interval'</span><span class="p">:</span> <span class="s1">'step'</span><span class="p">,</span>  <span class="c1"># or 'epoch'</span>
    <span class="s1">'monitor'</span><span class="p">:</span> <span class="s1">'val_f1'</span><span class="p">,</span>
    <span class="s1">'frequency'</span><span class="p">:</span> <span class="n">x</span><span class="p">,</span>
<span class="p">}</span>
</pre></div>
</div>
</li>
</ul>
</div>
</dd></dl>
<dl class="py method">
<dt class="sig sig-object py" id="NaMAZU.lightning_wingman.lit_multimodal.MultiModalNet.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x_list</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">torch.Tensor</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">ma</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">prior</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">→</span> <span class="sig-return-typehint"><span class="pre">torch.Tensor</span></span></span><a class="reference internal" href="_modules/NaMAZU/lightning_wingman/lit_multimodal.html#MultiModalNet.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#NaMAZU.lightning_wingman.lit_multimodal.MultiModalNet.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Same as <code class="xref py py-meth docutils literal notranslate"><span class="pre">torch.nn.Module.forward()</span></code>, however in Lightning you want this to define
the operations you want to use for prediction (i.e.: on a server or as a feature extractor).</p>
<p>Normally you’d call <code class="docutils literal notranslate"><span class="pre">self()</span></code> from your <a class="reference internal" href="#NaMAZU.lightning_wingman.lit_multimodal.MultiModalNet.training_step" title="NaMAZU.lightning_wingman.lit_multimodal.MultiModalNet.training_step"><code class="xref py py-meth docutils literal notranslate"><span class="pre">training_step()</span></code></a> method.
This makes it easy to write a complex system for training with the outputs
you’d want in a prediction setting.</p>
<p>You may also find the <code class="xref py py-func docutils literal notranslate"><span class="pre">auto_move_data()</span></code> decorator useful
when using the module outside Lightning in a production setting.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>*args</strong> – Whatever you decide to pass into the forward method.</p></li>
<li><p><strong>**kwargs</strong> – Keyword arguments are also possible.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Predicted output</p>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># example if we were using this model as a feature extractor</span>
<span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
    <span class="n">feature_maps</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">convnet</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">feature_maps</span>

<span class="k">def</span> <span class="nf">training_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">):</span>
    <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">batch</span>
    <span class="n">feature_maps</span> <span class="o">=</span> <span class="bp">self</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">logits</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">classifier</span><span class="p">(</span><span class="n">feature_maps</span><span class="p">)</span>

    <span class="c1"># ...</span>
    <span class="k">return</span> <span class="n">loss</span>

<span class="c1"># splitting it this way allows model to be used a feature extractor</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">MyModelAbove</span><span class="p">()</span>

<span class="n">inputs</span> <span class="o">=</span> <span class="n">server</span><span class="o">.</span><span class="n">get_request</span><span class="p">()</span>
<span class="n">results</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
<span class="n">server</span><span class="o">.</span><span class="n">write_results</span><span class="p">(</span><span class="n">results</span><span class="p">)</span>

<span class="c1"># -------------</span>
<span class="c1"># This is in stark contrast to torch.nn.Module where normally you would have this:</span>
<span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">):</span>
    <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">batch</span>
    <span class="n">feature_maps</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">convnet</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">logits</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">classifier</span><span class="p">(</span><span class="n">feature_maps</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">logits</span>
</pre></div>
</div>
</dd></dl>
<dl class="py method">
<dt class="sig sig-object py" id="NaMAZU.lightning_wingman.lit_multimodal.MultiModalNet.modality_dropout">
<span class="sig-name descname"><span class="pre">modality_dropout</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">batch_size</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_modalities</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">int</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">→</span> <span class="sig-return-typehint"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">torch.Tensor</span><span class="p"><span class="pre">]</span></span></span></span><a class="reference internal" href="_modules/NaMAZU/lightning_wingman/lit_multimodal.html#MultiModalNet.modality_dropout"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#NaMAZU.lightning_wingman.lit_multimodal.MultiModalNet.modality_dropout" title="Permalink to this definition">¶</a></dt>
<dd><p>Return a tensor of shape (batch_size, num_modalities) whose values are either 0 or 1.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>batch_size</strong> (<em>int</em>) – The batch size.</p></li>
<li><p><strong>num_modalities</strong> (<em>int</em>) – Number of modalities.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Dropped modalities or None if the dropout is disabled.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>Union[Tensor, None]</p>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt class="sig sig-object py" id="NaMAZU.lightning_wingman.lit_multimodal.MultiModalNet.stochstic_fusion">
<span class="sig-name descname"><span class="pre">stochstic_fusion</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">batch_size</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">int</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">→</span> <span class="sig-return-typehint"><span class="pre">torch.Tensor</span></span></span><a class="reference internal" href="_modules/NaMAZU/lightning_wingman/lit_multimodal.html#MultiModalNet.stochstic_fusion"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#NaMAZU.lightning_wingman.lit_multimodal.MultiModalNet.stochstic_fusion" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>
<dl class="py attribute">
<dt class="sig sig-object py" id="NaMAZU.lightning_wingman.lit_multimodal.MultiModalNet.training">
<span class="sig-name descname"><span class="pre">training</span></span><em class="property"><span class="pre">:</span> <span class="pre">bool</span></em><a class="headerlink" href="#NaMAZU.lightning_wingman.lit_multimodal.MultiModalNet.training" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>
<dl class="py method">
<dt class="sig sig-object py" id="NaMAZU.lightning_wingman.lit_multimodal.MultiModalNet.training_step">
<span class="sig-name descname"><span class="pre">training_step</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">batch</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/NaMAZU/lightning_wingman/lit_multimodal.html#MultiModalNet.training_step"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#NaMAZU.lightning_wingman.lit_multimodal.MultiModalNet.training_step" title="Permalink to this definition">¶</a></dt>
<dd><p>Here you compute and return the training loss and some additional metrics for e.g.
the progress bar or logger.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>batch</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code> | (<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>, …) | [<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>, …]) – The output of your <code class="xref py py-class docutils literal notranslate"><span class="pre">DataLoader</span></code>. A tensor, tuple or list.</p></li>
<li><p><strong>batch_idx</strong> (<em>int</em>) – Integer displaying index of this batch</p></li>
<li><p><strong>optimizer_idx</strong> (<em>int</em>) – When using multiple optimizers, this argument will also be present.</p></li>
<li><p><strong>hiddens</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>) – Passed in if
<a href="#id1"><span class="problematic" id="id2">:paramref:`~pytorch_lightning.trainer.trainer.Trainer.truncated_bptt_steps`</span></a> &gt; 0.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><p>Any of.</p>
<ul class="simple">
<li><p><code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code> - The loss tensor</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">dict</span></code> - A dictionary. Can include any keys, but must include the key <code class="docutils literal notranslate"><span class="pre">'loss'</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">None</span></code> - Training will skip to the next batch</p></li>
</ul>
</p>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Returning <code class="docutils literal notranslate"><span class="pre">None</span></code> is currently not supported for multi-GPU or TPU, or with 16-bit precision enabled.</p>
</div>
<p>In this step you’d normally do the forward pass and calculate the loss for a batch.
You can also do fancier things like multiple forward passes or something model specific.</p>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">training_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">):</span>
    <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">z</span> <span class="o">=</span> <span class="n">batch</span>
    <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">loss</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">loss</span>
</pre></div>
</div>
<p>If you define multiple optimizers, this step will be called with an additional
<code class="docutils literal notranslate"><span class="pre">optimizer_idx</span></code> parameter.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Multiple optimizers (e.g.: GANs)</span>
<span class="k">def</span> <span class="nf">training_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">,</span> <span class="n">optimizer_idx</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">optimizer_idx</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="c1"># do training_step with encoder</span>
    <span class="k">if</span> <span class="n">optimizer_idx</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
        <span class="c1"># do training_step with decoder</span>
</pre></div>
</div>
<p>If you add truncated back propagation through time you will also get an additional
argument with the hidden states of the previous step.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Truncated back-propagation through time</span>
<span class="k">def</span> <span class="nf">training_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">,</span> <span class="n">hiddens</span><span class="p">):</span>
    <span class="c1"># hiddens are the hidden states from the previous truncated backprop step</span>
    <span class="o">...</span>
    <span class="n">out</span><span class="p">,</span> <span class="n">hiddens</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">lstm</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">hiddens</span><span class="p">)</span>
    <span class="o">...</span>
    <span class="k">return</span> <span class="p">{</span><span class="s1">'loss'</span><span class="p">:</span> <span class="n">loss</span><span class="p">,</span> <span class="s1">'hiddens'</span><span class="p">:</span> <span class="n">hiddens</span><span class="p">}</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The loss value shown in the progress bar is smoothed (averaged) over the last values,
so it differs from the actual loss returned in train/validation step.</p>
</div>
</dd></dl>
</dd></dl>


<span id="namazu-lightning-wingman-lit-u2net-module"></span><h2 id="module-NaMAZU.lightning_wingman.lit_u2net">NaMAZU.lightning_wingman.lit_u2net module<a class="headerlink" href="#module-NaMAZU.lightning_wingman.lit_u2net" title="Permalink to this headline">¶</a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="NaMAZU.lightning_wingman.lit_u2net.LitU2Net">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">NaMAZU.lightning_wingman.lit_u2net.</span></span><span class="sig-name descname"><span class="pre">LitU2Net</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">in_chans</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">int</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">3</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">out_chans</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">int</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">model_type</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Literal</span><span class="p"><span class="pre">[</span></span><span class="pre">basic</span><span class="p"><span class="pre">,</span> </span><span class="pre">mobile</span><span class="p"><span class="pre">,</span> </span><span class="pre">human</span><span class="p"><span class="pre">,</span> </span><span class="pre">portrait</span><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">'basic'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">train_model</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">bool</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/NaMAZU/lightning_wingman/lit_u2net.html#LitU2Net"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#NaMAZU.lightning_wingman.lit_u2net.LitU2Net" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">pytorch_lightning.core.lightning.LightningModule</span></code></p>
<dl class="py method">
<dt class="sig sig-object py" id="NaMAZU.lightning_wingman.lit_u2net.LitU2Net.apply_mask">
<span class="sig-name descname"><span class="pre">apply_mask</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">prediction</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">torch.Tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">original_image</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">str</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">→</span> <span class="sig-return-typehint"><span class="pre">PIL.Image.Image</span></span></span><a class="reference internal" href="_modules/NaMAZU/lightning_wingman/lit_u2net.html#LitU2Net.apply_mask"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#NaMAZU.lightning_wingman.lit_u2net.LitU2Net.apply_mask" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>
<dl class="py method">
<dt class="sig sig-object py" id="NaMAZU.lightning_wingman.lit_u2net.LitU2Net.configure_optimizers">
<span class="sig-name descname"><span class="pre">configure_optimizers</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/NaMAZU/lightning_wingman/lit_u2net.html#LitU2Net.configure_optimizers"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#NaMAZU.lightning_wingman.lit_u2net.LitU2Net.configure_optimizers" title="Permalink to this definition">¶</a></dt>
<dd><p>Choose what optimizers and learning-rate schedulers to use in your optimization.
Normally you’d need one. But in the case of GANs or similar you might have multiple.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p><p>Any of these 6 options.</p>
<ul class="simple">
<li><p>Single optimizer.</p></li>
<li><p>List or Tuple - List of optimizers.</p></li>
<li><p>Two lists - The first list has multiple optimizers, the second a list of LR schedulers (or lr_dict).</p></li>
<li><p>Dictionary, with an ‘optimizer’ key, and (optionally) a ‘lr_scheduler’
key whose value is a single LR scheduler or lr_dict.</p></li>
<li><p>Tuple of dictionaries as described, with an optional ‘frequency’ key.</p></li>
<li><p>None - Fit will run without any optimizer.</p></li>
</ul>
</p>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The ‘frequency’ value is an int corresponding to the number of sequential batches
optimized with the specific optimizer. It should be given to none or to all of the optimizers.
There is a difference between passing multiple optimizers in a list,
and passing multiple optimizers in dictionaries with a frequency of 1:
In the former case, all optimizers will operate on the given batch in each optimization step.
In the latter, only one optimizer will operate on the given batch at every step.</p>
<p>The lr_dict is a dictionary which contains the scheduler and its associated configuration.
The default configuration is shown below.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="p">{</span>
    <span class="s1">'scheduler'</span><span class="p">:</span> <span class="n">lr_scheduler</span><span class="p">,</span> <span class="c1"># The LR scheduler instance (required)</span>
    <span class="s1">'interval'</span><span class="p">:</span> <span class="s1">'epoch'</span><span class="p">,</span> <span class="c1"># The unit of the scheduler's step size</span>
    <span class="s1">'frequency'</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span> <span class="c1"># The frequency of the scheduler</span>
    <span class="s1">'reduce_on_plateau'</span><span class="p">:</span> <span class="kc">False</span><span class="p">,</span> <span class="c1"># For ReduceLROnPlateau scheduler</span>
    <span class="s1">'monitor'</span><span class="p">:</span> <span class="s1">'val_loss'</span><span class="p">,</span> <span class="c1"># Metric for ReduceLROnPlateau to monitor</span>
    <span class="s1">'strict'</span><span class="p">:</span> <span class="kc">True</span><span class="p">,</span> <span class="c1"># Whether to crash the training if `monitor` is not found</span>
    <span class="s1">'name'</span><span class="p">:</span> <span class="kc">None</span><span class="p">,</span> <span class="c1"># Custom name for LearningRateMonitor to use</span>
<span class="p">}</span>
</pre></div>
</div>
<p>Only the <code class="docutils literal notranslate"><span class="pre">scheduler</span></code> key is required, the rest will be set to the defaults above.</p>
</div>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># most cases</span>
<span class="k">def</span> <span class="nf">configure_optimizers</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="n">opt</span> <span class="o">=</span> <span class="n">Adam</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">1e-3</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">opt</span>

<span class="c1"># multiple optimizer case (e.g.: GAN)</span>
<span class="k">def</span> <span class="nf">configure_optimizers</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="n">generator_opt</span> <span class="o">=</span> <span class="n">Adam</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model_gen</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span>
    <span class="n">disriminator_opt</span> <span class="o">=</span> <span class="n">Adam</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model_disc</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.02</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">generator_opt</span><span class="p">,</span> <span class="n">disriminator_opt</span>

<span class="c1"># example with learning rate schedulers</span>
<span class="k">def</span> <span class="nf">configure_optimizers</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="n">generator_opt</span> <span class="o">=</span> <span class="n">Adam</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model_gen</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span>
    <span class="n">disriminator_opt</span> <span class="o">=</span> <span class="n">Adam</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model_disc</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.02</span><span class="p">)</span>
    <span class="n">discriminator_sched</span> <span class="o">=</span> <span class="n">CosineAnnealing</span><span class="p">(</span><span class="n">discriminator_opt</span><span class="p">,</span> <span class="n">T_max</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
    <span class="k">return</span> <span class="p">[</span><span class="n">generator_opt</span><span class="p">,</span> <span class="n">disriminator_opt</span><span class="p">],</span> <span class="p">[</span><span class="n">discriminator_sched</span><span class="p">]</span>

<span class="c1"># example with step-based learning rate schedulers</span>
<span class="k">def</span> <span class="nf">configure_optimizers</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="n">gen_opt</span> <span class="o">=</span> <span class="n">Adam</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model_gen</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span>
    <span class="n">dis_opt</span> <span class="o">=</span> <span class="n">Adam</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model_disc</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.02</span><span class="p">)</span>
    <span class="n">gen_sched</span> <span class="o">=</span> <span class="p">{</span><span class="s1">'scheduler'</span><span class="p">:</span> <span class="n">ExponentialLR</span><span class="p">(</span><span class="n">gen_opt</span><span class="p">,</span> <span class="mf">0.99</span><span class="p">),</span>
                 <span class="s1">'interval'</span><span class="p">:</span> <span class="s1">'step'</span><span class="p">}</span>  <span class="c1"># called after each training step</span>
    <span class="n">dis_sched</span> <span class="o">=</span> <span class="n">CosineAnnealing</span><span class="p">(</span><span class="n">discriminator_opt</span><span class="p">,</span> <span class="n">T_max</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span> <span class="c1"># called every epoch</span>
    <span class="k">return</span> <span class="p">[</span><span class="n">gen_opt</span><span class="p">,</span> <span class="n">dis_opt</span><span class="p">],</span> <span class="p">[</span><span class="n">gen_sched</span><span class="p">,</span> <span class="n">dis_sched</span><span class="p">]</span>

<span class="c1"># example with optimizer frequencies</span>
<span class="c1"># see training procedure in `Improved Training of Wasserstein GANs`, Algorithm 1</span>
<span class="c1"># https://arxiv.org/abs/1704.00028</span>
<span class="k">def</span> <span class="nf">configure_optimizers</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="n">gen_opt</span> <span class="o">=</span> <span class="n">Adam</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model_gen</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span>
    <span class="n">dis_opt</span> <span class="o">=</span> <span class="n">Adam</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model_disc</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.02</span><span class="p">)</span>
    <span class="n">n_critic</span> <span class="o">=</span> <span class="mi">5</span>
    <span class="k">return</span> <span class="p">(</span>
        <span class="p">{</span><span class="s1">'optimizer'</span><span class="p">:</span> <span class="n">dis_opt</span><span class="p">,</span> <span class="s1">'frequency'</span><span class="p">:</span> <span class="n">n_critic</span><span class="p">},</span>
        <span class="p">{</span><span class="s1">'optimizer'</span><span class="p">:</span> <span class="n">gen_opt</span><span class="p">,</span> <span class="s1">'frequency'</span><span class="p">:</span> <span class="mi">1</span><span class="p">}</span>
    <span class="p">)</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Some things to know:</p>
<ul>
<li><p>Lightning calls <code class="docutils literal notranslate"><span class="pre">.backward()</span></code> and <code class="docutils literal notranslate"><span class="pre">.step()</span></code> on each optimizer
and learning rate scheduler as needed.</p></li>
<li><p>If you use 16-bit precision (<code class="docutils literal notranslate"><span class="pre">precision=16</span></code>), Lightning will automatically
handle the optimizers for you.</p></li>
<li><p>If you use multiple optimizers, <a class="reference internal" href="#NaMAZU.lightning_wingman.lit_u2net.LitU2Net.training_step" title="NaMAZU.lightning_wingman.lit_u2net.LitU2Net.training_step"><code class="xref py py-meth docutils literal notranslate"><span class="pre">training_step()</span></code></a> will have an additional
<code class="docutils literal notranslate"><span class="pre">optimizer_idx</span></code> parameter.</p></li>
<li><p>If you use LBFGS Lightning handles the closure function automatically for you.</p></li>
<li><p>If you use multiple optimizers, gradients will be calculated only
for the parameters of current optimizer at each training step.</p></li>
<li><p>If you need to control how often those optimizers step or override the
default <code class="docutils literal notranslate"><span class="pre">.step()</span></code> schedule, override the <code class="xref py py-meth docutils literal notranslate"><span class="pre">optimizer_step()</span></code> hook.</p></li>
<li><p>If you only want to call a learning rate scheduler every <code class="docutils literal notranslate"><span class="pre">x</span></code> step or epoch,
or want to monitor a custom metric, you can specify these in a lr_dict:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="p">{</span>
    <span class="s1">'scheduler'</span><span class="p">:</span> <span class="n">lr_scheduler</span><span class="p">,</span>
    <span class="s1">'interval'</span><span class="p">:</span> <span class="s1">'step'</span><span class="p">,</span>  <span class="c1"># or 'epoch'</span>
    <span class="s1">'monitor'</span><span class="p">:</span> <span class="s1">'val_f1'</span><span class="p">,</span>
    <span class="s1">'frequency'</span><span class="p">:</span> <span class="n">x</span><span class="p">,</span>
<span class="p">}</span>
</pre></div>
</div>
</li>
</ul>
</div>
</dd></dl>
<dl class="py method">
<dt class="sig sig-object py" id="NaMAZU.lightning_wingman.lit_u2net.LitU2Net.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">torch.Tensor</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">→</span> <span class="sig-return-typehint"><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">torch.Tensor</span><span class="p"><span class="pre">,</span> </span><span class="p"><span class="pre">...</span></span><span class="p"><span class="pre">]</span></span></span></span><a class="reference internal" href="_modules/NaMAZU/lightning_wingman/lit_u2net.html#LitU2Net.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#NaMAZU.lightning_wingman.lit_u2net.LitU2Net.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Same as <code class="xref py py-meth docutils literal notranslate"><span class="pre">torch.nn.Module.forward()</span></code>, however in Lightning you want this to define
the operations you want to use for prediction (i.e.: on a server or as a feature extractor).</p>
<p>Normally you’d call <code class="docutils literal notranslate"><span class="pre">self()</span></code> from your <a class="reference internal" href="#NaMAZU.lightning_wingman.lit_u2net.LitU2Net.training_step" title="NaMAZU.lightning_wingman.lit_u2net.LitU2Net.training_step"><code class="xref py py-meth docutils literal notranslate"><span class="pre">training_step()</span></code></a> method.
This makes it easy to write a complex system for training with the outputs
you’d want in a prediction setting.</p>
<p>You may also find the <code class="xref py py-func docutils literal notranslate"><span class="pre">auto_move_data()</span></code> decorator useful
when using the module outside Lightning in a production setting.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>*args</strong> – Whatever you decide to pass into the forward method.</p></li>
<li><p><strong>**kwargs</strong> – Keyword arguments are also possible.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Predicted output</p>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># example if we were using this model as a feature extractor</span>
<span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
    <span class="n">feature_maps</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">convnet</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">feature_maps</span>

<span class="k">def</span> <span class="nf">training_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">):</span>
    <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">batch</span>
    <span class="n">feature_maps</span> <span class="o">=</span> <span class="bp">self</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">logits</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">classifier</span><span class="p">(</span><span class="n">feature_maps</span><span class="p">)</span>

    <span class="c1"># ...</span>
    <span class="k">return</span> <span class="n">loss</span>

<span class="c1"># splitting it this way allows model to be used a feature extractor</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">MyModelAbove</span><span class="p">()</span>

<span class="n">inputs</span> <span class="o">=</span> <span class="n">server</span><span class="o">.</span><span class="n">get_request</span><span class="p">()</span>
<span class="n">results</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
<span class="n">server</span><span class="o">.</span><span class="n">write_results</span><span class="p">(</span><span class="n">results</span><span class="p">)</span>

<span class="c1"># -------------</span>
<span class="c1"># This is in stark contrast to torch.nn.Module where normally you would have this:</span>
<span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">):</span>
    <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">batch</span>
    <span class="n">feature_maps</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">convnet</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">logits</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">classifier</span><span class="p">(</span><span class="n">feature_maps</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">logits</span>
</pre></div>
</div>
</dd></dl>
<dl class="py method">
<dt class="sig sig-object py" id="NaMAZU.lightning_wingman.lit_u2net.LitU2Net.predict">
<span class="sig-name descname"><span class="pre">predict</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x_path</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">save</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">bool</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">save_path</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">str</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">''</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">→</span> <span class="sig-return-typehint"><span class="pre">torch.Tensor</span></span></span><a class="reference internal" href="_modules/NaMAZU/lightning_wingman/lit_u2net.html#LitU2Net.predict"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#NaMAZU.lightning_wingman.lit_u2net.LitU2Net.predict" title="Permalink to this definition">¶</a></dt>
<dd><p>Use this function with trainer.predict(…). Override if you need to add any processing logic.</p>
</dd></dl>
<dl class="py attribute">
<dt class="sig sig-object py" id="NaMAZU.lightning_wingman.lit_u2net.LitU2Net.training">
<span class="sig-name descname"><span class="pre">training</span></span><em class="property"><span class="pre">:</span> <span class="pre">bool</span></em><a class="headerlink" href="#NaMAZU.lightning_wingman.lit_u2net.LitU2Net.training" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>
<dl class="py method">
<dt class="sig sig-object py" id="NaMAZU.lightning_wingman.lit_u2net.LitU2Net.training_step">
<span class="sig-name descname"><span class="pre">training_step</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">batch</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_idx</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/NaMAZU/lightning_wingman/lit_u2net.html#LitU2Net.training_step"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#NaMAZU.lightning_wingman.lit_u2net.LitU2Net.training_step" title="Permalink to this definition">¶</a></dt>
<dd><p>Here you compute and return the training loss and some additional metrics for e.g.
the progress bar or logger.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>batch</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code> | (<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>, …) | [<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>, …]) – The output of your <code class="xref py py-class docutils literal notranslate"><span class="pre">DataLoader</span></code>. A tensor, tuple or list.</p></li>
<li><p><strong>batch_idx</strong> (<em>int</em>) – Integer displaying index of this batch</p></li>
<li><p><strong>optimizer_idx</strong> (<em>int</em>) – When using multiple optimizers, this argument will also be present.</p></li>
<li><p><strong>hiddens</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>) – Passed in if
<a href="#id3"><span class="problematic" id="id4">:paramref:`~pytorch_lightning.trainer.trainer.Trainer.truncated_bptt_steps`</span></a> &gt; 0.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><p>Any of.</p>
<ul class="simple">
<li><p><code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code> - The loss tensor</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">dict</span></code> - A dictionary. Can include any keys, but must include the key <code class="docutils literal notranslate"><span class="pre">'loss'</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">None</span></code> - Training will skip to the next batch</p></li>
</ul>
</p>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Returning <code class="docutils literal notranslate"><span class="pre">None</span></code> is currently not supported for multi-GPU or TPU, or with 16-bit precision enabled.</p>
</div>
<p>In this step you’d normally do the forward pass and calculate the loss for a batch.
You can also do fancier things like multiple forward passes or something model specific.</p>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">training_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">):</span>
    <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">z</span> <span class="o">=</span> <span class="n">batch</span>
    <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">loss</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">loss</span>
</pre></div>
</div>
<p>If you define multiple optimizers, this step will be called with an additional
<code class="docutils literal notranslate"><span class="pre">optimizer_idx</span></code> parameter.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Multiple optimizers (e.g.: GANs)</span>
<span class="k">def</span> <span class="nf">training_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">,</span> <span class="n">optimizer_idx</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">optimizer_idx</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="c1"># do training_step with encoder</span>
    <span class="k">if</span> <span class="n">optimizer_idx</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
        <span class="c1"># do training_step with decoder</span>
</pre></div>
</div>
<p>If you add truncated back propagation through time you will also get an additional
argument with the hidden states of the previous step.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Truncated back-propagation through time</span>
<span class="k">def</span> <span class="nf">training_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">,</span> <span class="n">hiddens</span><span class="p">):</span>
    <span class="c1"># hiddens are the hidden states from the previous truncated backprop step</span>
    <span class="o">...</span>
    <span class="n">out</span><span class="p">,</span> <span class="n">hiddens</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">lstm</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">hiddens</span><span class="p">)</span>
    <span class="o">...</span>
    <span class="k">return</span> <span class="p">{</span><span class="s1">'loss'</span><span class="p">:</span> <span class="n">loss</span><span class="p">,</span> <span class="s1">'hiddens'</span><span class="p">:</span> <span class="n">hiddens</span><span class="p">}</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The loss value shown in the progress bar is smoothed (averaged) over the last values,
so it differs from the actual loss returned in train/validation step.</p>
</div>
</dd></dl>
<dl class="py method">
<dt class="sig sig-object py" id="NaMAZU.lightning_wingman.lit_u2net.LitU2Net.validation_step">
<span class="sig-name descname"><span class="pre">validation_step</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">batch</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_idx</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/NaMAZU/lightning_wingman/lit_u2net.html#LitU2Net.validation_step"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#NaMAZU.lightning_wingman.lit_u2net.LitU2Net.validation_step" title="Permalink to this definition">¶</a></dt>
<dd><p>Operates on a single batch of data from the validation set.
In this step you’d might generate examples or calculate anything of interest like accuracy.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># the pseudocode for these calls</span>
<span class="n">val_outs</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">val_batch</span> <span class="ow">in</span> <span class="n">val_data</span><span class="p">:</span>
    <span class="n">out</span> <span class="o">=</span> <span class="n">validation_step</span><span class="p">(</span><span class="n">val_batch</span><span class="p">)</span>
    <span class="n">val_outs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
<span class="n">validation_epoch_end</span><span class="p">(</span><span class="n">val_outs</span><span class="p">)</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>batch</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code> | (<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>, …) | [<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>, …]) – The output of your <code class="xref py py-class docutils literal notranslate"><span class="pre">DataLoader</span></code>. A tensor, tuple or list.</p></li>
<li><p><strong>batch_idx</strong> (<em>int</em>) – The index of this batch</p></li>
<li><p><strong>dataloader_idx</strong> (<em>int</em>) – The index of the dataloader that produced this batch
(only if multiple val dataloaders used)</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><p>Any of.</p>
<blockquote>
<div><ul class="simple">
<li><p>Any object or value</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">None</span></code> - Validation will skip to the next batch</p></li>
</ul>
</div></blockquote>
</p>
</dd>
</dl>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># pseudocode of order</span>
<span class="n">val_outs</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">val_batch</span> <span class="ow">in</span> <span class="n">val_data</span><span class="p">:</span>
    <span class="n">out</span> <span class="o">=</span> <span class="n">validation_step</span><span class="p">(</span><span class="n">val_batch</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">defined</span><span class="p">(</span><span class="s1">'validation_step_end'</span><span class="p">):</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">validation_step_end</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
    <span class="n">val_outs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
<span class="n">val_outs</span> <span class="o">=</span> <span class="n">validation_epoch_end</span><span class="p">(</span><span class="n">val_outs</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># if you have one val dataloader:</span>
<span class="k">def</span> <span class="nf">validation_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">)</span>

<span class="c1"># if you have multiple val dataloaders:</span>
<span class="k">def</span> <span class="nf">validation_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">,</span> <span class="n">dataloader_idx</span><span class="p">)</span>
</pre></div>
</div>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># CASE 1: A single validation dataset</span>
<span class="k">def</span> <span class="nf">validation_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">):</span>
    <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">batch</span>

    <span class="c1"># implement your own</span>
    <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">loss</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

    <span class="c1"># log 6 example images</span>
    <span class="c1"># or generated text... or whatever</span>
    <span class="n">sample_imgs</span> <span class="o">=</span> <span class="n">x</span><span class="p">[:</span><span class="mi">6</span><span class="p">]</span>
    <span class="n">grid</span> <span class="o">=</span> <span class="n">torchvision</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">make_grid</span><span class="p">(</span><span class="n">sample_imgs</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">experiment</span><span class="o">.</span><span class="n">add_image</span><span class="p">(</span><span class="s1">'example_images'</span><span class="p">,</span> <span class="n">grid</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>

    <span class="c1"># calculate acc</span>
    <span class="n">labels_hat</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">val_acc</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">y</span> <span class="o">==</span> <span class="n">labels_hat</span><span class="p">)</span><span class="o">.</span><span class="n">item</span><span class="p">()</span> <span class="o">/</span> <span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">y</span><span class="p">)</span> <span class="o">*</span> <span class="mf">1.0</span><span class="p">)</span>

    <span class="c1"># log the outputs!</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">log_dict</span><span class="p">({</span><span class="s1">'val_loss'</span><span class="p">:</span> <span class="n">loss</span><span class="p">,</span> <span class="s1">'val_acc'</span><span class="p">:</span> <span class="n">val_acc</span><span class="p">})</span>
</pre></div>
</div>
<p>If you pass in multiple val dataloaders, <a class="reference internal" href="#NaMAZU.lightning_wingman.lit_u2net.LitU2Net.validation_step" title="NaMAZU.lightning_wingman.lit_u2net.LitU2Net.validation_step"><code class="xref py py-meth docutils literal notranslate"><span class="pre">validation_step()</span></code></a> will have an additional argument.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># CASE 2: multiple validation dataloaders</span>
<span class="k">def</span> <span class="nf">validation_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">,</span> <span class="n">dataloader_idx</span><span class="p">):</span>
    <span class="c1"># dataloader_idx tells you which dataset this is.</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If you don’t need to validate you don’t need to implement this method.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>When the <a class="reference internal" href="#NaMAZU.lightning_wingman.lit_u2net.LitU2Net.validation_step" title="NaMAZU.lightning_wingman.lit_u2net.LitU2Net.validation_step"><code class="xref py py-meth docutils literal notranslate"><span class="pre">validation_step()</span></code></a> is called, the model has been put in eval mode
and PyTorch gradients have been disabled. At the end of validation,
the model goes back to training mode and gradients are enabled.</p>
</div>
</dd></dl>
</dd></dl>


<span id="namazu-lightning-wingman-lit-video-clf-module"></span><h2 id="module-NaMAZU.lightning_wingman.lit_video_clf">NaMAZU.lightning_wingman.lit_video_clf module<a class="headerlink" href="#module-NaMAZU.lightning_wingman.lit_video_clf" title="Permalink to this headline">¶</a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="NaMAZU.lightning_wingman.lit_video_clf.LitVideoClf">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">NaMAZU.lightning_wingman.lit_video_clf.</span></span><span class="sig-name descname"><span class="pre">LitVideoClf</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">use_lstm</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">bool</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">model_config</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">dict</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/NaMAZU/lightning_wingman/lit_video_clf.html#LitVideoClf"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#NaMAZU.lightning_wingman.lit_video_clf.LitVideoClf" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">pytorch_lightning.core.lightning.LightningModule</span></code></p>
<dl class="py method">
<dt class="sig sig-object py" id="NaMAZU.lightning_wingman.lit_video_clf.LitVideoClf.configure_optimizers">
<span class="sig-name descname"><span class="pre">configure_optimizers</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">→</span> <span class="sig-return-typehint"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span> </span><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">torch.optim.adam.Adam</span><span class="p"><span class="pre">,</span> </span><span class="pre">torch.optim.lr_scheduler.StepLR</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></span><a class="reference internal" href="_modules/NaMAZU/lightning_wingman/lit_video_clf.html#LitVideoClf.configure_optimizers"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#NaMAZU.lightning_wingman.lit_video_clf.LitVideoClf.configure_optimizers" title="Permalink to this definition">¶</a></dt>
<dd><p>Choose what optimizers and learning-rate schedulers to use in your optimization.
Normally you’d need one. But in the case of GANs or similar you might have multiple.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p><p>Any of these 6 options.</p>
<ul class="simple">
<li><p>Single optimizer.</p></li>
<li><p>List or Tuple - List of optimizers.</p></li>
<li><p>Two lists - The first list has multiple optimizers, the second a list of LR schedulers (or lr_dict).</p></li>
<li><p>Dictionary, with an ‘optimizer’ key, and (optionally) a ‘lr_scheduler’
key whose value is a single LR scheduler or lr_dict.</p></li>
<li><p>Tuple of dictionaries as described, with an optional ‘frequency’ key.</p></li>
<li><p>None - Fit will run without any optimizer.</p></li>
</ul>
</p>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The ‘frequency’ value is an int corresponding to the number of sequential batches
optimized with the specific optimizer. It should be given to none or to all of the optimizers.
There is a difference between passing multiple optimizers in a list,
and passing multiple optimizers in dictionaries with a frequency of 1:
In the former case, all optimizers will operate on the given batch in each optimization step.
In the latter, only one optimizer will operate on the given batch at every step.</p>
<p>The lr_dict is a dictionary which contains the scheduler and its associated configuration.
The default configuration is shown below.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="p">{</span>
    <span class="s1">'scheduler'</span><span class="p">:</span> <span class="n">lr_scheduler</span><span class="p">,</span> <span class="c1"># The LR scheduler instance (required)</span>
    <span class="s1">'interval'</span><span class="p">:</span> <span class="s1">'epoch'</span><span class="p">,</span> <span class="c1"># The unit of the scheduler's step size</span>
    <span class="s1">'frequency'</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span> <span class="c1"># The frequency of the scheduler</span>
    <span class="s1">'reduce_on_plateau'</span><span class="p">:</span> <span class="kc">False</span><span class="p">,</span> <span class="c1"># For ReduceLROnPlateau scheduler</span>
    <span class="s1">'monitor'</span><span class="p">:</span> <span class="s1">'val_loss'</span><span class="p">,</span> <span class="c1"># Metric for ReduceLROnPlateau to monitor</span>
    <span class="s1">'strict'</span><span class="p">:</span> <span class="kc">True</span><span class="p">,</span> <span class="c1"># Whether to crash the training if `monitor` is not found</span>
    <span class="s1">'name'</span><span class="p">:</span> <span class="kc">None</span><span class="p">,</span> <span class="c1"># Custom name for LearningRateMonitor to use</span>
<span class="p">}</span>
</pre></div>
</div>
<p>Only the <code class="docutils literal notranslate"><span class="pre">scheduler</span></code> key is required, the rest will be set to the defaults above.</p>
</div>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># most cases</span>
<span class="k">def</span> <span class="nf">configure_optimizers</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="n">opt</span> <span class="o">=</span> <span class="n">Adam</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">1e-3</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">opt</span>

<span class="c1"># multiple optimizer case (e.g.: GAN)</span>
<span class="k">def</span> <span class="nf">configure_optimizers</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="n">generator_opt</span> <span class="o">=</span> <span class="n">Adam</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model_gen</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span>
    <span class="n">disriminator_opt</span> <span class="o">=</span> <span class="n">Adam</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model_disc</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.02</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">generator_opt</span><span class="p">,</span> <span class="n">disriminator_opt</span>

<span class="c1"># example with learning rate schedulers</span>
<span class="k">def</span> <span class="nf">configure_optimizers</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="n">generator_opt</span> <span class="o">=</span> <span class="n">Adam</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model_gen</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span>
    <span class="n">disriminator_opt</span> <span class="o">=</span> <span class="n">Adam</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model_disc</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.02</span><span class="p">)</span>
    <span class="n">discriminator_sched</span> <span class="o">=</span> <span class="n">CosineAnnealing</span><span class="p">(</span><span class="n">discriminator_opt</span><span class="p">,</span> <span class="n">T_max</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
    <span class="k">return</span> <span class="p">[</span><span class="n">generator_opt</span><span class="p">,</span> <span class="n">disriminator_opt</span><span class="p">],</span> <span class="p">[</span><span class="n">discriminator_sched</span><span class="p">]</span>

<span class="c1"># example with step-based learning rate schedulers</span>
<span class="k">def</span> <span class="nf">configure_optimizers</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="n">gen_opt</span> <span class="o">=</span> <span class="n">Adam</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model_gen</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span>
    <span class="n">dis_opt</span> <span class="o">=</span> <span class="n">Adam</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model_disc</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.02</span><span class="p">)</span>
    <span class="n">gen_sched</span> <span class="o">=</span> <span class="p">{</span><span class="s1">'scheduler'</span><span class="p">:</span> <span class="n">ExponentialLR</span><span class="p">(</span><span class="n">gen_opt</span><span class="p">,</span> <span class="mf">0.99</span><span class="p">),</span>
                 <span class="s1">'interval'</span><span class="p">:</span> <span class="s1">'step'</span><span class="p">}</span>  <span class="c1"># called after each training step</span>
    <span class="n">dis_sched</span> <span class="o">=</span> <span class="n">CosineAnnealing</span><span class="p">(</span><span class="n">discriminator_opt</span><span class="p">,</span> <span class="n">T_max</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span> <span class="c1"># called every epoch</span>
    <span class="k">return</span> <span class="p">[</span><span class="n">gen_opt</span><span class="p">,</span> <span class="n">dis_opt</span><span class="p">],</span> <span class="p">[</span><span class="n">gen_sched</span><span class="p">,</span> <span class="n">dis_sched</span><span class="p">]</span>

<span class="c1"># example with optimizer frequencies</span>
<span class="c1"># see training procedure in `Improved Training of Wasserstein GANs`, Algorithm 1</span>
<span class="c1"># https://arxiv.org/abs/1704.00028</span>
<span class="k">def</span> <span class="nf">configure_optimizers</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="n">gen_opt</span> <span class="o">=</span> <span class="n">Adam</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model_gen</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span>
    <span class="n">dis_opt</span> <span class="o">=</span> <span class="n">Adam</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model_disc</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.02</span><span class="p">)</span>
    <span class="n">n_critic</span> <span class="o">=</span> <span class="mi">5</span>
    <span class="k">return</span> <span class="p">(</span>
        <span class="p">{</span><span class="s1">'optimizer'</span><span class="p">:</span> <span class="n">dis_opt</span><span class="p">,</span> <span class="s1">'frequency'</span><span class="p">:</span> <span class="n">n_critic</span><span class="p">},</span>
        <span class="p">{</span><span class="s1">'optimizer'</span><span class="p">:</span> <span class="n">gen_opt</span><span class="p">,</span> <span class="s1">'frequency'</span><span class="p">:</span> <span class="mi">1</span><span class="p">}</span>
    <span class="p">)</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Some things to know:</p>
<ul>
<li><p>Lightning calls <code class="docutils literal notranslate"><span class="pre">.backward()</span></code> and <code class="docutils literal notranslate"><span class="pre">.step()</span></code> on each optimizer
and learning rate scheduler as needed.</p></li>
<li><p>If you use 16-bit precision (<code class="docutils literal notranslate"><span class="pre">precision=16</span></code>), Lightning will automatically
handle the optimizers for you.</p></li>
<li><p>If you use multiple optimizers, <a class="reference internal" href="#NaMAZU.lightning_wingman.lit_video_clf.LitVideoClf.training_step" title="NaMAZU.lightning_wingman.lit_video_clf.LitVideoClf.training_step"><code class="xref py py-meth docutils literal notranslate"><span class="pre">training_step()</span></code></a> will have an additional
<code class="docutils literal notranslate"><span class="pre">optimizer_idx</span></code> parameter.</p></li>
<li><p>If you use LBFGS Lightning handles the closure function automatically for you.</p></li>
<li><p>If you use multiple optimizers, gradients will be calculated only
for the parameters of current optimizer at each training step.</p></li>
<li><p>If you need to control how often those optimizers step or override the
default <code class="docutils literal notranslate"><span class="pre">.step()</span></code> schedule, override the <code class="xref py py-meth docutils literal notranslate"><span class="pre">optimizer_step()</span></code> hook.</p></li>
<li><p>If you only want to call a learning rate scheduler every <code class="docutils literal notranslate"><span class="pre">x</span></code> step or epoch,
or want to monitor a custom metric, you can specify these in a lr_dict:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="p">{</span>
    <span class="s1">'scheduler'</span><span class="p">:</span> <span class="n">lr_scheduler</span><span class="p">,</span>
    <span class="s1">'interval'</span><span class="p">:</span> <span class="s1">'step'</span><span class="p">,</span>  <span class="c1"># or 'epoch'</span>
    <span class="s1">'monitor'</span><span class="p">:</span> <span class="s1">'val_f1'</span><span class="p">,</span>
    <span class="s1">'frequency'</span><span class="p">:</span> <span class="n">x</span><span class="p">,</span>
<span class="p">}</span>
</pre></div>
</div>
</li>
</ul>
</div>
</dd></dl>
<dl class="py method">
<dt class="sig sig-object py" id="NaMAZU.lightning_wingman.lit_video_clf.LitVideoClf.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">batch</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">→</span> <span class="sig-return-typehint"><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">torch.Tensor</span><span class="p"><span class="pre">,</span> </span><span class="pre">torch.Tensor</span><span class="p"><span class="pre">]</span></span></span></span><a class="reference internal" href="_modules/NaMAZU/lightning_wingman/lit_video_clf.html#LitVideoClf.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#NaMAZU.lightning_wingman.lit_video_clf.LitVideoClf.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Same as <code class="xref py py-meth docutils literal notranslate"><span class="pre">torch.nn.Module.forward()</span></code>, however in Lightning you want this to define
the operations you want to use for prediction (i.e.: on a server or as a feature extractor).</p>
<p>Normally you’d call <code class="docutils literal notranslate"><span class="pre">self()</span></code> from your <a class="reference internal" href="#NaMAZU.lightning_wingman.lit_video_clf.LitVideoClf.training_step" title="NaMAZU.lightning_wingman.lit_video_clf.LitVideoClf.training_step"><code class="xref py py-meth docutils literal notranslate"><span class="pre">training_step()</span></code></a> method.
This makes it easy to write a complex system for training with the outputs
you’d want in a prediction setting.</p>
<p>You may also find the <code class="xref py py-func docutils literal notranslate"><span class="pre">auto_move_data()</span></code> decorator useful
when using the module outside Lightning in a production setting.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>*args</strong> – Whatever you decide to pass into the forward method.</p></li>
<li><p><strong>**kwargs</strong> – Keyword arguments are also possible.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Predicted output</p>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># example if we were using this model as a feature extractor</span>
<span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
    <span class="n">feature_maps</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">convnet</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">feature_maps</span>

<span class="k">def</span> <span class="nf">training_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">):</span>
    <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">batch</span>
    <span class="n">feature_maps</span> <span class="o">=</span> <span class="bp">self</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">logits</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">classifier</span><span class="p">(</span><span class="n">feature_maps</span><span class="p">)</span>

    <span class="c1"># ...</span>
    <span class="k">return</span> <span class="n">loss</span>

<span class="c1"># splitting it this way allows model to be used a feature extractor</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">MyModelAbove</span><span class="p">()</span>

<span class="n">inputs</span> <span class="o">=</span> <span class="n">server</span><span class="o">.</span><span class="n">get_request</span><span class="p">()</span>
<span class="n">results</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
<span class="n">server</span><span class="o">.</span><span class="n">write_results</span><span class="p">(</span><span class="n">results</span><span class="p">)</span>

<span class="c1"># -------------</span>
<span class="c1"># This is in stark contrast to torch.nn.Module where normally you would have this:</span>
<span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">):</span>
    <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">batch</span>
    <span class="n">feature_maps</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">convnet</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">logits</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">classifier</span><span class="p">(</span><span class="n">feature_maps</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">logits</span>
</pre></div>
</div>
</dd></dl>
<dl class="py method">
<dt class="sig sig-object py" id="NaMAZU.lightning_wingman.lit_video_clf.LitVideoClf.test_step">
<span class="sig-name descname"><span class="pre">test_step</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">batch</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_nb</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">→</span> <span class="sig-return-typehint"><span class="pre">torch.Tensor</span></span></span><a class="reference internal" href="_modules/NaMAZU/lightning_wingman/lit_video_clf.html#LitVideoClf.test_step"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#NaMAZU.lightning_wingman.lit_video_clf.LitVideoClf.test_step" title="Permalink to this definition">¶</a></dt>
<dd><p>Operates on a single batch of data from the test set.
In this step you’d normally generate examples or calculate anything of interest
such as accuracy.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># the pseudocode for these calls</span>
<span class="n">test_outs</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">test_batch</span> <span class="ow">in</span> <span class="n">test_data</span><span class="p">:</span>
    <span class="n">out</span> <span class="o">=</span> <span class="n">test_step</span><span class="p">(</span><span class="n">test_batch</span><span class="p">)</span>
    <span class="n">test_outs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
<span class="n">test_epoch_end</span><span class="p">(</span><span class="n">test_outs</span><span class="p">)</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>batch</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code> | (<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>, …) | [<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>, …]) – The output of your <code class="xref py py-class docutils literal notranslate"><span class="pre">DataLoader</span></code>. A tensor, tuple or list.</p></li>
<li><p><strong>batch_idx</strong> (<em>int</em>) – The index of this batch.</p></li>
<li><p><strong>dataloader_idx</strong> (<em>int</em>) – The index of the dataloader that produced this batch
(only if multiple test dataloaders used).</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><p>Any of.</p>
<blockquote>
<div><ul class="simple">
<li><p>Any object or value</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">None</span></code> - Testing will skip to the next batch</p></li>
</ul>
</div></blockquote>
</p>
</dd>
</dl>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># if you have one test dataloader:</span>
<span class="k">def</span> <span class="nf">test_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">)</span>

<span class="c1"># if you have multiple test dataloaders:</span>
<span class="k">def</span> <span class="nf">test_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">,</span> <span class="n">dataloader_idx</span><span class="p">)</span>
</pre></div>
</div>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># CASE 1: A single test dataset</span>
<span class="k">def</span> <span class="nf">test_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">):</span>
    <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">batch</span>

    <span class="c1"># implement your own</span>
    <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">loss</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

    <span class="c1"># log 6 example images</span>
    <span class="c1"># or generated text... or whatever</span>
    <span class="n">sample_imgs</span> <span class="o">=</span> <span class="n">x</span><span class="p">[:</span><span class="mi">6</span><span class="p">]</span>
    <span class="n">grid</span> <span class="o">=</span> <span class="n">torchvision</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">make_grid</span><span class="p">(</span><span class="n">sample_imgs</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">experiment</span><span class="o">.</span><span class="n">add_image</span><span class="p">(</span><span class="s1">'example_images'</span><span class="p">,</span> <span class="n">grid</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>

    <span class="c1"># calculate acc</span>
    <span class="n">labels_hat</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">test_acc</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">y</span> <span class="o">==</span> <span class="n">labels_hat</span><span class="p">)</span><span class="o">.</span><span class="n">item</span><span class="p">()</span> <span class="o">/</span> <span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">y</span><span class="p">)</span> <span class="o">*</span> <span class="mf">1.0</span><span class="p">)</span>

    <span class="c1"># log the outputs!</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">log_dict</span><span class="p">({</span><span class="s1">'test_loss'</span><span class="p">:</span> <span class="n">loss</span><span class="p">,</span> <span class="s1">'test_acc'</span><span class="p">:</span> <span class="n">test_acc</span><span class="p">})</span>
</pre></div>
</div>
<p>If you pass in multiple test dataloaders, <a class="reference internal" href="#NaMAZU.lightning_wingman.lit_video_clf.LitVideoClf.test_step" title="NaMAZU.lightning_wingman.lit_video_clf.LitVideoClf.test_step"><code class="xref py py-meth docutils literal notranslate"><span class="pre">test_step()</span></code></a> will have an additional argument.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># CASE 2: multiple test dataloaders</span>
<span class="k">def</span> <span class="nf">test_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">,</span> <span class="n">dataloader_idx</span><span class="p">):</span>
    <span class="c1"># dataloader_idx tells you which dataset this is.</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If you don’t need to test you don’t need to implement this method.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>When the <a class="reference internal" href="#NaMAZU.lightning_wingman.lit_video_clf.LitVideoClf.test_step" title="NaMAZU.lightning_wingman.lit_video_clf.LitVideoClf.test_step"><code class="xref py py-meth docutils literal notranslate"><span class="pre">test_step()</span></code></a> is called, the model has been put in eval mode and
PyTorch gradients have been disabled. At the end of the test epoch, the model goes back
to training mode and gradients are enabled.</p>
</div>
</dd></dl>
<dl class="py attribute">
<dt class="sig sig-object py" id="NaMAZU.lightning_wingman.lit_video_clf.LitVideoClf.training">
<span class="sig-name descname"><span class="pre">training</span></span><em class="property"><span class="pre">:</span> <span class="pre">bool</span></em><a class="headerlink" href="#NaMAZU.lightning_wingman.lit_video_clf.LitVideoClf.training" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>
<dl class="py method">
<dt class="sig sig-object py" id="NaMAZU.lightning_wingman.lit_video_clf.LitVideoClf.training_step">
<span class="sig-name descname"><span class="pre">training_step</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">batch</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_nb</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">→</span> <span class="sig-return-typehint"><span class="pre">torch.Tensor</span></span></span><a class="reference internal" href="_modules/NaMAZU/lightning_wingman/lit_video_clf.html#LitVideoClf.training_step"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#NaMAZU.lightning_wingman.lit_video_clf.LitVideoClf.training_step" title="Permalink to this definition">¶</a></dt>
<dd><p>Here you compute and return the training loss and some additional metrics for e.g.
the progress bar or logger.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>batch</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code> | (<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>, …) | [<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>, …]) – The output of your <code class="xref py py-class docutils literal notranslate"><span class="pre">DataLoader</span></code>. A tensor, tuple or list.</p></li>
<li><p><strong>batch_idx</strong> (<em>int</em>) – Integer displaying index of this batch</p></li>
<li><p><strong>optimizer_idx</strong> (<em>int</em>) – When using multiple optimizers, this argument will also be present.</p></li>
<li><p><strong>hiddens</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>) – Passed in if
<a href="#id5"><span class="problematic" id="id6">:paramref:`~pytorch_lightning.trainer.trainer.Trainer.truncated_bptt_steps`</span></a> &gt; 0.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><p>Any of.</p>
<ul class="simple">
<li><p><code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code> - The loss tensor</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">dict</span></code> - A dictionary. Can include any keys, but must include the key <code class="docutils literal notranslate"><span class="pre">'loss'</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">None</span></code> - Training will skip to the next batch</p></li>
</ul>
</p>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Returning <code class="docutils literal notranslate"><span class="pre">None</span></code> is currently not supported for multi-GPU or TPU, or with 16-bit precision enabled.</p>
</div>
<p>In this step you’d normally do the forward pass and calculate the loss for a batch.
You can also do fancier things like multiple forward passes or something model specific.</p>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">training_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">):</span>
    <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">z</span> <span class="o">=</span> <span class="n">batch</span>
    <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">loss</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">loss</span>
</pre></div>
</div>
<p>If you define multiple optimizers, this step will be called with an additional
<code class="docutils literal notranslate"><span class="pre">optimizer_idx</span></code> parameter.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Multiple optimizers (e.g.: GANs)</span>
<span class="k">def</span> <span class="nf">training_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">,</span> <span class="n">optimizer_idx</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">optimizer_idx</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="c1"># do training_step with encoder</span>
    <span class="k">if</span> <span class="n">optimizer_idx</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
        <span class="c1"># do training_step with decoder</span>
</pre></div>
</div>
<p>If you add truncated back propagation through time you will also get an additional
argument with the hidden states of the previous step.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Truncated back-propagation through time</span>
<span class="k">def</span> <span class="nf">training_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">,</span> <span class="n">hiddens</span><span class="p">):</span>
    <span class="c1"># hiddens are the hidden states from the previous truncated backprop step</span>
    <span class="o">...</span>
    <span class="n">out</span><span class="p">,</span> <span class="n">hiddens</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">lstm</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">hiddens</span><span class="p">)</span>
    <span class="o">...</span>
    <span class="k">return</span> <span class="p">{</span><span class="s1">'loss'</span><span class="p">:</span> <span class="n">loss</span><span class="p">,</span> <span class="s1">'hiddens'</span><span class="p">:</span> <span class="n">hiddens</span><span class="p">}</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The loss value shown in the progress bar is smoothed (averaged) over the last values,
so it differs from the actual loss returned in train/validation step.</p>
</div>
</dd></dl>
<dl class="py method">
<dt class="sig sig-object py" id="NaMAZU.lightning_wingman.lit_video_clf.LitVideoClf.validation_step">
<span class="sig-name descname"><span class="pre">validation_step</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">batch</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_nb</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">→</span> <span class="sig-return-typehint"><span class="pre">torch.Tensor</span></span></span><a class="reference internal" href="_modules/NaMAZU/lightning_wingman/lit_video_clf.html#LitVideoClf.validation_step"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#NaMAZU.lightning_wingman.lit_video_clf.LitVideoClf.validation_step" title="Permalink to this definition">¶</a></dt>
<dd><p>Operates on a single batch of data from the validation set.
In this step you’d might generate examples or calculate anything of interest like accuracy.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># the pseudocode for these calls</span>
<span class="n">val_outs</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">val_batch</span> <span class="ow">in</span> <span class="n">val_data</span><span class="p">:</span>
    <span class="n">out</span> <span class="o">=</span> <span class="n">validation_step</span><span class="p">(</span><span class="n">val_batch</span><span class="p">)</span>
    <span class="n">val_outs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
<span class="n">validation_epoch_end</span><span class="p">(</span><span class="n">val_outs</span><span class="p">)</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>batch</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code> | (<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>, …) | [<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>, …]) – The output of your <code class="xref py py-class docutils literal notranslate"><span class="pre">DataLoader</span></code>. A tensor, tuple or list.</p></li>
<li><p><strong>batch_idx</strong> (<em>int</em>) – The index of this batch</p></li>
<li><p><strong>dataloader_idx</strong> (<em>int</em>) – The index of the dataloader that produced this batch
(only if multiple val dataloaders used)</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><p>Any of.</p>
<blockquote>
<div><ul class="simple">
<li><p>Any object or value</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">None</span></code> - Validation will skip to the next batch</p></li>
</ul>
</div></blockquote>
</p>
</dd>
</dl>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># pseudocode of order</span>
<span class="n">val_outs</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">val_batch</span> <span class="ow">in</span> <span class="n">val_data</span><span class="p">:</span>
    <span class="n">out</span> <span class="o">=</span> <span class="n">validation_step</span><span class="p">(</span><span class="n">val_batch</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">defined</span><span class="p">(</span><span class="s1">'validation_step_end'</span><span class="p">):</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">validation_step_end</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
    <span class="n">val_outs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
<span class="n">val_outs</span> <span class="o">=</span> <span class="n">validation_epoch_end</span><span class="p">(</span><span class="n">val_outs</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># if you have one val dataloader:</span>
<span class="k">def</span> <span class="nf">validation_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">)</span>

<span class="c1"># if you have multiple val dataloaders:</span>
<span class="k">def</span> <span class="nf">validation_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">,</span> <span class="n">dataloader_idx</span><span class="p">)</span>
</pre></div>
</div>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># CASE 1: A single validation dataset</span>
<span class="k">def</span> <span class="nf">validation_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">):</span>
    <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">batch</span>

    <span class="c1"># implement your own</span>
    <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">loss</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

    <span class="c1"># log 6 example images</span>
    <span class="c1"># or generated text... or whatever</span>
    <span class="n">sample_imgs</span> <span class="o">=</span> <span class="n">x</span><span class="p">[:</span><span class="mi">6</span><span class="p">]</span>
    <span class="n">grid</span> <span class="o">=</span> <span class="n">torchvision</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">make_grid</span><span class="p">(</span><span class="n">sample_imgs</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">experiment</span><span class="o">.</span><span class="n">add_image</span><span class="p">(</span><span class="s1">'example_images'</span><span class="p">,</span> <span class="n">grid</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>

    <span class="c1"># calculate acc</span>
    <span class="n">labels_hat</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">val_acc</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">y</span> <span class="o">==</span> <span class="n">labels_hat</span><span class="p">)</span><span class="o">.</span><span class="n">item</span><span class="p">()</span> <span class="o">/</span> <span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">y</span><span class="p">)</span> <span class="o">*</span> <span class="mf">1.0</span><span class="p">)</span>

    <span class="c1"># log the outputs!</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">log_dict</span><span class="p">({</span><span class="s1">'val_loss'</span><span class="p">:</span> <span class="n">loss</span><span class="p">,</span> <span class="s1">'val_acc'</span><span class="p">:</span> <span class="n">val_acc</span><span class="p">})</span>
</pre></div>
</div>
<p>If you pass in multiple val dataloaders, <a class="reference internal" href="#NaMAZU.lightning_wingman.lit_video_clf.LitVideoClf.validation_step" title="NaMAZU.lightning_wingman.lit_video_clf.LitVideoClf.validation_step"><code class="xref py py-meth docutils literal notranslate"><span class="pre">validation_step()</span></code></a> will have an additional argument.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># CASE 2: multiple validation dataloaders</span>
<span class="k">def</span> <span class="nf">validation_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">,</span> <span class="n">dataloader_idx</span><span class="p">):</span>
    <span class="c1"># dataloader_idx tells you which dataset this is.</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If you don’t need to validate you don’t need to implement this method.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>When the <a class="reference internal" href="#NaMAZU.lightning_wingman.lit_video_clf.LitVideoClf.validation_step" title="NaMAZU.lightning_wingman.lit_video_clf.LitVideoClf.validation_step"><code class="xref py py-meth docutils literal notranslate"><span class="pre">validation_step()</span></code></a> is called, the model has been put in eval mode
and PyTorch gradients have been disabled. At the end of validation,
the model goes back to training mode and gradients are enabled.</p>
</div>
</dd></dl>
</dd></dl>


<span id="namazu-lightning-wingman-multi-modal-module"></span><h2 id="module-NaMAZU.lightning_wingman.multi_modal">NaMAZU.lightning_wingman.multi_modal module<a class="headerlink" href="#module-NaMAZU.lightning_wingman.multi_modal" title="Permalink to this headline">¶</a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="NaMAZU.lightning_wingman.multi_modal.StochasticFusion">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">NaMAZU.lightning_wingman.multi_modal.</span></span><span class="sig-name descname"><span class="pre">StochasticFusion</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">num_modalities</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mu</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">torch.Tensor</span><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">rho</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">torch.Tensor</span><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/NaMAZU/lightning_wingman/multi_modal.html#StochasticFusion"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#NaMAZU.lightning_wingman.multi_modal.StochasticFusion" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.module.Module</span></code></p>
<dl class="py method">
<dt class="sig sig-object py" id="NaMAZU.lightning_wingman.multi_modal.StochasticFusion.sample">
<span class="sig-name descname"><span class="pre">sample</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/NaMAZU/lightning_wingman/multi_modal.html#StochasticFusion.sample"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#NaMAZU.lightning_wingman.multi_modal.StochasticFusion.sample" title="Permalink to this definition">¶</a></dt>
<dd><p>Samples weights by sampling form a Normal distribution, multiplying by a sigma, which is
a function from a trainable parameter, and adding a mean
sets those weights as the current ones
:returns: torch.tensor with same shape as self.mu and self.rho</p>
</dd></dl>
<dl class="py attribute">
<dt class="sig sig-object py" id="NaMAZU.lightning_wingman.multi_modal.StochasticFusion.training">
<span class="sig-name descname"><span class="pre">training</span></span><em class="property"><span class="pre">:</span> <span class="pre">bool</span></em><a class="headerlink" href="#NaMAZU.lightning_wingman.multi_modal.StochasticFusion.training" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>
</dd></dl>


<span id="namazu-lightning-wingman-pretrainer-module"></span><h2 id="module-NaMAZU.lightning_wingman.pretrainer">NaMAZU.lightning_wingman.pretrainer module<a class="headerlink" href="#module-NaMAZU.lightning_wingman.pretrainer" title="Permalink to this headline">¶</a></h2>
<dl class="py function">
<dt class="sig sig-object py" id="NaMAZU.lightning_wingman.pretrainer.self_supervised_training">
<span class="sig-prename descclassname"><span class="pre">NaMAZU.lightning_wingman.pretrainer.</span></span><span class="sig-name descname"><span class="pre">self_supervised_training</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">model_choice</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">image_dirs</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_size</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_threads</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">int</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">6</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_iterations</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">int</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">10000</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">save_dir</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">str</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">''</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">device</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">str</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">'cuda'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">simsiam</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">bool</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">→</span> <span class="sig-return-typehint"><span class="pre">torch.nn.modules.module.Module</span></span></span><a class="reference internal" href="_modules/NaMAZU/lightning_wingman/pretrainer.html#self_supervised_training"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#NaMAZU.lightning_wingman.pretrainer.self_supervised_training" title="Permalink to this definition">¶</a></dt>
<dd><p>Run self supervised training on given model with images given by image_dirs.</p>
<p>If len(image_dirs) is 1, then the model is trained on the single dataset.
Otherwise, mini-batch consists of images drown n // len(dataset) times from each dataset.
Argument simsiam is used to indicate whether to do SimSiam training or not.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>model_choice</strong> (<em>str</em>) – Model to train currently VGG, ResNet and DenseNet are supported.</p></li>
<li><p><strong>image_dirs</strong> (<em>List</em><em>[</em><em>str</em><em>]</em>) – List of pathes of datasets.</p></li>
<li><p><strong>batch_size</strong> (<em>int</em>) – Batch size.</p></li>
<li><p><strong>num_threads</strong> (<em>int</em>) – Number of cpu threads to use.</p></li>
<li><p><strong>num_iterations</strong> (<em>int</em>) – Number of epochs. Default is 10000.</p></li>
<li><p><strong>save_dir</strong> (<em>str</em><em>, </em><em>optional</em>) – Trained model is saved to the directory if given otherwise returned. Defaults to “”.</p></li>
<li><p><strong>device</strong> (<em>str</em><em>, </em><em>optional</em>) – Device to use. Defaults to “cuda”.</p></li>
<li><p><strong>simsiam</strong> (<em>bool</em><em>, </em><em>optional</em>) – If True, use SimSiam. Defaults to False.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Trained model.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>torch.nn.Module</p>
</dd>
</dl>
</dd></dl>


<span id="namazu-lightning-wingman-torch-gmm-module"></span><h2 id="module-NaMAZU.lightning_wingman.torch_gmm">NaMAZU.lightning_wingman.torch_gmm module<a class="headerlink" href="#module-NaMAZU.lightning_wingman.torch_gmm" title="Permalink to this headline">¶</a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="NaMAZU.lightning_wingman.torch_gmm.GMM">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">NaMAZU.lightning_wingman.torch_gmm.</span></span><span class="sig-name descname"><span class="pre">GMM</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">n_components</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_features</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">covariance_type</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">str</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">'full'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">eps</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">float</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">1e-06</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">init_params</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">str</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">'kmeans'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mu_init</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">torch.Tensor</span><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">var_init</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">torch.Tensor</span><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/NaMAZU/lightning_wingman/torch_gmm.html#GMM"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#NaMAZU.lightning_wingman.torch_gmm.GMM" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.module.Module</span></code></p>
<p>PyTorch implementation of Gauusian Mixture Model with pytorch lightning support.</p>
<dl class="py attribute">
<dt class="sig sig-object py" id="NaMAZU.lightning_wingman.torch_gmm.GMM.var">
<span class="sig-name descname"><span class="pre">var</span></span><a class="headerlink" href="#NaMAZU.lightning_wingman.torch_gmm.GMM.var" title="Permalink to this definition">¶</a></dt>
<dd><p>Variance of the Gaussian distribution.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>Tensor</p>
</dd>
</dl>
</dd></dl>
<dl class="py attribute">
<dt class="sig sig-object py" id="NaMAZU.lightning_wingman.torch_gmm.GMM.mu">
<span class="sig-name descname"><span class="pre">mu</span></span><a class="headerlink" href="#NaMAZU.lightning_wingman.torch_gmm.GMM.mu" title="Permalink to this definition">¶</a></dt>
<dd><p>Mean of the Gaussian distribution.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>Tensor</p>
</dd>
</dl>
</dd></dl>
<dl class="py attribute">
<dt class="sig sig-object py" id="NaMAZU.lightning_wingman.torch_gmm.GMM.pi">
<span class="sig-name descname"><span class="pre">pi</span></span><a class="headerlink" href="#NaMAZU.lightning_wingman.torch_gmm.GMM.pi" title="Permalink to this definition">¶</a></dt>
<dd><p>Weight of the Gaussian distribution.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>Tensor</p>
</dd>
</dl>
</dd></dl>
<dl class="py attribute">
<dt class="sig sig-object py" id="NaMAZU.lightning_wingman.torch_gmm.GMM.covariance_type">
<span class="sig-name descname"><span class="pre">covariance_type</span></span><a class="headerlink" href="#NaMAZU.lightning_wingman.torch_gmm.GMM.covariance_type" title="Permalink to this definition">¶</a></dt>
<dd><p>Type of covariance, one of [“diag”, “full”].</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>str</p>
</dd>
</dl>
</dd></dl>
<dl class="py attribute">
<dt class="sig sig-object py" id="NaMAZU.lightning_wingman.torch_gmm.GMM.eps">
<span class="sig-name descname"><span class="pre">eps</span></span><a class="headerlink" href="#NaMAZU.lightning_wingman.torch_gmm.GMM.eps" title="Permalink to this definition">¶</a></dt>
<dd><p>Precision.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>float</p>
</dd>
</dl>
</dd></dl>
<dl class="py attribute">
<dt class="sig sig-object py" id="NaMAZU.lightning_wingman.torch_gmm.GMM.init_params">
<span class="sig-name descname"><span class="pre">init_params</span></span><a class="headerlink" href="#NaMAZU.lightning_wingman.torch_gmm.GMM.init_params" title="Permalink to this definition">¶</a></dt>
<dd><p>Method to init params, one of [“random”,”kmeans”].</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>str</p>
</dd>
</dl>
</dd></dl>
<dl class="py attribute">
<dt class="sig sig-object py" id="NaMAZU.lightning_wingman.torch_gmm.GMM.log_likelihood">
<span class="sig-name descname"><span class="pre">log_likelihood</span></span><a class="headerlink" href="#NaMAZU.lightning_wingman.torch_gmm.GMM.log_likelihood" title="Permalink to this definition">¶</a></dt>
<dd><p>Log-likelihood of the data.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>float</p>
</dd>
</dl>
</dd></dl>
<dl class="py attribute">
<dt class="sig sig-object py" id="NaMAZU.lightning_wingman.torch_gmm.GMM.n_components">
<span class="sig-name descname"><span class="pre">n_components</span></span><a class="headerlink" href="#NaMAZU.lightning_wingman.torch_gmm.GMM.n_components" title="Permalink to this definition">¶</a></dt>
<dd><p>Number of mixture components.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>int</p>
</dd>
</dl>
</dd></dl>
<dl class="py attribute">
<dt class="sig sig-object py" id="NaMAZU.lightning_wingman.torch_gmm.GMM.n_features">
<span class="sig-name descname"><span class="pre">n_features</span></span><a class="headerlink" href="#NaMAZU.lightning_wingman.torch_gmm.GMM.n_features" title="Permalink to this definition">¶</a></dt>
<dd><p>Number of features per sample.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>int</p>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt class="sig sig-object py" id="NaMAZU.lightning_wingman.torch_gmm.GMM.bic">
<span class="sig-name descname"><span class="pre">bic</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">torch.Tensor</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">→</span> <span class="sig-return-typehint"><span class="pre">float</span></span></span><a class="reference internal" href="_modules/NaMAZU/lightning_wingman/torch_gmm.html#GMM.bic"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#NaMAZU.lightning_wingman.torch_gmm.GMM.bic" title="Permalink to this definition">¶</a></dt>
<dd><p>Bayesian information criterion for a batch of samples.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>x</strong> (<em>Tensor</em>) – Samples of shape (n, d) or (n, 1, d).</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>BIC score.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>float</p>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt class="sig sig-object py" id="NaMAZU.lightning_wingman.torch_gmm.GMM.fit">
<span class="sig-name descname"><span class="pre">fit</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">torch.Tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">delta</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">float</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">0.001</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_iter</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">int</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">100</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">warm_start</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">→</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="reference internal" href="_modules/NaMAZU/lightning_wingman/torch_gmm.html#GMM.fit"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#NaMAZU.lightning_wingman.torch_gmm.GMM.fit" title="Permalink to this definition">¶</a></dt>
<dd><p>Fits a mixture of k=1,..,K Gaussians to the input data (K is supplied via n_components).</p>
<p>Input tensors are expected to be flat with dimensions (n: number of samples, d: number of features).
The model then extends them to (n, 1, d).
The model parametrization (mu, sigma) is stored as (1, k, d),
probabilities are shaped (n, k, 1) if they relate to an individual sample,
or (1, k, 1) if they assign membership probabilities to one of the mixture components.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>x</strong> (<em>Tensor</em>) – A tensor of shape (n, d) or (n, 1, d).</p></li>
<li><p><strong>delta</strong> (<em>float</em><em>, </em><em>optional</em>) – Delta param for EM algorithm. Defaults to 1e-3.</p></li>
<li><p><strong>n_iter</strong> (<em>int</em><em>, </em><em>optional</em>) – Number of iteration to fit. Defaults to 100.</p></li>
<li><p><strong>warm_start</strong> (<em>bool</em><em>, </em><em>optional</em>) – True to prevent initializing parameters. Defaults to False.</p></li>
</ul>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt class="sig sig-object py" id="NaMAZU.lightning_wingman.torch_gmm.GMM.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">probs</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">→</span> <span class="sig-return-typehint"><span class="pre">torch.Tensor</span></span></span><a class="reference internal" href="_modules/NaMAZU/lightning_wingman/torch_gmm.html#GMM.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#NaMAZU.lightning_wingman.torch_gmm.GMM.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Predict assignment of x to mixture components by calculating the
likelihood that each component is responsible for each point in x.
If prob is True, returns normalized probability of each class for each point.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>x</strong> (<em>Tensor</em>) – A tensor of shape (n, d) or (n, 1, d).</p></li>
<li><p><strong>probs</strong> (<em>bool</em><em>, </em><em>optional</em>) – True to get normalized probabilities for each class.
Defaults to False.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A tensor of shape (n) if probs is False, else (n, k)</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>Tensor</p>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt class="sig sig-object py" id="NaMAZU.lightning_wingman.torch_gmm.GMM.get_kmeans_mu">
<span class="sig-name descname"><span class="pre">get_kmeans_mu</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_centers</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">init_times</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">int</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">50</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">min_delta</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">float</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">0.001</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">→</span> <span class="sig-return-typehint"><span class="pre">torch.Tensor</span></span></span><a class="reference internal" href="_modules/NaMAZU/lightning_wingman/torch_gmm.html#GMM.get_kmeans_mu"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#NaMAZU.lightning_wingman.torch_gmm.GMM.get_kmeans_mu" title="Permalink to this definition">¶</a></dt>
<dd><p>Find an initial value for the mean.</p>
<p>Requires a threshold min_delta for the k-means algorithm to stop iterating.
The algorithm is repeated init_times often, after which the best centerpoint is returned.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>x</strong> (<em>Tensor</em>) – A tensor of shape of (n, d) or (n, 1, d).</p></li>
<li><p><strong>init_times</strong> (<em>int</em>) – Number of times to run the k-means algorithm.</p></li>
<li><p><strong>min_delta</strong> (<em>float</em>) – Minimum change in the loss function to stop the k-means algorithm.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A tensor of shape (1, k, d).</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>Tensor</p>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt class="sig sig-object py" id="NaMAZU.lightning_wingman.torch_gmm.GMM.predict_proba">
<span class="sig-name descname"><span class="pre">predict_proba</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">torch.Tensor</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">→</span> <span class="sig-return-typehint"><span class="pre">torch.Tensor</span></span></span><a class="reference internal" href="_modules/NaMAZU/lightning_wingman/torch_gmm.html#GMM.predict_proba"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#NaMAZU.lightning_wingman.torch_gmm.GMM.predict_proba" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns normalized probabilities of class membership.
:param x: A tensor of shape (n, d) or (n, 1, d).
:type x: Tensor</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>A tensor of shape (n)</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p>Tensor</p>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt class="sig sig-object py" id="NaMAZU.lightning_wingman.torch_gmm.GMM.score_samples">
<span class="sig-name descname"><span class="pre">score_samples</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">torch.Tensor</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">→</span> <span class="sig-return-typehint"><span class="pre">torch.Tensor</span></span></span><a class="reference internal" href="_modules/NaMAZU/lightning_wingman/torch_gmm.html#GMM.score_samples"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#NaMAZU.lightning_wingman.torch_gmm.GMM.score_samples" title="Permalink to this definition">¶</a></dt>
<dd><p>Return log-likelihood of data under the model with current params.
:param x: A tensor of shape (n, d) or (n, 1, d).
:type x: Tensor</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>A tensor of shape (n).</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p>Tensor</p>
</dd>
</dl>
</dd></dl>
<dl class="py attribute">
<dt class="sig sig-object py" id="NaMAZU.lightning_wingman.torch_gmm.GMM.training">
<span class="sig-name descname"><span class="pre">training</span></span><em class="property"><span class="pre">:</span> <span class="pre">bool</span></em><a class="headerlink" href="#NaMAZU.lightning_wingman.torch_gmm.GMM.training" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>
</dd></dl>


<span id="namazu-lightning-wingman-torch-knn-module"></span><h2 id="module-NaMAZU.lightning_wingman.torch_knn">NaMAZU.lightning_wingman.torch_knn module<a class="headerlink" href="#module-NaMAZU.lightning_wingman.torch_knn" title="Permalink to this headline">¶</a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="NaMAZU.lightning_wingman.torch_knn.KNN">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">NaMAZU.lightning_wingman.torch_knn.</span></span><span class="sig-name descname"><span class="pre">KNN</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">n_neighbors</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">training_data</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">torch.Tensor</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">tensor([])</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">distance_measure</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">str</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">'euclidean'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">training_labels</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">torch.Tensor</span><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/NaMAZU/lightning_wingman/torch_knn.html#KNN"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#NaMAZU.lightning_wingman.torch_knn.KNN" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.module.Module</span></code></p>
<dl class="py method">
<dt class="sig sig-object py" id="NaMAZU.lightning_wingman.torch_knn.KNN.fit">
<span class="sig-name descname"><span class="pre">fit</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">training_data</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">torch.Tensor</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">→</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="reference internal" href="_modules/NaMAZU/lightning_wingman/torch_knn.html#KNN.fit"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#NaMAZU.lightning_wingman.torch_knn.KNN.fit" title="Permalink to this definition">¶</a></dt>
<dd><p>Train KNN.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>training_data</strong> (<em>Tensor</em>) – Training data.</p>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt class="sig sig-object py" id="NaMAZU.lightning_wingman.torch_knn.KNN.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">torch.Tensor</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">→</span> <span class="sig-return-typehint"><span class="pre">torch.Tensor</span></span></span><a class="reference internal" href="_modules/NaMAZU/lightning_wingman/torch_knn.html#KNN.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#NaMAZU.lightning_wingman.torch_knn.KNN.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Return the indices of the k nearest neighbors of x.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>x</strong> (<em>Tensor</em>) – input point.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>N by K tensor of k indices of the k nearest neighbors of x.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>Tensor</p>
</dd>
<dt class="field-even">Raises</dt>
<dd class="field-even"><p><strong>ValueError</strong> – Raise if the model has not been fit.</p>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt class="sig sig-object py" id="NaMAZU.lightning_wingman.torch_knn.KNN.get_k_nearest_labels">
<span class="sig-name descname"><span class="pre">get_k_nearest_labels</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">torch.Tensor</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">→</span> <span class="sig-return-typehint"><span class="pre">torch.Tensor</span></span></span><a class="reference internal" href="_modules/NaMAZU/lightning_wingman/torch_knn.html#KNN.get_k_nearest_labels"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#NaMAZU.lightning_wingman.torch_knn.KNN.get_k_nearest_labels" title="Permalink to this definition">¶</a></dt>
<dd><p>Return the k nearest labels of x.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>x</strong> (<em>Tensor</em>) – input point.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>1d tensor of k nearest labels of x.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>Tensor</p>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt class="sig sig-object py" id="NaMAZU.lightning_wingman.torch_knn.KNN.get_k_nearest_neighbors">
<span class="sig-name descname"><span class="pre">get_k_nearest_neighbors</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">torch.Tensor</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">→</span> <span class="sig-return-typehint"><span class="pre">torch.Tensor</span></span></span><a class="reference internal" href="_modules/NaMAZU/lightning_wingman/torch_knn.html#KNN.get_k_nearest_neighbors"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#NaMAZU.lightning_wingman.torch_knn.KNN.get_k_nearest_neighbors" title="Permalink to this definition">¶</a></dt>
<dd><p>Return the k nearest neighbors of x.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>x</strong> (<em>Tensor</em>) – input point.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>1d tensor of k nearest neighbors of x.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>Tensor</p>
</dd>
</dl>
</dd></dl>
<dl class="py attribute">
<dt class="sig sig-object py" id="NaMAZU.lightning_wingman.torch_knn.KNN.training">
<span class="sig-name descname"><span class="pre">training</span></span><em class="property"><span class="pre">:</span> <span class="pre">bool</span></em><a class="headerlink" href="#NaMAZU.lightning_wingman.torch_knn.KNN.training" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>
</dd></dl>


<span id="namazu-lightning-wingman-torch-nbc-module"></span><h2 id="module-NaMAZU.lightning_wingman.torch_nbc">NaMAZU.lightning_wingman.torch_nbc module<a class="headerlink" href="#module-NaMAZU.lightning_wingman.torch_nbc" title="Permalink to this headline">¶</a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="NaMAZU.lightning_wingman.torch_nbc.NBC">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">NaMAZU.lightning_wingman.torch_nbc.</span></span><span class="sig-name descname"><span class="pre">NBC</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">offset</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">int</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">training_data</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">torch.Tensor</span><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">training_labels</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">torch.Tensor</span><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">is_categorical</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">bool</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/NaMAZU/lightning_wingman/torch_nbc.html#NBC"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#NaMAZU.lightning_wingman.torch_nbc.NBC" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.module.Module</span></code></p>
<p>Naive Bayes Classifier.</p>
<p>Initializer takes optional training data for auto training.</p>
<dl class="py attribute">
<dt class="sig sig-object py" id="NaMAZU.lightning_wingman.torch_nbc.NBC.offset">
<span class="sig-name descname"><span class="pre">offset</span></span><a class="headerlink" href="#NaMAZU.lightning_wingman.torch_nbc.NBC.offset" title="Permalink to this definition">¶</a></dt>
<dd><p>An integer to increment the conditional probabilities
in order to smooth probabilities to avoid that a posterior
probability be 0.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>int</p>
</dd>
</dl>
</dd></dl>
<dl class="py attribute">
<dt class="sig sig-object py" id="NaMAZU.lightning_wingman.torch_nbc.NBC.is_categorical">
<span class="sig-name descname"><span class="pre">is_categorical</span></span><a class="headerlink" href="#NaMAZU.lightning_wingman.torch_nbc.NBC.is_categorical" title="Permalink to this definition">¶</a></dt>
<dd><p>A list of boolean for indicating if a feature is categorical or numerical.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>List[bool]</p>
</dd>
</dl>
</dd></dl>
<dl class="py attribute">
<dt class="sig sig-object py" id="NaMAZU.lightning_wingman.torch_nbc.NBC.nb_features">
<span class="sig-name descname"><span class="pre">nb_features</span></span><a class="headerlink" href="#NaMAZU.lightning_wingman.torch_nbc.NBC.nb_features" title="Permalink to this definition">¶</a></dt>
<dd><p>An integer for the numbers of feature of the data.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>int</p>
</dd>
</dl>
</dd></dl>
<dl class="py attribute">
<dt class="sig sig-object py" id="NaMAZU.lightning_wingman.torch_nbc.NBC.nb_class">
<span class="sig-name descname"><span class="pre">nb_class</span></span><a class="headerlink" href="#NaMAZU.lightning_wingman.torch_nbc.NBC.nb_class" title="Permalink to this definition">¶</a></dt>
<dd><p>An integer for the number of classes in the labels.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>int</p>
</dd>
</dl>
</dd></dl>
<dl class="py attribute">
<dt class="sig sig-object py" id="NaMAZU.lightning_wingman.torch_nbc.NBC.class_probs">
<span class="sig-name descname"><span class="pre">class_probs</span></span><a class="headerlink" href="#NaMAZU.lightning_wingman.torch_nbc.NBC.class_probs" title="Permalink to this definition">¶</a></dt>
<dd><p>A torch tensor for the proportion of each class.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>Tensor</p>
</dd>
</dl>
</dd></dl>
<dl class="py attribute">
<dt class="sig sig-object py" id="NaMAZU.lightning_wingman.torch_nbc.NBC.cond_probs">
<span class="sig-name descname"><span class="pre">cond_probs</span></span><a class="headerlink" href="#NaMAZU.lightning_wingman.torch_nbc.NBC.cond_probs" title="Permalink to this definition">¶</a></dt>
<dd><p>A torch tensor for the conditional probability of
having a given value on a certain feature in the population
of each class.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>Tensor</p>
</dd>
</dl>
</dd></dl>
<dl class="py attribute">
<dt class="sig sig-object py" id="NaMAZU.lightning_wingman.torch_nbc.NBC.pi">
<span class="sig-name descname"><span class="pre">pi</span></span><a class="headerlink" href="#NaMAZU.lightning_wingman.torch_nbc.NBC.pi" title="Permalink to this definition">¶</a></dt>
<dd><p>A torch tensor for the value of pi.</p>
</dd></dl>
<dl class="py method">
<dt class="sig sig-object py" id="NaMAZU.lightning_wingman.torch_nbc.NBC.fit">
<span class="sig-name descname"><span class="pre">fit</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">torch.Tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">torch.Tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">is_categorical</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">bool</span><span class="p"><span class="pre">]</span></span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/NaMAZU/lightning_wingman/torch_nbc.html#NBC.fit"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#NaMAZU.lightning_wingman.torch_nbc.NBC.fit" title="Permalink to this definition">¶</a></dt>
<dd><p>Fits the model given data and labels as input</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>X</strong> (<em>Tensor</em>) – A torch tensor containing a batch of data.</p></li>
<li><p><strong>y</strong> (<em>Tensor</em>) – A torch tensor containing a batch of labels.</p></li>
<li><p><strong>is_categorical</strong> (<em>Tensor</em>) – A list of boolean for indicating if a feature is categorical or numerical.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>0 if the model is fitted</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>int</p>
</dd>
<dt class="field-even">Raises</dt>
<dd class="field-even"><p><strong>ValueError</strong> – If the number of features is different from the number of features in X.</p>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt class="sig sig-object py" id="NaMAZU.lightning_wingman.torch_nbc.NBC.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/NaMAZU/lightning_wingman/torch_nbc.html#NBC.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#NaMAZU.lightning_wingman.torch_nbc.NBC.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Predicts labels given an input</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>X</strong> – A torch tensor containing a batch of data.</p>
</dd>
</dl>
</dd></dl>
<dl class="py attribute">
<dt class="sig sig-object py" id="NaMAZU.lightning_wingman.torch_nbc.NBC.training">
<span class="sig-name descname"><span class="pre">training</span></span><em class="property"><span class="pre">:</span> <span class="pre">bool</span></em><a class="headerlink" href="#NaMAZU.lightning_wingman.torch_nbc.NBC.training" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>
</dd></dl>


<span id="namazu-lightning-wingman-u2net-module"></span><h2 id="module-NaMAZU.lightning_wingman.u2net">NaMAZU.lightning_wingman.u2net module<a class="headerlink" href="#module-NaMAZU.lightning_wingman.u2net" title="Permalink to this headline">¶</a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="NaMAZU.lightning_wingman.u2net.REBNCONV">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">NaMAZU.lightning_wingman.u2net.</span></span><span class="sig-name descname"><span class="pre">REBNCONV</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">in_ch</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">3</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">out_ch</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">3</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dirate</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/NaMAZU/lightning_wingman/u2net.html#REBNCONV"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#NaMAZU.lightning_wingman.u2net.REBNCONV" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.module.Module</span></code></p>
<dl class="py method">
<dt class="sig sig-object py" id="NaMAZU.lightning_wingman.u2net.REBNCONV.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/NaMAZU/lightning_wingman/u2net.html#REBNCONV.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#NaMAZU.lightning_wingman.u2net.REBNCONV.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>
<dl class="py attribute">
<dt class="sig sig-object py" id="NaMAZU.lightning_wingman.u2net.REBNCONV.training">
<span class="sig-name descname"><span class="pre">training</span></span><em class="property"><span class="pre">:</span> <span class="pre">bool</span></em><a class="headerlink" href="#NaMAZU.lightning_wingman.u2net.REBNCONV.training" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>
</dd></dl>
<dl class="py class">
<dt class="sig sig-object py" id="NaMAZU.lightning_wingman.u2net.RSU4">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">NaMAZU.lightning_wingman.u2net.</span></span><span class="sig-name descname"><span class="pre">RSU4</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">in_ch</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">3</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mid_ch</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">12</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">out_ch</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">3</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/NaMAZU/lightning_wingman/u2net.html#RSU4"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#NaMAZU.lightning_wingman.u2net.RSU4" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.module.Module</span></code></p>
<dl class="py method">
<dt class="sig sig-object py" id="NaMAZU.lightning_wingman.u2net.RSU4.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/NaMAZU/lightning_wingman/u2net.html#RSU4.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#NaMAZU.lightning_wingman.u2net.RSU4.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>
<dl class="py attribute">
<dt class="sig sig-object py" id="NaMAZU.lightning_wingman.u2net.RSU4.training">
<span class="sig-name descname"><span class="pre">training</span></span><em class="property"><span class="pre">:</span> <span class="pre">bool</span></em><a class="headerlink" href="#NaMAZU.lightning_wingman.u2net.RSU4.training" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>
</dd></dl>
<dl class="py class">
<dt class="sig sig-object py" id="NaMAZU.lightning_wingman.u2net.RSU4F">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">NaMAZU.lightning_wingman.u2net.</span></span><span class="sig-name descname"><span class="pre">RSU4F</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">in_ch</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">3</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mid_ch</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">12</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">out_ch</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">3</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/NaMAZU/lightning_wingman/u2net.html#RSU4F"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#NaMAZU.lightning_wingman.u2net.RSU4F" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.module.Module</span></code></p>
<dl class="py method">
<dt class="sig sig-object py" id="NaMAZU.lightning_wingman.u2net.RSU4F.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/NaMAZU/lightning_wingman/u2net.html#RSU4F.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#NaMAZU.lightning_wingman.u2net.RSU4F.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>
<dl class="py attribute">
<dt class="sig sig-object py" id="NaMAZU.lightning_wingman.u2net.RSU4F.training">
<span class="sig-name descname"><span class="pre">training</span></span><em class="property"><span class="pre">:</span> <span class="pre">bool</span></em><a class="headerlink" href="#NaMAZU.lightning_wingman.u2net.RSU4F.training" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>
</dd></dl>
<dl class="py class">
<dt class="sig sig-object py" id="NaMAZU.lightning_wingman.u2net.RSU5">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">NaMAZU.lightning_wingman.u2net.</span></span><span class="sig-name descname"><span class="pre">RSU5</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">in_ch</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">3</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mid_ch</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">12</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">out_ch</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">3</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/NaMAZU/lightning_wingman/u2net.html#RSU5"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#NaMAZU.lightning_wingman.u2net.RSU5" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.module.Module</span></code></p>
<dl class="py method">
<dt class="sig sig-object py" id="NaMAZU.lightning_wingman.u2net.RSU5.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/NaMAZU/lightning_wingman/u2net.html#RSU5.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#NaMAZU.lightning_wingman.u2net.RSU5.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>
<dl class="py attribute">
<dt class="sig sig-object py" id="NaMAZU.lightning_wingman.u2net.RSU5.training">
<span class="sig-name descname"><span class="pre">training</span></span><em class="property"><span class="pre">:</span> <span class="pre">bool</span></em><a class="headerlink" href="#NaMAZU.lightning_wingman.u2net.RSU5.training" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>
</dd></dl>
<dl class="py class">
<dt class="sig sig-object py" id="NaMAZU.lightning_wingman.u2net.RSU6">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">NaMAZU.lightning_wingman.u2net.</span></span><span class="sig-name descname"><span class="pre">RSU6</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">in_ch</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">3</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mid_ch</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">12</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">out_ch</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">3</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/NaMAZU/lightning_wingman/u2net.html#RSU6"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#NaMAZU.lightning_wingman.u2net.RSU6" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.module.Module</span></code></p>
<dl class="py method">
<dt class="sig sig-object py" id="NaMAZU.lightning_wingman.u2net.RSU6.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/NaMAZU/lightning_wingman/u2net.html#RSU6.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#NaMAZU.lightning_wingman.u2net.RSU6.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>
<dl class="py attribute">
<dt class="sig sig-object py" id="NaMAZU.lightning_wingman.u2net.RSU6.training">
<span class="sig-name descname"><span class="pre">training</span></span><em class="property"><span class="pre">:</span> <span class="pre">bool</span></em><a class="headerlink" href="#NaMAZU.lightning_wingman.u2net.RSU6.training" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>
</dd></dl>
<dl class="py class">
<dt class="sig sig-object py" id="NaMAZU.lightning_wingman.u2net.RSU7">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">NaMAZU.lightning_wingman.u2net.</span></span><span class="sig-name descname"><span class="pre">RSU7</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">in_ch</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">3</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mid_ch</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">12</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">out_ch</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">3</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/NaMAZU/lightning_wingman/u2net.html#RSU7"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#NaMAZU.lightning_wingman.u2net.RSU7" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.module.Module</span></code></p>
<dl class="py method">
<dt class="sig sig-object py" id="NaMAZU.lightning_wingman.u2net.RSU7.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/NaMAZU/lightning_wingman/u2net.html#RSU7.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#NaMAZU.lightning_wingman.u2net.RSU7.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>
<dl class="py attribute">
<dt class="sig sig-object py" id="NaMAZU.lightning_wingman.u2net.RSU7.training">
<span class="sig-name descname"><span class="pre">training</span></span><em class="property"><span class="pre">:</span> <span class="pre">bool</span></em><a class="headerlink" href="#NaMAZU.lightning_wingman.u2net.RSU7.training" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>
</dd></dl>
<dl class="py class">
<dt class="sig sig-object py" id="NaMAZU.lightning_wingman.u2net.RandomCrop">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">NaMAZU.lightning_wingman.u2net.</span></span><span class="sig-name descname"><span class="pre">RandomCrop</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">output_size</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/NaMAZU/lightning_wingman/u2net.html#RandomCrop"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#NaMAZU.lightning_wingman.u2net.RandomCrop" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></p>
</dd></dl>
<dl class="py class">
<dt class="sig sig-object py" id="NaMAZU.lightning_wingman.u2net.Rescale">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">NaMAZU.lightning_wingman.u2net.</span></span><span class="sig-name descname"><span class="pre">Rescale</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">output_size</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/NaMAZU/lightning_wingman/u2net.html#Rescale"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#NaMAZU.lightning_wingman.u2net.Rescale" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></p>
</dd></dl>
<dl class="py class">
<dt class="sig sig-object py" id="NaMAZU.lightning_wingman.u2net.RescaleT">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">NaMAZU.lightning_wingman.u2net.</span></span><span class="sig-name descname"><span class="pre">RescaleT</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">output_size</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/NaMAZU/lightning_wingman/u2net.html#RescaleT"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#NaMAZU.lightning_wingman.u2net.RescaleT" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></p>
</dd></dl>
<dl class="py class">
<dt class="sig sig-object py" id="NaMAZU.lightning_wingman.u2net.SalObjDataset">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">NaMAZU.lightning_wingman.u2net.</span></span><span class="sig-name descname"><span class="pre">SalObjDataset</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">img_name_list</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">lbl_name_list</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">transform</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/NaMAZU/lightning_wingman/u2net.html#SalObjDataset"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#NaMAZU.lightning_wingman.u2net.SalObjDataset" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">Generic</span></code>[<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.utils.data.dataset.T_co</span></code>]</p>
</dd></dl>
<dl class="py class">
<dt class="sig sig-object py" id="NaMAZU.lightning_wingman.u2net.ToTensor">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">NaMAZU.lightning_wingman.u2net.</span></span><span class="sig-name descname"><span class="pre">ToTensor</span></span><a class="reference internal" href="_modules/NaMAZU/lightning_wingman/u2net.html#ToTensor"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#NaMAZU.lightning_wingman.u2net.ToTensor" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></p>
<p>Convert ndarrays in sample to Tensors.</p>
</dd></dl>
<dl class="py class">
<dt class="sig sig-object py" id="NaMAZU.lightning_wingman.u2net.ToTensorLab">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">NaMAZU.lightning_wingman.u2net.</span></span><span class="sig-name descname"><span class="pre">ToTensorLab</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">flag</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/NaMAZU/lightning_wingman/u2net.html#ToTensorLab"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#NaMAZU.lightning_wingman.u2net.ToTensorLab" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></p>
<p>Convert ndarrays in sample to Tensors.</p>
</dd></dl>
<dl class="py class">
<dt class="sig sig-object py" id="NaMAZU.lightning_wingman.u2net.U2NET">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">NaMAZU.lightning_wingman.u2net.</span></span><span class="sig-name descname"><span class="pre">U2NET</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">in_ch</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">3</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">out_ch</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/NaMAZU/lightning_wingman/u2net.html#U2NET"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#NaMAZU.lightning_wingman.u2net.U2NET" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.module.Module</span></code></p>
<dl class="py method">
<dt class="sig sig-object py" id="NaMAZU.lightning_wingman.u2net.U2NET.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/NaMAZU/lightning_wingman/u2net.html#U2NET.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#NaMAZU.lightning_wingman.u2net.U2NET.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>
<dl class="py attribute">
<dt class="sig sig-object py" id="NaMAZU.lightning_wingman.u2net.U2NET.training">
<span class="sig-name descname"><span class="pre">training</span></span><em class="property"><span class="pre">:</span> <span class="pre">bool</span></em><a class="headerlink" href="#NaMAZU.lightning_wingman.u2net.U2NET.training" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>
</dd></dl>
<dl class="py class">
<dt class="sig sig-object py" id="NaMAZU.lightning_wingman.u2net.U2NETP">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">NaMAZU.lightning_wingman.u2net.</span></span><span class="sig-name descname"><span class="pre">U2NETP</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">in_ch</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">3</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">out_ch</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/NaMAZU/lightning_wingman/u2net.html#U2NETP"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#NaMAZU.lightning_wingman.u2net.U2NETP" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.module.Module</span></code></p>
<dl class="py method">
<dt class="sig sig-object py" id="NaMAZU.lightning_wingman.u2net.U2NETP.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/NaMAZU/lightning_wingman/u2net.html#U2NETP.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#NaMAZU.lightning_wingman.u2net.U2NETP.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>
<dl class="py attribute">
<dt class="sig sig-object py" id="NaMAZU.lightning_wingman.u2net.U2NETP.training">
<span class="sig-name descname"><span class="pre">training</span></span><em class="property"><span class="pre">:</span> <span class="pre">bool</span></em><a class="headerlink" href="#NaMAZU.lightning_wingman.u2net.U2NETP.training" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>
</dd></dl>


<span id="namazu-lightning-wingman-video-clf-module"></span><h2 id="module-NaMAZU.lightning_wingman.video_clf">NaMAZU.lightning_wingman.video_clf module<a class="headerlink" href="#module-NaMAZU.lightning_wingman.video_clf" title="Permalink to this headline">¶</a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="NaMAZU.lightning_wingman.video_clf.CNNClassifier">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">NaMAZU.lightning_wingman.video_clf.</span></span><span class="sig-name descname"><span class="pre">CNNClassifier</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">num_classes</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">latent_dim</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">cnn</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">str</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">'resnet152d'</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/NaMAZU/lightning_wingman/video_clf.html#CNNClassifier"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#NaMAZU.lightning_wingman.video_clf.CNNClassifier" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.module.Module</span></code></p>
<p>Simple CNN classifier</p>
<dl class="py method">
<dt class="sig sig-object py" id="NaMAZU.lightning_wingman.video_clf.CNNClassifier.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">torch.Tensor</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">→</span> <span class="sig-return-typehint"><span class="pre">torch.Tensor</span></span></span><a class="reference internal" href="_modules/NaMAZU/lightning_wingman/video_clf.html#CNNClassifier.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#NaMAZU.lightning_wingman.video_clf.CNNClassifier.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>
<dl class="py attribute">
<dt class="sig sig-object py" id="NaMAZU.lightning_wingman.video_clf.CNNClassifier.training">
<span class="sig-name descname"><span class="pre">training</span></span><em class="property"><span class="pre">:</span> <span class="pre">bool</span></em><a class="headerlink" href="#NaMAZU.lightning_wingman.video_clf.CNNClassifier.training" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>
</dd></dl>
<dl class="py class">
<dt class="sig sig-object py" id="NaMAZU.lightning_wingman.video_clf.CNNLSTM">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">NaMAZU.lightning_wingman.video_clf.</span></span><span class="sig-name descname"><span class="pre">CNNLSTM</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">num_classes</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">latent_dim</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">int</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">512</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">cnn</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">str</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">'resnet152d'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">lstm_layers</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">int</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">hidden_dim</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">int</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">1024</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">bidirectional</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">bool</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">attention</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">bool</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/NaMAZU/lightning_wingman/video_clf.html#CNNLSTM"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#NaMAZU.lightning_wingman.video_clf.CNNLSTM" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.module.Module</span></code></p>
<dl class="py method">
<dt class="sig sig-object py" id="NaMAZU.lightning_wingman.video_clf.CNNLSTM.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/NaMAZU/lightning_wingman/video_clf.html#CNNLSTM.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#NaMAZU.lightning_wingman.video_clf.CNNLSTM.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>
<dl class="py attribute">
<dt class="sig sig-object py" id="NaMAZU.lightning_wingman.video_clf.CNNLSTM.training">
<span class="sig-name descname"><span class="pre">training</span></span><em class="property"><span class="pre">:</span> <span class="pre">bool</span></em><a class="headerlink" href="#NaMAZU.lightning_wingman.video_clf.CNNLSTM.training" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>
</dd></dl>


<span id="module-contents"></span><h2 id="module-NaMAZU.lightning_wingman">Module contents<a class="headerlink" href="#module-NaMAZU.lightning_wingman" title="Permalink to this headline">¶</a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="NaMAZU.lightning_wingman.AniNet">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">NaMAZU.lightning_wingman.</span></span><span class="sig-name descname"><span class="pre">AniNet</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">choice</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">str</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">'50'</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/NaMAZU/lightning_wingman/lit_aninet.html#AniNet"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#NaMAZU.lightning_wingman.AniNet" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">pytorch_lightning.core.lightning.LightningModule</span></code></p>
<dl class="py method">
<dt class="sig sig-object py" id="NaMAZU.lightning_wingman.AniNet.calc_result">
<span class="sig-name descname"><span class="pre">calc_result</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">probs</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">torch.Tensor</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">thresh</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.3</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">→</span> <span class="sig-return-typehint"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span></span></span><a class="reference internal" href="_modules/NaMAZU/lightning_wingman/lit_aninet.html#AniNet.calc_result"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#NaMAZU.lightning_wingman.AniNet.calc_result" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>
<dl class="py method">
<dt class="sig sig-object py" id="NaMAZU.lightning_wingman.AniNet.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">torch.Tensor</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">→</span> <span class="sig-return-typehint"><span class="pre">torch.Tensor</span></span></span><a class="reference internal" href="_modules/NaMAZU/lightning_wingman/lit_aninet.html#AniNet.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#NaMAZU.lightning_wingman.AniNet.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Same as <code class="xref py py-meth docutils literal notranslate"><span class="pre">torch.nn.Module.forward()</span></code>, however in Lightning you want this to define
the operations you want to use for prediction (i.e.: on a server or as a feature extractor).</p>
<p>Normally you’d call <code class="docutils literal notranslate"><span class="pre">self()</span></code> from your <code class="xref py py-meth docutils literal notranslate"><span class="pre">training_step()</span></code> method.
This makes it easy to write a complex system for training with the outputs
you’d want in a prediction setting.</p>
<p>You may also find the <code class="xref py py-func docutils literal notranslate"><span class="pre">auto_move_data()</span></code> decorator useful
when using the module outside Lightning in a production setting.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>*args</strong> – Whatever you decide to pass into the forward method.</p></li>
<li><p><strong>**kwargs</strong> – Keyword arguments are also possible.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Predicted output</p>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># example if we were using this model as a feature extractor</span>
<span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
    <span class="n">feature_maps</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">convnet</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">feature_maps</span>

<span class="k">def</span> <span class="nf">training_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">):</span>
    <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">batch</span>
    <span class="n">feature_maps</span> <span class="o">=</span> <span class="bp">self</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">logits</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">classifier</span><span class="p">(</span><span class="n">feature_maps</span><span class="p">)</span>

    <span class="c1"># ...</span>
    <span class="k">return</span> <span class="n">loss</span>

<span class="c1"># splitting it this way allows model to be used a feature extractor</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">MyModelAbove</span><span class="p">()</span>

<span class="n">inputs</span> <span class="o">=</span> <span class="n">server</span><span class="o">.</span><span class="n">get_request</span><span class="p">()</span>
<span class="n">results</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
<span class="n">server</span><span class="o">.</span><span class="n">write_results</span><span class="p">(</span><span class="n">results</span><span class="p">)</span>

<span class="c1"># -------------</span>
<span class="c1"># This is in stark contrast to torch.nn.Module where normally you would have this:</span>
<span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">):</span>
    <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">batch</span>
    <span class="n">feature_maps</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">convnet</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">logits</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">classifier</span><span class="p">(</span><span class="n">feature_maps</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">logits</span>
</pre></div>
</div>
</dd></dl>
<dl class="py method">
<dt class="sig sig-object py" id="NaMAZU.lightning_wingman.AniNet.predict">
<span class="sig-name descname"><span class="pre">predict</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">image_path</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span> </span><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">→</span> <span class="sig-return-typehint"><span class="pre">torch.Tensor</span></span></span><a class="reference internal" href="_modules/NaMAZU/lightning_wingman/lit_aninet.html#AniNet.predict"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#NaMAZU.lightning_wingman.AniNet.predict" title="Permalink to this definition">¶</a></dt>
<dd><p>Use this function with trainer.predict(…). Override if you need to add any processing logic.</p>
</dd></dl>
<dl class="py method">
<dt class="sig sig-object py" id="NaMAZU.lightning_wingman.AniNet.predict_probs">
<span class="sig-name descname"><span class="pre">predict_probs</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">outputs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/NaMAZU/lightning_wingman/lit_aninet.html#AniNet.predict_probs"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#NaMAZU.lightning_wingman.AniNet.predict_probs" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>
<dl class="py method">
<dt class="sig sig-object py" id="NaMAZU.lightning_wingman.AniNet.run_batch_prediction">
<span class="sig-name descname"><span class="pre">run_batch_prediction</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">image_dir</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">str</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/NaMAZU/lightning_wingman/lit_aninet.html#AniNet.run_batch_prediction"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#NaMAZU.lightning_wingman.AniNet.run_batch_prediction" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>
<dl class="py attribute">
<dt class="sig sig-object py" id="NaMAZU.lightning_wingman.AniNet.training">
<span class="sig-name descname"><span class="pre">training</span></span><em class="property"><span class="pre">:</span> <span class="pre">bool</span></em><a class="headerlink" href="#NaMAZU.lightning_wingman.AniNet.training" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>
</dd></dl>
<dl class="py class">
<dt class="sig sig-object py" id="NaMAZU.lightning_wingman.GMM">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">NaMAZU.lightning_wingman.</span></span><span class="sig-name descname"><span class="pre">GMM</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">n_components</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_features</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">covariance_type</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">str</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">'full'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">eps</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">float</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">1e-06</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">init_params</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">str</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">'kmeans'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mu_init</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">torch.Tensor</span><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">var_init</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">torch.Tensor</span><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/NaMAZU/lightning_wingman/torch_gmm.html#GMM"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#NaMAZU.lightning_wingman.GMM" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.module.Module</span></code></p>
<p>PyTorch implementation of Gauusian Mixture Model with pytorch lightning support.</p>
<dl class="py attribute">
<dt class="sig sig-object py" id="NaMAZU.lightning_wingman.GMM.var">
<span class="sig-name descname"><span class="pre">var</span></span><a class="headerlink" href="#NaMAZU.lightning_wingman.GMM.var" title="Permalink to this definition">¶</a></dt>
<dd><p>Variance of the Gaussian distribution.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>Tensor</p>
</dd>
</dl>
</dd></dl>
<dl class="py attribute">
<dt class="sig sig-object py" id="NaMAZU.lightning_wingman.GMM.mu">
<span class="sig-name descname"><span class="pre">mu</span></span><a class="headerlink" href="#NaMAZU.lightning_wingman.GMM.mu" title="Permalink to this definition">¶</a></dt>
<dd><p>Mean of the Gaussian distribution.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>Tensor</p>
</dd>
</dl>
</dd></dl>
<dl class="py attribute">
<dt class="sig sig-object py" id="NaMAZU.lightning_wingman.GMM.pi">
<span class="sig-name descname"><span class="pre">pi</span></span><a class="headerlink" href="#NaMAZU.lightning_wingman.GMM.pi" title="Permalink to this definition">¶</a></dt>
<dd><p>Weight of the Gaussian distribution.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>Tensor</p>
</dd>
</dl>
</dd></dl>
<dl class="py attribute">
<dt class="sig sig-object py" id="NaMAZU.lightning_wingman.GMM.covariance_type">
<span class="sig-name descname"><span class="pre">covariance_type</span></span><a class="headerlink" href="#NaMAZU.lightning_wingman.GMM.covariance_type" title="Permalink to this definition">¶</a></dt>
<dd><p>Type of covariance, one of [“diag”, “full”].</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>str</p>
</dd>
</dl>
</dd></dl>
<dl class="py attribute">
<dt class="sig sig-object py" id="NaMAZU.lightning_wingman.GMM.eps">
<span class="sig-name descname"><span class="pre">eps</span></span><a class="headerlink" href="#NaMAZU.lightning_wingman.GMM.eps" title="Permalink to this definition">¶</a></dt>
<dd><p>Precision.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>float</p>
</dd>
</dl>
</dd></dl>
<dl class="py attribute">
<dt class="sig sig-object py" id="NaMAZU.lightning_wingman.GMM.init_params">
<span class="sig-name descname"><span class="pre">init_params</span></span><a class="headerlink" href="#NaMAZU.lightning_wingman.GMM.init_params" title="Permalink to this definition">¶</a></dt>
<dd><p>Method to init params, one of [“random”,”kmeans”].</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>str</p>
</dd>
</dl>
</dd></dl>
<dl class="py attribute">
<dt class="sig sig-object py" id="NaMAZU.lightning_wingman.GMM.log_likelihood">
<span class="sig-name descname"><span class="pre">log_likelihood</span></span><a class="headerlink" href="#NaMAZU.lightning_wingman.GMM.log_likelihood" title="Permalink to this definition">¶</a></dt>
<dd><p>Log-likelihood of the data.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>float</p>
</dd>
</dl>
</dd></dl>
<dl class="py attribute">
<dt class="sig sig-object py" id="NaMAZU.lightning_wingman.GMM.n_components">
<span class="sig-name descname"><span class="pre">n_components</span></span><a class="headerlink" href="#NaMAZU.lightning_wingman.GMM.n_components" title="Permalink to this definition">¶</a></dt>
<dd><p>Number of mixture components.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>int</p>
</dd>
</dl>
</dd></dl>
<dl class="py attribute">
<dt class="sig sig-object py" id="NaMAZU.lightning_wingman.GMM.n_features">
<span class="sig-name descname"><span class="pre">n_features</span></span><a class="headerlink" href="#NaMAZU.lightning_wingman.GMM.n_features" title="Permalink to this definition">¶</a></dt>
<dd><p>Number of features per sample.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>int</p>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt class="sig sig-object py" id="NaMAZU.lightning_wingman.GMM.bic">
<span class="sig-name descname"><span class="pre">bic</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">torch.Tensor</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">→</span> <span class="sig-return-typehint"><span class="pre">float</span></span></span><a class="reference internal" href="_modules/NaMAZU/lightning_wingman/torch_gmm.html#GMM.bic"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#NaMAZU.lightning_wingman.GMM.bic" title="Permalink to this definition">¶</a></dt>
<dd><p>Bayesian information criterion for a batch of samples.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>x</strong> (<em>Tensor</em>) – Samples of shape (n, d) or (n, 1, d).</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>BIC score.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>float</p>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt class="sig sig-object py" id="NaMAZU.lightning_wingman.GMM.fit">
<span class="sig-name descname"><span class="pre">fit</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">torch.Tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">delta</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">float</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">0.001</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_iter</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">int</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">100</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">warm_start</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">→</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="reference internal" href="_modules/NaMAZU/lightning_wingman/torch_gmm.html#GMM.fit"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#NaMAZU.lightning_wingman.GMM.fit" title="Permalink to this definition">¶</a></dt>
<dd><p>Fits a mixture of k=1,..,K Gaussians to the input data (K is supplied via n_components).</p>
<p>Input tensors are expected to be flat with dimensions (n: number of samples, d: number of features).
The model then extends them to (n, 1, d).
The model parametrization (mu, sigma) is stored as (1, k, d),
probabilities are shaped (n, k, 1) if they relate to an individual sample,
or (1, k, 1) if they assign membership probabilities to one of the mixture components.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>x</strong> (<em>Tensor</em>) – A tensor of shape (n, d) or (n, 1, d).</p></li>
<li><p><strong>delta</strong> (<em>float</em><em>, </em><em>optional</em>) – Delta param for EM algorithm. Defaults to 1e-3.</p></li>
<li><p><strong>n_iter</strong> (<em>int</em><em>, </em><em>optional</em>) – Number of iteration to fit. Defaults to 100.</p></li>
<li><p><strong>warm_start</strong> (<em>bool</em><em>, </em><em>optional</em>) – True to prevent initializing parameters. Defaults to False.</p></li>
</ul>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt class="sig sig-object py" id="NaMAZU.lightning_wingman.GMM.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">probs</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">→</span> <span class="sig-return-typehint"><span class="pre">torch.Tensor</span></span></span><a class="reference internal" href="_modules/NaMAZU/lightning_wingman/torch_gmm.html#GMM.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#NaMAZU.lightning_wingman.GMM.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Predict assignment of x to mixture components by calculating the
likelihood that each component is responsible for each point in x.
If prob is True, returns normalized probability of each class for each point.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>x</strong> (<em>Tensor</em>) – A tensor of shape (n, d) or (n, 1, d).</p></li>
<li><p><strong>probs</strong> (<em>bool</em><em>, </em><em>optional</em>) – True to get normalized probabilities for each class.
Defaults to False.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A tensor of shape (n) if probs is False, else (n, k)</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>Tensor</p>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt class="sig sig-object py" id="NaMAZU.lightning_wingman.GMM.get_kmeans_mu">
<span class="sig-name descname"><span class="pre">get_kmeans_mu</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_centers</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">init_times</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">int</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">50</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">min_delta</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">float</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">0.001</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">→</span> <span class="sig-return-typehint"><span class="pre">torch.Tensor</span></span></span><a class="reference internal" href="_modules/NaMAZU/lightning_wingman/torch_gmm.html#GMM.get_kmeans_mu"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#NaMAZU.lightning_wingman.GMM.get_kmeans_mu" title="Permalink to this definition">¶</a></dt>
<dd><p>Find an initial value for the mean.</p>
<p>Requires a threshold min_delta for the k-means algorithm to stop iterating.
The algorithm is repeated init_times often, after which the best centerpoint is returned.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>x</strong> (<em>Tensor</em>) – A tensor of shape of (n, d) or (n, 1, d).</p></li>
<li><p><strong>init_times</strong> (<em>int</em>) – Number of times to run the k-means algorithm.</p></li>
<li><p><strong>min_delta</strong> (<em>float</em>) – Minimum change in the loss function to stop the k-means algorithm.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A tensor of shape (1, k, d).</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>Tensor</p>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt class="sig sig-object py" id="NaMAZU.lightning_wingman.GMM.predict_proba">
<span class="sig-name descname"><span class="pre">predict_proba</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">torch.Tensor</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">→</span> <span class="sig-return-typehint"><span class="pre">torch.Tensor</span></span></span><a class="reference internal" href="_modules/NaMAZU/lightning_wingman/torch_gmm.html#GMM.predict_proba"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#NaMAZU.lightning_wingman.GMM.predict_proba" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns normalized probabilities of class membership.
:param x: A tensor of shape (n, d) or (n, 1, d).
:type x: Tensor</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>A tensor of shape (n)</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p>Tensor</p>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt class="sig sig-object py" id="NaMAZU.lightning_wingman.GMM.score_samples">
<span class="sig-name descname"><span class="pre">score_samples</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">torch.Tensor</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">→</span> <span class="sig-return-typehint"><span class="pre">torch.Tensor</span></span></span><a class="reference internal" href="_modules/NaMAZU/lightning_wingman/torch_gmm.html#GMM.score_samples"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#NaMAZU.lightning_wingman.GMM.score_samples" title="Permalink to this definition">¶</a></dt>
<dd><p>Return log-likelihood of data under the model with current params.
:param x: A tensor of shape (n, d) or (n, 1, d).
:type x: Tensor</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>A tensor of shape (n).</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p>Tensor</p>
</dd>
</dl>
</dd></dl>
<dl class="py attribute">
<dt class="sig sig-object py" id="NaMAZU.lightning_wingman.GMM.training">
<span class="sig-name descname"><span class="pre">training</span></span><em class="property"><span class="pre">:</span> <span class="pre">bool</span></em><a class="headerlink" href="#NaMAZU.lightning_wingman.GMM.training" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>
</dd></dl>
<dl class="py class">
<dt class="sig sig-object py" id="NaMAZU.lightning_wingman.KNN">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">NaMAZU.lightning_wingman.</span></span><span class="sig-name descname"><span class="pre">KNN</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">n_neighbors</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">training_data</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">torch.Tensor</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">tensor([])</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">distance_measure</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">str</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">'euclidean'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">training_labels</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">torch.Tensor</span><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/NaMAZU/lightning_wingman/torch_knn.html#KNN"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#NaMAZU.lightning_wingman.KNN" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.module.Module</span></code></p>
<dl class="py method">
<dt class="sig sig-object py" id="NaMAZU.lightning_wingman.KNN.fit">
<span class="sig-name descname"><span class="pre">fit</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">training_data</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">torch.Tensor</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">→</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="reference internal" href="_modules/NaMAZU/lightning_wingman/torch_knn.html#KNN.fit"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#NaMAZU.lightning_wingman.KNN.fit" title="Permalink to this definition">¶</a></dt>
<dd><p>Train KNN.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>training_data</strong> (<em>Tensor</em>) – Training data.</p>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt class="sig sig-object py" id="NaMAZU.lightning_wingman.KNN.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">torch.Tensor</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">→</span> <span class="sig-return-typehint"><span class="pre">torch.Tensor</span></span></span><a class="reference internal" href="_modules/NaMAZU/lightning_wingman/torch_knn.html#KNN.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#NaMAZU.lightning_wingman.KNN.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Return the indices of the k nearest neighbors of x.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>x</strong> (<em>Tensor</em>) – input point.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>N by K tensor of k indices of the k nearest neighbors of x.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>Tensor</p>
</dd>
<dt class="field-even">Raises</dt>
<dd class="field-even"><p><strong>ValueError</strong> – Raise if the model has not been fit.</p>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt class="sig sig-object py" id="NaMAZU.lightning_wingman.KNN.get_k_nearest_labels">
<span class="sig-name descname"><span class="pre">get_k_nearest_labels</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">torch.Tensor</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">→</span> <span class="sig-return-typehint"><span class="pre">torch.Tensor</span></span></span><a class="reference internal" href="_modules/NaMAZU/lightning_wingman/torch_knn.html#KNN.get_k_nearest_labels"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#NaMAZU.lightning_wingman.KNN.get_k_nearest_labels" title="Permalink to this definition">¶</a></dt>
<dd><p>Return the k nearest labels of x.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>x</strong> (<em>Tensor</em>) – input point.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>1d tensor of k nearest labels of x.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>Tensor</p>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt class="sig sig-object py" id="NaMAZU.lightning_wingman.KNN.get_k_nearest_neighbors">
<span class="sig-name descname"><span class="pre">get_k_nearest_neighbors</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">torch.Tensor</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">→</span> <span class="sig-return-typehint"><span class="pre">torch.Tensor</span></span></span><a class="reference internal" href="_modules/NaMAZU/lightning_wingman/torch_knn.html#KNN.get_k_nearest_neighbors"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#NaMAZU.lightning_wingman.KNN.get_k_nearest_neighbors" title="Permalink to this definition">¶</a></dt>
<dd><p>Return the k nearest neighbors of x.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>x</strong> (<em>Tensor</em>) – input point.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>1d tensor of k nearest neighbors of x.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>Tensor</p>
</dd>
</dl>
</dd></dl>
<dl class="py attribute">
<dt class="sig sig-object py" id="NaMAZU.lightning_wingman.KNN.num_classes">
<span class="sig-name descname"><span class="pre">num_classes</span></span><em class="property"><span class="pre">:</span> <span class="pre">int</span></em><a class="headerlink" href="#NaMAZU.lightning_wingman.KNN.num_classes" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>
<dl class="py attribute">
<dt class="sig sig-object py" id="NaMAZU.lightning_wingman.KNN.training">
<span class="sig-name descname"><span class="pre">training</span></span><em class="property"><span class="pre">:</span> <span class="pre">bool</span></em><a class="headerlink" href="#NaMAZU.lightning_wingman.KNN.training" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>
</dd></dl>
<dl class="py class">
<dt class="sig sig-object py" id="NaMAZU.lightning_wingman.LitU2Net">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">NaMAZU.lightning_wingman.</span></span><span class="sig-name descname"><span class="pre">LitU2Net</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">in_chans</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">int</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">3</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">out_chans</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">int</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">model_type</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Literal</span><span class="p"><span class="pre">[</span></span><span class="pre">basic</span><span class="p"><span class="pre">,</span> </span><span class="pre">mobile</span><span class="p"><span class="pre">,</span> </span><span class="pre">human</span><span class="p"><span class="pre">,</span> </span><span class="pre">portrait</span><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">'basic'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">train_model</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">bool</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/NaMAZU/lightning_wingman/lit_u2net.html#LitU2Net"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#NaMAZU.lightning_wingman.LitU2Net" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">pytorch_lightning.core.lightning.LightningModule</span></code></p>
<dl class="py method">
<dt class="sig sig-object py" id="NaMAZU.lightning_wingman.LitU2Net.apply_mask">
<span class="sig-name descname"><span class="pre">apply_mask</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">prediction</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">torch.Tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">original_image</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">str</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">→</span> <span class="sig-return-typehint"><span class="pre">PIL.Image.Image</span></span></span><a class="reference internal" href="_modules/NaMAZU/lightning_wingman/lit_u2net.html#LitU2Net.apply_mask"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#NaMAZU.lightning_wingman.LitU2Net.apply_mask" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>
<dl class="py method">
<dt class="sig sig-object py" id="NaMAZU.lightning_wingman.LitU2Net.configure_optimizers">
<span class="sig-name descname"><span class="pre">configure_optimizers</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/NaMAZU/lightning_wingman/lit_u2net.html#LitU2Net.configure_optimizers"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#NaMAZU.lightning_wingman.LitU2Net.configure_optimizers" title="Permalink to this definition">¶</a></dt>
<dd><p>Choose what optimizers and learning-rate schedulers to use in your optimization.
Normally you’d need one. But in the case of GANs or similar you might have multiple.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p><p>Any of these 6 options.</p>
<ul class="simple">
<li><p>Single optimizer.</p></li>
<li><p>List or Tuple - List of optimizers.</p></li>
<li><p>Two lists - The first list has multiple optimizers, the second a list of LR schedulers (or lr_dict).</p></li>
<li><p>Dictionary, with an ‘optimizer’ key, and (optionally) a ‘lr_scheduler’
key whose value is a single LR scheduler or lr_dict.</p></li>
<li><p>Tuple of dictionaries as described, with an optional ‘frequency’ key.</p></li>
<li><p>None - Fit will run without any optimizer.</p></li>
</ul>
</p>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The ‘frequency’ value is an int corresponding to the number of sequential batches
optimized with the specific optimizer. It should be given to none or to all of the optimizers.
There is a difference between passing multiple optimizers in a list,
and passing multiple optimizers in dictionaries with a frequency of 1:
In the former case, all optimizers will operate on the given batch in each optimization step.
In the latter, only one optimizer will operate on the given batch at every step.</p>
<p>The lr_dict is a dictionary which contains the scheduler and its associated configuration.
The default configuration is shown below.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="p">{</span>
    <span class="s1">'scheduler'</span><span class="p">:</span> <span class="n">lr_scheduler</span><span class="p">,</span> <span class="c1"># The LR scheduler instance (required)</span>
    <span class="s1">'interval'</span><span class="p">:</span> <span class="s1">'epoch'</span><span class="p">,</span> <span class="c1"># The unit of the scheduler's step size</span>
    <span class="s1">'frequency'</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span> <span class="c1"># The frequency of the scheduler</span>
    <span class="s1">'reduce_on_plateau'</span><span class="p">:</span> <span class="kc">False</span><span class="p">,</span> <span class="c1"># For ReduceLROnPlateau scheduler</span>
    <span class="s1">'monitor'</span><span class="p">:</span> <span class="s1">'val_loss'</span><span class="p">,</span> <span class="c1"># Metric for ReduceLROnPlateau to monitor</span>
    <span class="s1">'strict'</span><span class="p">:</span> <span class="kc">True</span><span class="p">,</span> <span class="c1"># Whether to crash the training if `monitor` is not found</span>
    <span class="s1">'name'</span><span class="p">:</span> <span class="kc">None</span><span class="p">,</span> <span class="c1"># Custom name for LearningRateMonitor to use</span>
<span class="p">}</span>
</pre></div>
</div>
<p>Only the <code class="docutils literal notranslate"><span class="pre">scheduler</span></code> key is required, the rest will be set to the defaults above.</p>
</div>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># most cases</span>
<span class="k">def</span> <span class="nf">configure_optimizers</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="n">opt</span> <span class="o">=</span> <span class="n">Adam</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">1e-3</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">opt</span>

<span class="c1"># multiple optimizer case (e.g.: GAN)</span>
<span class="k">def</span> <span class="nf">configure_optimizers</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="n">generator_opt</span> <span class="o">=</span> <span class="n">Adam</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model_gen</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span>
    <span class="n">disriminator_opt</span> <span class="o">=</span> <span class="n">Adam</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model_disc</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.02</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">generator_opt</span><span class="p">,</span> <span class="n">disriminator_opt</span>

<span class="c1"># example with learning rate schedulers</span>
<span class="k">def</span> <span class="nf">configure_optimizers</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="n">generator_opt</span> <span class="o">=</span> <span class="n">Adam</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model_gen</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span>
    <span class="n">disriminator_opt</span> <span class="o">=</span> <span class="n">Adam</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model_disc</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.02</span><span class="p">)</span>
    <span class="n">discriminator_sched</span> <span class="o">=</span> <span class="n">CosineAnnealing</span><span class="p">(</span><span class="n">discriminator_opt</span><span class="p">,</span> <span class="n">T_max</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
    <span class="k">return</span> <span class="p">[</span><span class="n">generator_opt</span><span class="p">,</span> <span class="n">disriminator_opt</span><span class="p">],</span> <span class="p">[</span><span class="n">discriminator_sched</span><span class="p">]</span>

<span class="c1"># example with step-based learning rate schedulers</span>
<span class="k">def</span> <span class="nf">configure_optimizers</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="n">gen_opt</span> <span class="o">=</span> <span class="n">Adam</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model_gen</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span>
    <span class="n">dis_opt</span> <span class="o">=</span> <span class="n">Adam</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model_disc</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.02</span><span class="p">)</span>
    <span class="n">gen_sched</span> <span class="o">=</span> <span class="p">{</span><span class="s1">'scheduler'</span><span class="p">:</span> <span class="n">ExponentialLR</span><span class="p">(</span><span class="n">gen_opt</span><span class="p">,</span> <span class="mf">0.99</span><span class="p">),</span>
                 <span class="s1">'interval'</span><span class="p">:</span> <span class="s1">'step'</span><span class="p">}</span>  <span class="c1"># called after each training step</span>
    <span class="n">dis_sched</span> <span class="o">=</span> <span class="n">CosineAnnealing</span><span class="p">(</span><span class="n">discriminator_opt</span><span class="p">,</span> <span class="n">T_max</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span> <span class="c1"># called every epoch</span>
    <span class="k">return</span> <span class="p">[</span><span class="n">gen_opt</span><span class="p">,</span> <span class="n">dis_opt</span><span class="p">],</span> <span class="p">[</span><span class="n">gen_sched</span><span class="p">,</span> <span class="n">dis_sched</span><span class="p">]</span>

<span class="c1"># example with optimizer frequencies</span>
<span class="c1"># see training procedure in `Improved Training of Wasserstein GANs`, Algorithm 1</span>
<span class="c1"># https://arxiv.org/abs/1704.00028</span>
<span class="k">def</span> <span class="nf">configure_optimizers</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="n">gen_opt</span> <span class="o">=</span> <span class="n">Adam</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model_gen</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span>
    <span class="n">dis_opt</span> <span class="o">=</span> <span class="n">Adam</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model_disc</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.02</span><span class="p">)</span>
    <span class="n">n_critic</span> <span class="o">=</span> <span class="mi">5</span>
    <span class="k">return</span> <span class="p">(</span>
        <span class="p">{</span><span class="s1">'optimizer'</span><span class="p">:</span> <span class="n">dis_opt</span><span class="p">,</span> <span class="s1">'frequency'</span><span class="p">:</span> <span class="n">n_critic</span><span class="p">},</span>
        <span class="p">{</span><span class="s1">'optimizer'</span><span class="p">:</span> <span class="n">gen_opt</span><span class="p">,</span> <span class="s1">'frequency'</span><span class="p">:</span> <span class="mi">1</span><span class="p">}</span>
    <span class="p">)</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Some things to know:</p>
<ul>
<li><p>Lightning calls <code class="docutils literal notranslate"><span class="pre">.backward()</span></code> and <code class="docutils literal notranslate"><span class="pre">.step()</span></code> on each optimizer
and learning rate scheduler as needed.</p></li>
<li><p>If you use 16-bit precision (<code class="docutils literal notranslate"><span class="pre">precision=16</span></code>), Lightning will automatically
handle the optimizers for you.</p></li>
<li><p>If you use multiple optimizers, <a class="reference internal" href="#NaMAZU.lightning_wingman.LitU2Net.training_step" title="NaMAZU.lightning_wingman.LitU2Net.training_step"><code class="xref py py-meth docutils literal notranslate"><span class="pre">training_step()</span></code></a> will have an additional
<code class="docutils literal notranslate"><span class="pre">optimizer_idx</span></code> parameter.</p></li>
<li><p>If you use LBFGS Lightning handles the closure function automatically for you.</p></li>
<li><p>If you use multiple optimizers, gradients will be calculated only
for the parameters of current optimizer at each training step.</p></li>
<li><p>If you need to control how often those optimizers step or override the
default <code class="docutils literal notranslate"><span class="pre">.step()</span></code> schedule, override the <code class="xref py py-meth docutils literal notranslate"><span class="pre">optimizer_step()</span></code> hook.</p></li>
<li><p>If you only want to call a learning rate scheduler every <code class="docutils literal notranslate"><span class="pre">x</span></code> step or epoch,
or want to monitor a custom metric, you can specify these in a lr_dict:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="p">{</span>
    <span class="s1">'scheduler'</span><span class="p">:</span> <span class="n">lr_scheduler</span><span class="p">,</span>
    <span class="s1">'interval'</span><span class="p">:</span> <span class="s1">'step'</span><span class="p">,</span>  <span class="c1"># or 'epoch'</span>
    <span class="s1">'monitor'</span><span class="p">:</span> <span class="s1">'val_f1'</span><span class="p">,</span>
    <span class="s1">'frequency'</span><span class="p">:</span> <span class="n">x</span><span class="p">,</span>
<span class="p">}</span>
</pre></div>
</div>
</li>
</ul>
</div>
</dd></dl>
<dl class="py method">
<dt class="sig sig-object py" id="NaMAZU.lightning_wingman.LitU2Net.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">torch.Tensor</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">→</span> <span class="sig-return-typehint"><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">torch.Tensor</span><span class="p"><span class="pre">,</span> </span><span class="p"><span class="pre">...</span></span><span class="p"><span class="pre">]</span></span></span></span><a class="reference internal" href="_modules/NaMAZU/lightning_wingman/lit_u2net.html#LitU2Net.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#NaMAZU.lightning_wingman.LitU2Net.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Same as <code class="xref py py-meth docutils literal notranslate"><span class="pre">torch.nn.Module.forward()</span></code>, however in Lightning you want this to define
the operations you want to use for prediction (i.e.: on a server or as a feature extractor).</p>
<p>Normally you’d call <code class="docutils literal notranslate"><span class="pre">self()</span></code> from your <a class="reference internal" href="#NaMAZU.lightning_wingman.LitU2Net.training_step" title="NaMAZU.lightning_wingman.LitU2Net.training_step"><code class="xref py py-meth docutils literal notranslate"><span class="pre">training_step()</span></code></a> method.
This makes it easy to write a complex system for training with the outputs
you’d want in a prediction setting.</p>
<p>You may also find the <code class="xref py py-func docutils literal notranslate"><span class="pre">auto_move_data()</span></code> decorator useful
when using the module outside Lightning in a production setting.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>*args</strong> – Whatever you decide to pass into the forward method.</p></li>
<li><p><strong>**kwargs</strong> – Keyword arguments are also possible.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Predicted output</p>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># example if we were using this model as a feature extractor</span>
<span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
    <span class="n">feature_maps</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">convnet</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">feature_maps</span>

<span class="k">def</span> <span class="nf">training_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">):</span>
    <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">batch</span>
    <span class="n">feature_maps</span> <span class="o">=</span> <span class="bp">self</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">logits</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">classifier</span><span class="p">(</span><span class="n">feature_maps</span><span class="p">)</span>

    <span class="c1"># ...</span>
    <span class="k">return</span> <span class="n">loss</span>

<span class="c1"># splitting it this way allows model to be used a feature extractor</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">MyModelAbove</span><span class="p">()</span>

<span class="n">inputs</span> <span class="o">=</span> <span class="n">server</span><span class="o">.</span><span class="n">get_request</span><span class="p">()</span>
<span class="n">results</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
<span class="n">server</span><span class="o">.</span><span class="n">write_results</span><span class="p">(</span><span class="n">results</span><span class="p">)</span>

<span class="c1"># -------------</span>
<span class="c1"># This is in stark contrast to torch.nn.Module where normally you would have this:</span>
<span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">):</span>
    <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">batch</span>
    <span class="n">feature_maps</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">convnet</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">logits</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">classifier</span><span class="p">(</span><span class="n">feature_maps</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">logits</span>
</pre></div>
</div>
</dd></dl>
<dl class="py method">
<dt class="sig sig-object py" id="NaMAZU.lightning_wingman.LitU2Net.predict">
<span class="sig-name descname"><span class="pre">predict</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x_path</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">save</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">bool</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">save_path</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">str</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">''</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">→</span> <span class="sig-return-typehint"><span class="pre">torch.Tensor</span></span></span><a class="reference internal" href="_modules/NaMAZU/lightning_wingman/lit_u2net.html#LitU2Net.predict"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#NaMAZU.lightning_wingman.LitU2Net.predict" title="Permalink to this definition">¶</a></dt>
<dd><p>Use this function with trainer.predict(…). Override if you need to add any processing logic.</p>
</dd></dl>
<dl class="py attribute">
<dt class="sig sig-object py" id="NaMAZU.lightning_wingman.LitU2Net.training">
<span class="sig-name descname"><span class="pre">training</span></span><em class="property"><span class="pre">:</span> <span class="pre">bool</span></em><a class="headerlink" href="#NaMAZU.lightning_wingman.LitU2Net.training" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>
<dl class="py method">
<dt class="sig sig-object py" id="NaMAZU.lightning_wingman.LitU2Net.training_step">
<span class="sig-name descname"><span class="pre">training_step</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">batch</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_idx</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/NaMAZU/lightning_wingman/lit_u2net.html#LitU2Net.training_step"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#NaMAZU.lightning_wingman.LitU2Net.training_step" title="Permalink to this definition">¶</a></dt>
<dd><p>Here you compute and return the training loss and some additional metrics for e.g.
the progress bar or logger.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>batch</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code> | (<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>, …) | [<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>, …]) – The output of your <code class="xref py py-class docutils literal notranslate"><span class="pre">DataLoader</span></code>. A tensor, tuple or list.</p></li>
<li><p><strong>batch_idx</strong> (<em>int</em>) – Integer displaying index of this batch</p></li>
<li><p><strong>optimizer_idx</strong> (<em>int</em>) – When using multiple optimizers, this argument will also be present.</p></li>
<li><p><strong>hiddens</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>) – Passed in if
<a href="#id7"><span class="problematic" id="id8">:paramref:`~pytorch_lightning.trainer.trainer.Trainer.truncated_bptt_steps`</span></a> &gt; 0.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><p>Any of.</p>
<ul class="simple">
<li><p><code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code> - The loss tensor</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">dict</span></code> - A dictionary. Can include any keys, but must include the key <code class="docutils literal notranslate"><span class="pre">'loss'</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">None</span></code> - Training will skip to the next batch</p></li>
</ul>
</p>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Returning <code class="docutils literal notranslate"><span class="pre">None</span></code> is currently not supported for multi-GPU or TPU, or with 16-bit precision enabled.</p>
</div>
<p>In this step you’d normally do the forward pass and calculate the loss for a batch.
You can also do fancier things like multiple forward passes or something model specific.</p>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">training_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">):</span>
    <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">z</span> <span class="o">=</span> <span class="n">batch</span>
    <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">loss</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">loss</span>
</pre></div>
</div>
<p>If you define multiple optimizers, this step will be called with an additional
<code class="docutils literal notranslate"><span class="pre">optimizer_idx</span></code> parameter.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Multiple optimizers (e.g.: GANs)</span>
<span class="k">def</span> <span class="nf">training_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">,</span> <span class="n">optimizer_idx</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">optimizer_idx</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="c1"># do training_step with encoder</span>
    <span class="k">if</span> <span class="n">optimizer_idx</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
        <span class="c1"># do training_step with decoder</span>
</pre></div>
</div>
<p>If you add truncated back propagation through time you will also get an additional
argument with the hidden states of the previous step.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Truncated back-propagation through time</span>
<span class="k">def</span> <span class="nf">training_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">,</span> <span class="n">hiddens</span><span class="p">):</span>
    <span class="c1"># hiddens are the hidden states from the previous truncated backprop step</span>
    <span class="o">...</span>
    <span class="n">out</span><span class="p">,</span> <span class="n">hiddens</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">lstm</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">hiddens</span><span class="p">)</span>
    <span class="o">...</span>
    <span class="k">return</span> <span class="p">{</span><span class="s1">'loss'</span><span class="p">:</span> <span class="n">loss</span><span class="p">,</span> <span class="s1">'hiddens'</span><span class="p">:</span> <span class="n">hiddens</span><span class="p">}</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The loss value shown in the progress bar is smoothed (averaged) over the last values,
so it differs from the actual loss returned in train/validation step.</p>
</div>
</dd></dl>
<dl class="py method">
<dt class="sig sig-object py" id="NaMAZU.lightning_wingman.LitU2Net.validation_step">
<span class="sig-name descname"><span class="pre">validation_step</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">batch</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_idx</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/NaMAZU/lightning_wingman/lit_u2net.html#LitU2Net.validation_step"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#NaMAZU.lightning_wingman.LitU2Net.validation_step" title="Permalink to this definition">¶</a></dt>
<dd><p>Operates on a single batch of data from the validation set.
In this step you’d might generate examples or calculate anything of interest like accuracy.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># the pseudocode for these calls</span>
<span class="n">val_outs</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">val_batch</span> <span class="ow">in</span> <span class="n">val_data</span><span class="p">:</span>
    <span class="n">out</span> <span class="o">=</span> <span class="n">validation_step</span><span class="p">(</span><span class="n">val_batch</span><span class="p">)</span>
    <span class="n">val_outs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
<span class="n">validation_epoch_end</span><span class="p">(</span><span class="n">val_outs</span><span class="p">)</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>batch</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code> | (<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>, …) | [<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>, …]) – The output of your <code class="xref py py-class docutils literal notranslate"><span class="pre">DataLoader</span></code>. A tensor, tuple or list.</p></li>
<li><p><strong>batch_idx</strong> (<em>int</em>) – The index of this batch</p></li>
<li><p><strong>dataloader_idx</strong> (<em>int</em>) – The index of the dataloader that produced this batch
(only if multiple val dataloaders used)</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><p>Any of.</p>
<blockquote>
<div><ul class="simple">
<li><p>Any object or value</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">None</span></code> - Validation will skip to the next batch</p></li>
</ul>
</div></blockquote>
</p>
</dd>
</dl>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># pseudocode of order</span>
<span class="n">val_outs</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">val_batch</span> <span class="ow">in</span> <span class="n">val_data</span><span class="p">:</span>
    <span class="n">out</span> <span class="o">=</span> <span class="n">validation_step</span><span class="p">(</span><span class="n">val_batch</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">defined</span><span class="p">(</span><span class="s1">'validation_step_end'</span><span class="p">):</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">validation_step_end</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
    <span class="n">val_outs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
<span class="n">val_outs</span> <span class="o">=</span> <span class="n">validation_epoch_end</span><span class="p">(</span><span class="n">val_outs</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># if you have one val dataloader:</span>
<span class="k">def</span> <span class="nf">validation_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">)</span>

<span class="c1"># if you have multiple val dataloaders:</span>
<span class="k">def</span> <span class="nf">validation_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">,</span> <span class="n">dataloader_idx</span><span class="p">)</span>
</pre></div>
</div>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># CASE 1: A single validation dataset</span>
<span class="k">def</span> <span class="nf">validation_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">):</span>
    <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">batch</span>

    <span class="c1"># implement your own</span>
    <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">loss</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

    <span class="c1"># log 6 example images</span>
    <span class="c1"># or generated text... or whatever</span>
    <span class="n">sample_imgs</span> <span class="o">=</span> <span class="n">x</span><span class="p">[:</span><span class="mi">6</span><span class="p">]</span>
    <span class="n">grid</span> <span class="o">=</span> <span class="n">torchvision</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">make_grid</span><span class="p">(</span><span class="n">sample_imgs</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">experiment</span><span class="o">.</span><span class="n">add_image</span><span class="p">(</span><span class="s1">'example_images'</span><span class="p">,</span> <span class="n">grid</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>

    <span class="c1"># calculate acc</span>
    <span class="n">labels_hat</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">val_acc</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">y</span> <span class="o">==</span> <span class="n">labels_hat</span><span class="p">)</span><span class="o">.</span><span class="n">item</span><span class="p">()</span> <span class="o">/</span> <span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">y</span><span class="p">)</span> <span class="o">*</span> <span class="mf">1.0</span><span class="p">)</span>

    <span class="c1"># log the outputs!</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">log_dict</span><span class="p">({</span><span class="s1">'val_loss'</span><span class="p">:</span> <span class="n">loss</span><span class="p">,</span> <span class="s1">'val_acc'</span><span class="p">:</span> <span class="n">val_acc</span><span class="p">})</span>
</pre></div>
</div>
<p>If you pass in multiple val dataloaders, <a class="reference internal" href="#NaMAZU.lightning_wingman.LitU2Net.validation_step" title="NaMAZU.lightning_wingman.LitU2Net.validation_step"><code class="xref py py-meth docutils literal notranslate"><span class="pre">validation_step()</span></code></a> will have an additional argument.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># CASE 2: multiple validation dataloaders</span>
<span class="k">def</span> <span class="nf">validation_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">,</span> <span class="n">dataloader_idx</span><span class="p">):</span>
    <span class="c1"># dataloader_idx tells you which dataset this is.</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If you don’t need to validate you don’t need to implement this method.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>When the <a class="reference internal" href="#NaMAZU.lightning_wingman.LitU2Net.validation_step" title="NaMAZU.lightning_wingman.LitU2Net.validation_step"><code class="xref py py-meth docutils literal notranslate"><span class="pre">validation_step()</span></code></a> is called, the model has been put in eval mode
and PyTorch gradients have been disabled. At the end of validation,
the model goes back to training mode and gradients are enabled.</p>
</div>
</dd></dl>
</dd></dl>
<dl class="py class">
<dt class="sig sig-object py" id="NaMAZU.lightning_wingman.LitVideoClf">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">NaMAZU.lightning_wingman.</span></span><span class="sig-name descname"><span class="pre">LitVideoClf</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">use_lstm</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">bool</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">model_config</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">dict</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/NaMAZU/lightning_wingman/lit_video_clf.html#LitVideoClf"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#NaMAZU.lightning_wingman.LitVideoClf" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">pytorch_lightning.core.lightning.LightningModule</span></code></p>
<dl class="py method">
<dt class="sig sig-object py" id="NaMAZU.lightning_wingman.LitVideoClf.configure_optimizers">
<span class="sig-name descname"><span class="pre">configure_optimizers</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">→</span> <span class="sig-return-typehint"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span> </span><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">torch.optim.adam.Adam</span><span class="p"><span class="pre">,</span> </span><span class="pre">torch.optim.lr_scheduler.StepLR</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></span><a class="reference internal" href="_modules/NaMAZU/lightning_wingman/lit_video_clf.html#LitVideoClf.configure_optimizers"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#NaMAZU.lightning_wingman.LitVideoClf.configure_optimizers" title="Permalink to this definition">¶</a></dt>
<dd><p>Choose what optimizers and learning-rate schedulers to use in your optimization.
Normally you’d need one. But in the case of GANs or similar you might have multiple.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p><p>Any of these 6 options.</p>
<ul class="simple">
<li><p>Single optimizer.</p></li>
<li><p>List or Tuple - List of optimizers.</p></li>
<li><p>Two lists - The first list has multiple optimizers, the second a list of LR schedulers (or lr_dict).</p></li>
<li><p>Dictionary, with an ‘optimizer’ key, and (optionally) a ‘lr_scheduler’
key whose value is a single LR scheduler or lr_dict.</p></li>
<li><p>Tuple of dictionaries as described, with an optional ‘frequency’ key.</p></li>
<li><p>None - Fit will run without any optimizer.</p></li>
</ul>
</p>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The ‘frequency’ value is an int corresponding to the number of sequential batches
optimized with the specific optimizer. It should be given to none or to all of the optimizers.
There is a difference between passing multiple optimizers in a list,
and passing multiple optimizers in dictionaries with a frequency of 1:
In the former case, all optimizers will operate on the given batch in each optimization step.
In the latter, only one optimizer will operate on the given batch at every step.</p>
<p>The lr_dict is a dictionary which contains the scheduler and its associated configuration.
The default configuration is shown below.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="p">{</span>
    <span class="s1">'scheduler'</span><span class="p">:</span> <span class="n">lr_scheduler</span><span class="p">,</span> <span class="c1"># The LR scheduler instance (required)</span>
    <span class="s1">'interval'</span><span class="p">:</span> <span class="s1">'epoch'</span><span class="p">,</span> <span class="c1"># The unit of the scheduler's step size</span>
    <span class="s1">'frequency'</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span> <span class="c1"># The frequency of the scheduler</span>
    <span class="s1">'reduce_on_plateau'</span><span class="p">:</span> <span class="kc">False</span><span class="p">,</span> <span class="c1"># For ReduceLROnPlateau scheduler</span>
    <span class="s1">'monitor'</span><span class="p">:</span> <span class="s1">'val_loss'</span><span class="p">,</span> <span class="c1"># Metric for ReduceLROnPlateau to monitor</span>
    <span class="s1">'strict'</span><span class="p">:</span> <span class="kc">True</span><span class="p">,</span> <span class="c1"># Whether to crash the training if `monitor` is not found</span>
    <span class="s1">'name'</span><span class="p">:</span> <span class="kc">None</span><span class="p">,</span> <span class="c1"># Custom name for LearningRateMonitor to use</span>
<span class="p">}</span>
</pre></div>
</div>
<p>Only the <code class="docutils literal notranslate"><span class="pre">scheduler</span></code> key is required, the rest will be set to the defaults above.</p>
</div>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># most cases</span>
<span class="k">def</span> <span class="nf">configure_optimizers</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="n">opt</span> <span class="o">=</span> <span class="n">Adam</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">1e-3</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">opt</span>

<span class="c1"># multiple optimizer case (e.g.: GAN)</span>
<span class="k">def</span> <span class="nf">configure_optimizers</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="n">generator_opt</span> <span class="o">=</span> <span class="n">Adam</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model_gen</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span>
    <span class="n">disriminator_opt</span> <span class="o">=</span> <span class="n">Adam</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model_disc</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.02</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">generator_opt</span><span class="p">,</span> <span class="n">disriminator_opt</span>

<span class="c1"># example with learning rate schedulers</span>
<span class="k">def</span> <span class="nf">configure_optimizers</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="n">generator_opt</span> <span class="o">=</span> <span class="n">Adam</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model_gen</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span>
    <span class="n">disriminator_opt</span> <span class="o">=</span> <span class="n">Adam</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model_disc</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.02</span><span class="p">)</span>
    <span class="n">discriminator_sched</span> <span class="o">=</span> <span class="n">CosineAnnealing</span><span class="p">(</span><span class="n">discriminator_opt</span><span class="p">,</span> <span class="n">T_max</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
    <span class="k">return</span> <span class="p">[</span><span class="n">generator_opt</span><span class="p">,</span> <span class="n">disriminator_opt</span><span class="p">],</span> <span class="p">[</span><span class="n">discriminator_sched</span><span class="p">]</span>

<span class="c1"># example with step-based learning rate schedulers</span>
<span class="k">def</span> <span class="nf">configure_optimizers</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="n">gen_opt</span> <span class="o">=</span> <span class="n">Adam</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model_gen</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span>
    <span class="n">dis_opt</span> <span class="o">=</span> <span class="n">Adam</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model_disc</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.02</span><span class="p">)</span>
    <span class="n">gen_sched</span> <span class="o">=</span> <span class="p">{</span><span class="s1">'scheduler'</span><span class="p">:</span> <span class="n">ExponentialLR</span><span class="p">(</span><span class="n">gen_opt</span><span class="p">,</span> <span class="mf">0.99</span><span class="p">),</span>
                 <span class="s1">'interval'</span><span class="p">:</span> <span class="s1">'step'</span><span class="p">}</span>  <span class="c1"># called after each training step</span>
    <span class="n">dis_sched</span> <span class="o">=</span> <span class="n">CosineAnnealing</span><span class="p">(</span><span class="n">discriminator_opt</span><span class="p">,</span> <span class="n">T_max</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span> <span class="c1"># called every epoch</span>
    <span class="k">return</span> <span class="p">[</span><span class="n">gen_opt</span><span class="p">,</span> <span class="n">dis_opt</span><span class="p">],</span> <span class="p">[</span><span class="n">gen_sched</span><span class="p">,</span> <span class="n">dis_sched</span><span class="p">]</span>

<span class="c1"># example with optimizer frequencies</span>
<span class="c1"># see training procedure in `Improved Training of Wasserstein GANs`, Algorithm 1</span>
<span class="c1"># https://arxiv.org/abs/1704.00028</span>
<span class="k">def</span> <span class="nf">configure_optimizers</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="n">gen_opt</span> <span class="o">=</span> <span class="n">Adam</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model_gen</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span>
    <span class="n">dis_opt</span> <span class="o">=</span> <span class="n">Adam</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model_disc</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.02</span><span class="p">)</span>
    <span class="n">n_critic</span> <span class="o">=</span> <span class="mi">5</span>
    <span class="k">return</span> <span class="p">(</span>
        <span class="p">{</span><span class="s1">'optimizer'</span><span class="p">:</span> <span class="n">dis_opt</span><span class="p">,</span> <span class="s1">'frequency'</span><span class="p">:</span> <span class="n">n_critic</span><span class="p">},</span>
        <span class="p">{</span><span class="s1">'optimizer'</span><span class="p">:</span> <span class="n">gen_opt</span><span class="p">,</span> <span class="s1">'frequency'</span><span class="p">:</span> <span class="mi">1</span><span class="p">}</span>
    <span class="p">)</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Some things to know:</p>
<ul>
<li><p>Lightning calls <code class="docutils literal notranslate"><span class="pre">.backward()</span></code> and <code class="docutils literal notranslate"><span class="pre">.step()</span></code> on each optimizer
and learning rate scheduler as needed.</p></li>
<li><p>If you use 16-bit precision (<code class="docutils literal notranslate"><span class="pre">precision=16</span></code>), Lightning will automatically
handle the optimizers for you.</p></li>
<li><p>If you use multiple optimizers, <a class="reference internal" href="#NaMAZU.lightning_wingman.LitVideoClf.training_step" title="NaMAZU.lightning_wingman.LitVideoClf.training_step"><code class="xref py py-meth docutils literal notranslate"><span class="pre">training_step()</span></code></a> will have an additional
<code class="docutils literal notranslate"><span class="pre">optimizer_idx</span></code> parameter.</p></li>
<li><p>If you use LBFGS Lightning handles the closure function automatically for you.</p></li>
<li><p>If you use multiple optimizers, gradients will be calculated only
for the parameters of current optimizer at each training step.</p></li>
<li><p>If you need to control how often those optimizers step or override the
default <code class="docutils literal notranslate"><span class="pre">.step()</span></code> schedule, override the <code class="xref py py-meth docutils literal notranslate"><span class="pre">optimizer_step()</span></code> hook.</p></li>
<li><p>If you only want to call a learning rate scheduler every <code class="docutils literal notranslate"><span class="pre">x</span></code> step or epoch,
or want to monitor a custom metric, you can specify these in a lr_dict:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="p">{</span>
    <span class="s1">'scheduler'</span><span class="p">:</span> <span class="n">lr_scheduler</span><span class="p">,</span>
    <span class="s1">'interval'</span><span class="p">:</span> <span class="s1">'step'</span><span class="p">,</span>  <span class="c1"># or 'epoch'</span>
    <span class="s1">'monitor'</span><span class="p">:</span> <span class="s1">'val_f1'</span><span class="p">,</span>
    <span class="s1">'frequency'</span><span class="p">:</span> <span class="n">x</span><span class="p">,</span>
<span class="p">}</span>
</pre></div>
</div>
</li>
</ul>
</div>
</dd></dl>
<dl class="py method">
<dt class="sig sig-object py" id="NaMAZU.lightning_wingman.LitVideoClf.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">batch</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">→</span> <span class="sig-return-typehint"><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">torch.Tensor</span><span class="p"><span class="pre">,</span> </span><span class="pre">torch.Tensor</span><span class="p"><span class="pre">]</span></span></span></span><a class="reference internal" href="_modules/NaMAZU/lightning_wingman/lit_video_clf.html#LitVideoClf.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#NaMAZU.lightning_wingman.LitVideoClf.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Same as <code class="xref py py-meth docutils literal notranslate"><span class="pre">torch.nn.Module.forward()</span></code>, however in Lightning you want this to define
the operations you want to use for prediction (i.e.: on a server or as a feature extractor).</p>
<p>Normally you’d call <code class="docutils literal notranslate"><span class="pre">self()</span></code> from your <a class="reference internal" href="#NaMAZU.lightning_wingman.LitVideoClf.training_step" title="NaMAZU.lightning_wingman.LitVideoClf.training_step"><code class="xref py py-meth docutils literal notranslate"><span class="pre">training_step()</span></code></a> method.
This makes it easy to write a complex system for training with the outputs
you’d want in a prediction setting.</p>
<p>You may also find the <code class="xref py py-func docutils literal notranslate"><span class="pre">auto_move_data()</span></code> decorator useful
when using the module outside Lightning in a production setting.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>*args</strong> – Whatever you decide to pass into the forward method.</p></li>
<li><p><strong>**kwargs</strong> – Keyword arguments are also possible.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Predicted output</p>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># example if we were using this model as a feature extractor</span>
<span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
    <span class="n">feature_maps</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">convnet</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">feature_maps</span>

<span class="k">def</span> <span class="nf">training_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">):</span>
    <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">batch</span>
    <span class="n">feature_maps</span> <span class="o">=</span> <span class="bp">self</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">logits</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">classifier</span><span class="p">(</span><span class="n">feature_maps</span><span class="p">)</span>

    <span class="c1"># ...</span>
    <span class="k">return</span> <span class="n">loss</span>

<span class="c1"># splitting it this way allows model to be used a feature extractor</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">MyModelAbove</span><span class="p">()</span>

<span class="n">inputs</span> <span class="o">=</span> <span class="n">server</span><span class="o">.</span><span class="n">get_request</span><span class="p">()</span>
<span class="n">results</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
<span class="n">server</span><span class="o">.</span><span class="n">write_results</span><span class="p">(</span><span class="n">results</span><span class="p">)</span>

<span class="c1"># -------------</span>
<span class="c1"># This is in stark contrast to torch.nn.Module where normally you would have this:</span>
<span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">):</span>
    <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">batch</span>
    <span class="n">feature_maps</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">convnet</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">logits</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">classifier</span><span class="p">(</span><span class="n">feature_maps</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">logits</span>
</pre></div>
</div>
</dd></dl>
<dl class="py method">
<dt class="sig sig-object py" id="NaMAZU.lightning_wingman.LitVideoClf.test_step">
<span class="sig-name descname"><span class="pre">test_step</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">batch</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_nb</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">→</span> <span class="sig-return-typehint"><span class="pre">torch.Tensor</span></span></span><a class="reference internal" href="_modules/NaMAZU/lightning_wingman/lit_video_clf.html#LitVideoClf.test_step"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#NaMAZU.lightning_wingman.LitVideoClf.test_step" title="Permalink to this definition">¶</a></dt>
<dd><p>Operates on a single batch of data from the test set.
In this step you’d normally generate examples or calculate anything of interest
such as accuracy.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># the pseudocode for these calls</span>
<span class="n">test_outs</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">test_batch</span> <span class="ow">in</span> <span class="n">test_data</span><span class="p">:</span>
    <span class="n">out</span> <span class="o">=</span> <span class="n">test_step</span><span class="p">(</span><span class="n">test_batch</span><span class="p">)</span>
    <span class="n">test_outs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
<span class="n">test_epoch_end</span><span class="p">(</span><span class="n">test_outs</span><span class="p">)</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>batch</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code> | (<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>, …) | [<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>, …]) – The output of your <code class="xref py py-class docutils literal notranslate"><span class="pre">DataLoader</span></code>. A tensor, tuple or list.</p></li>
<li><p><strong>batch_idx</strong> (<em>int</em>) – The index of this batch.</p></li>
<li><p><strong>dataloader_idx</strong> (<em>int</em>) – The index of the dataloader that produced this batch
(only if multiple test dataloaders used).</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><p>Any of.</p>
<blockquote>
<div><ul class="simple">
<li><p>Any object or value</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">None</span></code> - Testing will skip to the next batch</p></li>
</ul>
</div></blockquote>
</p>
</dd>
</dl>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># if you have one test dataloader:</span>
<span class="k">def</span> <span class="nf">test_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">)</span>

<span class="c1"># if you have multiple test dataloaders:</span>
<span class="k">def</span> <span class="nf">test_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">,</span> <span class="n">dataloader_idx</span><span class="p">)</span>
</pre></div>
</div>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># CASE 1: A single test dataset</span>
<span class="k">def</span> <span class="nf">test_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">):</span>
    <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">batch</span>

    <span class="c1"># implement your own</span>
    <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">loss</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

    <span class="c1"># log 6 example images</span>
    <span class="c1"># or generated text... or whatever</span>
    <span class="n">sample_imgs</span> <span class="o">=</span> <span class="n">x</span><span class="p">[:</span><span class="mi">6</span><span class="p">]</span>
    <span class="n">grid</span> <span class="o">=</span> <span class="n">torchvision</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">make_grid</span><span class="p">(</span><span class="n">sample_imgs</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">experiment</span><span class="o">.</span><span class="n">add_image</span><span class="p">(</span><span class="s1">'example_images'</span><span class="p">,</span> <span class="n">grid</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>

    <span class="c1"># calculate acc</span>
    <span class="n">labels_hat</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">test_acc</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">y</span> <span class="o">==</span> <span class="n">labels_hat</span><span class="p">)</span><span class="o">.</span><span class="n">item</span><span class="p">()</span> <span class="o">/</span> <span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">y</span><span class="p">)</span> <span class="o">*</span> <span class="mf">1.0</span><span class="p">)</span>

    <span class="c1"># log the outputs!</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">log_dict</span><span class="p">({</span><span class="s1">'test_loss'</span><span class="p">:</span> <span class="n">loss</span><span class="p">,</span> <span class="s1">'test_acc'</span><span class="p">:</span> <span class="n">test_acc</span><span class="p">})</span>
</pre></div>
</div>
<p>If you pass in multiple test dataloaders, <a class="reference internal" href="#NaMAZU.lightning_wingman.LitVideoClf.test_step" title="NaMAZU.lightning_wingman.LitVideoClf.test_step"><code class="xref py py-meth docutils literal notranslate"><span class="pre">test_step()</span></code></a> will have an additional argument.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># CASE 2: multiple test dataloaders</span>
<span class="k">def</span> <span class="nf">test_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">,</span> <span class="n">dataloader_idx</span><span class="p">):</span>
    <span class="c1"># dataloader_idx tells you which dataset this is.</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If you don’t need to test you don’t need to implement this method.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>When the <a class="reference internal" href="#NaMAZU.lightning_wingman.LitVideoClf.test_step" title="NaMAZU.lightning_wingman.LitVideoClf.test_step"><code class="xref py py-meth docutils literal notranslate"><span class="pre">test_step()</span></code></a> is called, the model has been put in eval mode and
PyTorch gradients have been disabled. At the end of the test epoch, the model goes back
to training mode and gradients are enabled.</p>
</div>
</dd></dl>
<dl class="py attribute">
<dt class="sig sig-object py" id="NaMAZU.lightning_wingman.LitVideoClf.training">
<span class="sig-name descname"><span class="pre">training</span></span><em class="property"><span class="pre">:</span> <span class="pre">bool</span></em><a class="headerlink" href="#NaMAZU.lightning_wingman.LitVideoClf.training" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>
<dl class="py method">
<dt class="sig sig-object py" id="NaMAZU.lightning_wingman.LitVideoClf.training_step">
<span class="sig-name descname"><span class="pre">training_step</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">batch</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_nb</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">→</span> <span class="sig-return-typehint"><span class="pre">torch.Tensor</span></span></span><a class="reference internal" href="_modules/NaMAZU/lightning_wingman/lit_video_clf.html#LitVideoClf.training_step"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#NaMAZU.lightning_wingman.LitVideoClf.training_step" title="Permalink to this definition">¶</a></dt>
<dd><p>Here you compute and return the training loss and some additional metrics for e.g.
the progress bar or logger.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>batch</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code> | (<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>, …) | [<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>, …]) – The output of your <code class="xref py py-class docutils literal notranslate"><span class="pre">DataLoader</span></code>. A tensor, tuple or list.</p></li>
<li><p><strong>batch_idx</strong> (<em>int</em>) – Integer displaying index of this batch</p></li>
<li><p><strong>optimizer_idx</strong> (<em>int</em>) – When using multiple optimizers, this argument will also be present.</p></li>
<li><p><strong>hiddens</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>) – Passed in if
<a href="#id9"><span class="problematic" id="id10">:paramref:`~pytorch_lightning.trainer.trainer.Trainer.truncated_bptt_steps`</span></a> &gt; 0.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><p>Any of.</p>
<ul class="simple">
<li><p><code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code> - The loss tensor</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">dict</span></code> - A dictionary. Can include any keys, but must include the key <code class="docutils literal notranslate"><span class="pre">'loss'</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">None</span></code> - Training will skip to the next batch</p></li>
</ul>
</p>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Returning <code class="docutils literal notranslate"><span class="pre">None</span></code> is currently not supported for multi-GPU or TPU, or with 16-bit precision enabled.</p>
</div>
<p>In this step you’d normally do the forward pass and calculate the loss for a batch.
You can also do fancier things like multiple forward passes or something model specific.</p>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">training_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">):</span>
    <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">z</span> <span class="o">=</span> <span class="n">batch</span>
    <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">loss</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">loss</span>
</pre></div>
</div>
<p>If you define multiple optimizers, this step will be called with an additional
<code class="docutils literal notranslate"><span class="pre">optimizer_idx</span></code> parameter.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Multiple optimizers (e.g.: GANs)</span>
<span class="k">def</span> <span class="nf">training_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">,</span> <span class="n">optimizer_idx</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">optimizer_idx</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="c1"># do training_step with encoder</span>
    <span class="k">if</span> <span class="n">optimizer_idx</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
        <span class="c1"># do training_step with decoder</span>
</pre></div>
</div>
<p>If you add truncated back propagation through time you will also get an additional
argument with the hidden states of the previous step.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Truncated back-propagation through time</span>
<span class="k">def</span> <span class="nf">training_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">,</span> <span class="n">hiddens</span><span class="p">):</span>
    <span class="c1"># hiddens are the hidden states from the previous truncated backprop step</span>
    <span class="o">...</span>
    <span class="n">out</span><span class="p">,</span> <span class="n">hiddens</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">lstm</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">hiddens</span><span class="p">)</span>
    <span class="o">...</span>
    <span class="k">return</span> <span class="p">{</span><span class="s1">'loss'</span><span class="p">:</span> <span class="n">loss</span><span class="p">,</span> <span class="s1">'hiddens'</span><span class="p">:</span> <span class="n">hiddens</span><span class="p">}</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The loss value shown in the progress bar is smoothed (averaged) over the last values,
so it differs from the actual loss returned in train/validation step.</p>
</div>
</dd></dl>
<dl class="py method">
<dt class="sig sig-object py" id="NaMAZU.lightning_wingman.LitVideoClf.validation_step">
<span class="sig-name descname"><span class="pre">validation_step</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">batch</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_nb</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">→</span> <span class="sig-return-typehint"><span class="pre">torch.Tensor</span></span></span><a class="reference internal" href="_modules/NaMAZU/lightning_wingman/lit_video_clf.html#LitVideoClf.validation_step"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#NaMAZU.lightning_wingman.LitVideoClf.validation_step" title="Permalink to this definition">¶</a></dt>
<dd><p>Operates on a single batch of data from the validation set.
In this step you’d might generate examples or calculate anything of interest like accuracy.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># the pseudocode for these calls</span>
<span class="n">val_outs</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">val_batch</span> <span class="ow">in</span> <span class="n">val_data</span><span class="p">:</span>
    <span class="n">out</span> <span class="o">=</span> <span class="n">validation_step</span><span class="p">(</span><span class="n">val_batch</span><span class="p">)</span>
    <span class="n">val_outs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
<span class="n">validation_epoch_end</span><span class="p">(</span><span class="n">val_outs</span><span class="p">)</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>batch</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code> | (<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>, …) | [<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>, …]) – The output of your <code class="xref py py-class docutils literal notranslate"><span class="pre">DataLoader</span></code>. A tensor, tuple or list.</p></li>
<li><p><strong>batch_idx</strong> (<em>int</em>) – The index of this batch</p></li>
<li><p><strong>dataloader_idx</strong> (<em>int</em>) – The index of the dataloader that produced this batch
(only if multiple val dataloaders used)</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><p>Any of.</p>
<blockquote>
<div><ul class="simple">
<li><p>Any object or value</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">None</span></code> - Validation will skip to the next batch</p></li>
</ul>
</div></blockquote>
</p>
</dd>
</dl>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># pseudocode of order</span>
<span class="n">val_outs</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">val_batch</span> <span class="ow">in</span> <span class="n">val_data</span><span class="p">:</span>
    <span class="n">out</span> <span class="o">=</span> <span class="n">validation_step</span><span class="p">(</span><span class="n">val_batch</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">defined</span><span class="p">(</span><span class="s1">'validation_step_end'</span><span class="p">):</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">validation_step_end</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
    <span class="n">val_outs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
<span class="n">val_outs</span> <span class="o">=</span> <span class="n">validation_epoch_end</span><span class="p">(</span><span class="n">val_outs</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># if you have one val dataloader:</span>
<span class="k">def</span> <span class="nf">validation_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">)</span>

<span class="c1"># if you have multiple val dataloaders:</span>
<span class="k">def</span> <span class="nf">validation_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">,</span> <span class="n">dataloader_idx</span><span class="p">)</span>
</pre></div>
</div>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># CASE 1: A single validation dataset</span>
<span class="k">def</span> <span class="nf">validation_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">):</span>
    <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">batch</span>

    <span class="c1"># implement your own</span>
    <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">loss</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

    <span class="c1"># log 6 example images</span>
    <span class="c1"># or generated text... or whatever</span>
    <span class="n">sample_imgs</span> <span class="o">=</span> <span class="n">x</span><span class="p">[:</span><span class="mi">6</span><span class="p">]</span>
    <span class="n">grid</span> <span class="o">=</span> <span class="n">torchvision</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">make_grid</span><span class="p">(</span><span class="n">sample_imgs</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">experiment</span><span class="o">.</span><span class="n">add_image</span><span class="p">(</span><span class="s1">'example_images'</span><span class="p">,</span> <span class="n">grid</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>

    <span class="c1"># calculate acc</span>
    <span class="n">labels_hat</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">val_acc</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">y</span> <span class="o">==</span> <span class="n">labels_hat</span><span class="p">)</span><span class="o">.</span><span class="n">item</span><span class="p">()</span> <span class="o">/</span> <span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">y</span><span class="p">)</span> <span class="o">*</span> <span class="mf">1.0</span><span class="p">)</span>

    <span class="c1"># log the outputs!</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">log_dict</span><span class="p">({</span><span class="s1">'val_loss'</span><span class="p">:</span> <span class="n">loss</span><span class="p">,</span> <span class="s1">'val_acc'</span><span class="p">:</span> <span class="n">val_acc</span><span class="p">})</span>
</pre></div>
</div>
<p>If you pass in multiple val dataloaders, <a class="reference internal" href="#NaMAZU.lightning_wingman.LitVideoClf.validation_step" title="NaMAZU.lightning_wingman.LitVideoClf.validation_step"><code class="xref py py-meth docutils literal notranslate"><span class="pre">validation_step()</span></code></a> will have an additional argument.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># CASE 2: multiple validation dataloaders</span>
<span class="k">def</span> <span class="nf">validation_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">,</span> <span class="n">dataloader_idx</span><span class="p">):</span>
    <span class="c1"># dataloader_idx tells you which dataset this is.</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If you don’t need to validate you don’t need to implement this method.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>When the <a class="reference internal" href="#NaMAZU.lightning_wingman.LitVideoClf.validation_step" title="NaMAZU.lightning_wingman.LitVideoClf.validation_step"><code class="xref py py-meth docutils literal notranslate"><span class="pre">validation_step()</span></code></a> is called, the model has been put in eval mode
and PyTorch gradients have been disabled. At the end of validation,
the model goes back to training mode and gradients are enabled.</p>
</div>
</dd></dl>
</dd></dl>
<dl class="py class">
<dt class="sig sig-object py" id="NaMAZU.lightning_wingman.MultiModalNet">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">NaMAZU.lightning_wingman.</span></span><span class="sig-name descname"><span class="pre">MultiModalNet</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">modality_dimensions</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">modality_encoders</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">torch.nn.modules.module.Module</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">latent_dim</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">int</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">512</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">direct_fusion</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">bool</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_classes</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">int</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">is_regression</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">bool</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">train_encoders</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">bool</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">use_modality_dropout</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">bool</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">md_prob</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">float</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">0.5</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">use_stochastic_fusion</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">bool</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/NaMAZU/lightning_wingman/lit_multimodal.html#MultiModalNet"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#NaMAZU.lightning_wingman.MultiModalNet" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">pytorch_lightning.core.lightning.LightningModule</span></code></p>
<dl class="py method">
<dt class="sig sig-object py" id="NaMAZU.lightning_wingman.MultiModalNet.configure_optimizers">
<span class="sig-name descname"><span class="pre">configure_optimizers</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">→</span> <span class="sig-return-typehint"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span> </span><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">torch.optim.adam.Adam</span><span class="p"><span class="pre">,</span> </span><span class="pre">torch.optim.lr_scheduler.StepLR</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></span><a class="reference internal" href="_modules/NaMAZU/lightning_wingman/lit_multimodal.html#MultiModalNet.configure_optimizers"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#NaMAZU.lightning_wingman.MultiModalNet.configure_optimizers" title="Permalink to this definition">¶</a></dt>
<dd><p>Choose what optimizers and learning-rate schedulers to use in your optimization.
Normally you’d need one. But in the case of GANs or similar you might have multiple.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p><p>Any of these 6 options.</p>
<ul class="simple">
<li><p>Single optimizer.</p></li>
<li><p>List or Tuple - List of optimizers.</p></li>
<li><p>Two lists - The first list has multiple optimizers, the second a list of LR schedulers (or lr_dict).</p></li>
<li><p>Dictionary, with an ‘optimizer’ key, and (optionally) a ‘lr_scheduler’
key whose value is a single LR scheduler or lr_dict.</p></li>
<li><p>Tuple of dictionaries as described, with an optional ‘frequency’ key.</p></li>
<li><p>None - Fit will run without any optimizer.</p></li>
</ul>
</p>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The ‘frequency’ value is an int corresponding to the number of sequential batches
optimized with the specific optimizer. It should be given to none or to all of the optimizers.
There is a difference between passing multiple optimizers in a list,
and passing multiple optimizers in dictionaries with a frequency of 1:
In the former case, all optimizers will operate on the given batch in each optimization step.
In the latter, only one optimizer will operate on the given batch at every step.</p>
<p>The lr_dict is a dictionary which contains the scheduler and its associated configuration.
The default configuration is shown below.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="p">{</span>
    <span class="s1">'scheduler'</span><span class="p">:</span> <span class="n">lr_scheduler</span><span class="p">,</span> <span class="c1"># The LR scheduler instance (required)</span>
    <span class="s1">'interval'</span><span class="p">:</span> <span class="s1">'epoch'</span><span class="p">,</span> <span class="c1"># The unit of the scheduler's step size</span>
    <span class="s1">'frequency'</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span> <span class="c1"># The frequency of the scheduler</span>
    <span class="s1">'reduce_on_plateau'</span><span class="p">:</span> <span class="kc">False</span><span class="p">,</span> <span class="c1"># For ReduceLROnPlateau scheduler</span>
    <span class="s1">'monitor'</span><span class="p">:</span> <span class="s1">'val_loss'</span><span class="p">,</span> <span class="c1"># Metric for ReduceLROnPlateau to monitor</span>
    <span class="s1">'strict'</span><span class="p">:</span> <span class="kc">True</span><span class="p">,</span> <span class="c1"># Whether to crash the training if `monitor` is not found</span>
    <span class="s1">'name'</span><span class="p">:</span> <span class="kc">None</span><span class="p">,</span> <span class="c1"># Custom name for LearningRateMonitor to use</span>
<span class="p">}</span>
</pre></div>
</div>
<p>Only the <code class="docutils literal notranslate"><span class="pre">scheduler</span></code> key is required, the rest will be set to the defaults above.</p>
</div>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># most cases</span>
<span class="k">def</span> <span class="nf">configure_optimizers</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="n">opt</span> <span class="o">=</span> <span class="n">Adam</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">1e-3</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">opt</span>

<span class="c1"># multiple optimizer case (e.g.: GAN)</span>
<span class="k">def</span> <span class="nf">configure_optimizers</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="n">generator_opt</span> <span class="o">=</span> <span class="n">Adam</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model_gen</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span>
    <span class="n">disriminator_opt</span> <span class="o">=</span> <span class="n">Adam</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model_disc</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.02</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">generator_opt</span><span class="p">,</span> <span class="n">disriminator_opt</span>

<span class="c1"># example with learning rate schedulers</span>
<span class="k">def</span> <span class="nf">configure_optimizers</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="n">generator_opt</span> <span class="o">=</span> <span class="n">Adam</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model_gen</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span>
    <span class="n">disriminator_opt</span> <span class="o">=</span> <span class="n">Adam</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model_disc</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.02</span><span class="p">)</span>
    <span class="n">discriminator_sched</span> <span class="o">=</span> <span class="n">CosineAnnealing</span><span class="p">(</span><span class="n">discriminator_opt</span><span class="p">,</span> <span class="n">T_max</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
    <span class="k">return</span> <span class="p">[</span><span class="n">generator_opt</span><span class="p">,</span> <span class="n">disriminator_opt</span><span class="p">],</span> <span class="p">[</span><span class="n">discriminator_sched</span><span class="p">]</span>

<span class="c1"># example with step-based learning rate schedulers</span>
<span class="k">def</span> <span class="nf">configure_optimizers</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="n">gen_opt</span> <span class="o">=</span> <span class="n">Adam</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model_gen</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span>
    <span class="n">dis_opt</span> <span class="o">=</span> <span class="n">Adam</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model_disc</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.02</span><span class="p">)</span>
    <span class="n">gen_sched</span> <span class="o">=</span> <span class="p">{</span><span class="s1">'scheduler'</span><span class="p">:</span> <span class="n">ExponentialLR</span><span class="p">(</span><span class="n">gen_opt</span><span class="p">,</span> <span class="mf">0.99</span><span class="p">),</span>
                 <span class="s1">'interval'</span><span class="p">:</span> <span class="s1">'step'</span><span class="p">}</span>  <span class="c1"># called after each training step</span>
    <span class="n">dis_sched</span> <span class="o">=</span> <span class="n">CosineAnnealing</span><span class="p">(</span><span class="n">discriminator_opt</span><span class="p">,</span> <span class="n">T_max</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span> <span class="c1"># called every epoch</span>
    <span class="k">return</span> <span class="p">[</span><span class="n">gen_opt</span><span class="p">,</span> <span class="n">dis_opt</span><span class="p">],</span> <span class="p">[</span><span class="n">gen_sched</span><span class="p">,</span> <span class="n">dis_sched</span><span class="p">]</span>

<span class="c1"># example with optimizer frequencies</span>
<span class="c1"># see training procedure in `Improved Training of Wasserstein GANs`, Algorithm 1</span>
<span class="c1"># https://arxiv.org/abs/1704.00028</span>
<span class="k">def</span> <span class="nf">configure_optimizers</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="n">gen_opt</span> <span class="o">=</span> <span class="n">Adam</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model_gen</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span>
    <span class="n">dis_opt</span> <span class="o">=</span> <span class="n">Adam</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model_disc</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.02</span><span class="p">)</span>
    <span class="n">n_critic</span> <span class="o">=</span> <span class="mi">5</span>
    <span class="k">return</span> <span class="p">(</span>
        <span class="p">{</span><span class="s1">'optimizer'</span><span class="p">:</span> <span class="n">dis_opt</span><span class="p">,</span> <span class="s1">'frequency'</span><span class="p">:</span> <span class="n">n_critic</span><span class="p">},</span>
        <span class="p">{</span><span class="s1">'optimizer'</span><span class="p">:</span> <span class="n">gen_opt</span><span class="p">,</span> <span class="s1">'frequency'</span><span class="p">:</span> <span class="mi">1</span><span class="p">}</span>
    <span class="p">)</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Some things to know:</p>
<ul>
<li><p>Lightning calls <code class="docutils literal notranslate"><span class="pre">.backward()</span></code> and <code class="docutils literal notranslate"><span class="pre">.step()</span></code> on each optimizer
and learning rate scheduler as needed.</p></li>
<li><p>If you use 16-bit precision (<code class="docutils literal notranslate"><span class="pre">precision=16</span></code>), Lightning will automatically
handle the optimizers for you.</p></li>
<li><p>If you use multiple optimizers, <a class="reference internal" href="#NaMAZU.lightning_wingman.MultiModalNet.training_step" title="NaMAZU.lightning_wingman.MultiModalNet.training_step"><code class="xref py py-meth docutils literal notranslate"><span class="pre">training_step()</span></code></a> will have an additional
<code class="docutils literal notranslate"><span class="pre">optimizer_idx</span></code> parameter.</p></li>
<li><p>If you use LBFGS Lightning handles the closure function automatically for you.</p></li>
<li><p>If you use multiple optimizers, gradients will be calculated only
for the parameters of current optimizer at each training step.</p></li>
<li><p>If you need to control how often those optimizers step or override the
default <code class="docutils literal notranslate"><span class="pre">.step()</span></code> schedule, override the <code class="xref py py-meth docutils literal notranslate"><span class="pre">optimizer_step()</span></code> hook.</p></li>
<li><p>If you only want to call a learning rate scheduler every <code class="docutils literal notranslate"><span class="pre">x</span></code> step or epoch,
or want to monitor a custom metric, you can specify these in a lr_dict:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="p">{</span>
    <span class="s1">'scheduler'</span><span class="p">:</span> <span class="n">lr_scheduler</span><span class="p">,</span>
    <span class="s1">'interval'</span><span class="p">:</span> <span class="s1">'step'</span><span class="p">,</span>  <span class="c1"># or 'epoch'</span>
    <span class="s1">'monitor'</span><span class="p">:</span> <span class="s1">'val_f1'</span><span class="p">,</span>
    <span class="s1">'frequency'</span><span class="p">:</span> <span class="n">x</span><span class="p">,</span>
<span class="p">}</span>
</pre></div>
</div>
</li>
</ul>
</div>
</dd></dl>
<dl class="py method">
<dt class="sig sig-object py" id="NaMAZU.lightning_wingman.MultiModalNet.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x_list</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">torch.Tensor</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">ma</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">prior</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">→</span> <span class="sig-return-typehint"><span class="pre">torch.Tensor</span></span></span><a class="reference internal" href="_modules/NaMAZU/lightning_wingman/lit_multimodal.html#MultiModalNet.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#NaMAZU.lightning_wingman.MultiModalNet.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Same as <code class="xref py py-meth docutils literal notranslate"><span class="pre">torch.nn.Module.forward()</span></code>, however in Lightning you want this to define
the operations you want to use for prediction (i.e.: on a server or as a feature extractor).</p>
<p>Normally you’d call <code class="docutils literal notranslate"><span class="pre">self()</span></code> from your <a class="reference internal" href="#NaMAZU.lightning_wingman.MultiModalNet.training_step" title="NaMAZU.lightning_wingman.MultiModalNet.training_step"><code class="xref py py-meth docutils literal notranslate"><span class="pre">training_step()</span></code></a> method.
This makes it easy to write a complex system for training with the outputs
you’d want in a prediction setting.</p>
<p>You may also find the <code class="xref py py-func docutils literal notranslate"><span class="pre">auto_move_data()</span></code> decorator useful
when using the module outside Lightning in a production setting.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>*args</strong> – Whatever you decide to pass into the forward method.</p></li>
<li><p><strong>**kwargs</strong> – Keyword arguments are also possible.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Predicted output</p>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># example if we were using this model as a feature extractor</span>
<span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
    <span class="n">feature_maps</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">convnet</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">feature_maps</span>

<span class="k">def</span> <span class="nf">training_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">):</span>
    <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">batch</span>
    <span class="n">feature_maps</span> <span class="o">=</span> <span class="bp">self</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">logits</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">classifier</span><span class="p">(</span><span class="n">feature_maps</span><span class="p">)</span>

    <span class="c1"># ...</span>
    <span class="k">return</span> <span class="n">loss</span>

<span class="c1"># splitting it this way allows model to be used a feature extractor</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">MyModelAbove</span><span class="p">()</span>

<span class="n">inputs</span> <span class="o">=</span> <span class="n">server</span><span class="o">.</span><span class="n">get_request</span><span class="p">()</span>
<span class="n">results</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
<span class="n">server</span><span class="o">.</span><span class="n">write_results</span><span class="p">(</span><span class="n">results</span><span class="p">)</span>

<span class="c1"># -------------</span>
<span class="c1"># This is in stark contrast to torch.nn.Module where normally you would have this:</span>
<span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">):</span>
    <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">batch</span>
    <span class="n">feature_maps</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">convnet</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">logits</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">classifier</span><span class="p">(</span><span class="n">feature_maps</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">logits</span>
</pre></div>
</div>
</dd></dl>
<dl class="py method">
<dt class="sig sig-object py" id="NaMAZU.lightning_wingman.MultiModalNet.modality_dropout">
<span class="sig-name descname"><span class="pre">modality_dropout</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">batch_size</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_modalities</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">int</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">→</span> <span class="sig-return-typehint"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">torch.Tensor</span><span class="p"><span class="pre">]</span></span></span></span><a class="reference internal" href="_modules/NaMAZU/lightning_wingman/lit_multimodal.html#MultiModalNet.modality_dropout"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#NaMAZU.lightning_wingman.MultiModalNet.modality_dropout" title="Permalink to this definition">¶</a></dt>
<dd><p>Return a tensor of shape (batch_size, num_modalities) whose values are either 0 or 1.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>batch_size</strong> (<em>int</em>) – The batch size.</p></li>
<li><p><strong>num_modalities</strong> (<em>int</em>) – Number of modalities.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Dropped modalities or None if the dropout is disabled.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>Union[Tensor, None]</p>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt class="sig sig-object py" id="NaMAZU.lightning_wingman.MultiModalNet.stochstic_fusion">
<span class="sig-name descname"><span class="pre">stochstic_fusion</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">batch_size</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">int</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">→</span> <span class="sig-return-typehint"><span class="pre">torch.Tensor</span></span></span><a class="reference internal" href="_modules/NaMAZU/lightning_wingman/lit_multimodal.html#MultiModalNet.stochstic_fusion"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#NaMAZU.lightning_wingman.MultiModalNet.stochstic_fusion" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>
<dl class="py attribute">
<dt class="sig sig-object py" id="NaMAZU.lightning_wingman.MultiModalNet.training">
<span class="sig-name descname"><span class="pre">training</span></span><em class="property"><span class="pre">:</span> <span class="pre">bool</span></em><a class="headerlink" href="#NaMAZU.lightning_wingman.MultiModalNet.training" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>
<dl class="py method">
<dt class="sig sig-object py" id="NaMAZU.lightning_wingman.MultiModalNet.training_step">
<span class="sig-name descname"><span class="pre">training_step</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">batch</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/NaMAZU/lightning_wingman/lit_multimodal.html#MultiModalNet.training_step"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#NaMAZU.lightning_wingman.MultiModalNet.training_step" title="Permalink to this definition">¶</a></dt>
<dd><p>Here you compute and return the training loss and some additional metrics for e.g.
the progress bar or logger.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>batch</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code> | (<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>, …) | [<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>, …]) – The output of your <code class="xref py py-class docutils literal notranslate"><span class="pre">DataLoader</span></code>. A tensor, tuple or list.</p></li>
<li><p><strong>batch_idx</strong> (<em>int</em>) – Integer displaying index of this batch</p></li>
<li><p><strong>optimizer_idx</strong> (<em>int</em>) – When using multiple optimizers, this argument will also be present.</p></li>
<li><p><strong>hiddens</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>) – Passed in if
<a href="#id11"><span class="problematic" id="id12">:paramref:`~pytorch_lightning.trainer.trainer.Trainer.truncated_bptt_steps`</span></a> &gt; 0.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><p>Any of.</p>
<ul class="simple">
<li><p><code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code> - The loss tensor</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">dict</span></code> - A dictionary. Can include any keys, but must include the key <code class="docutils literal notranslate"><span class="pre">'loss'</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">None</span></code> - Training will skip to the next batch</p></li>
</ul>
</p>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Returning <code class="docutils literal notranslate"><span class="pre">None</span></code> is currently not supported for multi-GPU or TPU, or with 16-bit precision enabled.</p>
</div>
<p>In this step you’d normally do the forward pass and calculate the loss for a batch.
You can also do fancier things like multiple forward passes or something model specific.</p>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">training_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">):</span>
    <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">z</span> <span class="o">=</span> <span class="n">batch</span>
    <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">loss</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">loss</span>
</pre></div>
</div>
<p>If you define multiple optimizers, this step will be called with an additional
<code class="docutils literal notranslate"><span class="pre">optimizer_idx</span></code> parameter.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Multiple optimizers (e.g.: GANs)</span>
<span class="k">def</span> <span class="nf">training_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">,</span> <span class="n">optimizer_idx</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">optimizer_idx</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="c1"># do training_step with encoder</span>
    <span class="k">if</span> <span class="n">optimizer_idx</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
        <span class="c1"># do training_step with decoder</span>
</pre></div>
</div>
<p>If you add truncated back propagation through time you will also get an additional
argument with the hidden states of the previous step.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Truncated back-propagation through time</span>
<span class="k">def</span> <span class="nf">training_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">,</span> <span class="n">hiddens</span><span class="p">):</span>
    <span class="c1"># hiddens are the hidden states from the previous truncated backprop step</span>
    <span class="o">...</span>
    <span class="n">out</span><span class="p">,</span> <span class="n">hiddens</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">lstm</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">hiddens</span><span class="p">)</span>
    <span class="o">...</span>
    <span class="k">return</span> <span class="p">{</span><span class="s1">'loss'</span><span class="p">:</span> <span class="n">loss</span><span class="p">,</span> <span class="s1">'hiddens'</span><span class="p">:</span> <span class="n">hiddens</span><span class="p">}</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The loss value shown in the progress bar is smoothed (averaged) over the last values,
so it differs from the actual loss returned in train/validation step.</p>
</div>
</dd></dl>
</dd></dl>
<dl class="py class">
<dt class="sig sig-object py" id="NaMAZU.lightning_wingman.NBC">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">NaMAZU.lightning_wingman.</span></span><span class="sig-name descname"><span class="pre">NBC</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">offset</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">int</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">training_data</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">torch.Tensor</span><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">training_labels</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">torch.Tensor</span><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">is_categorical</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">bool</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/NaMAZU/lightning_wingman/torch_nbc.html#NBC"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#NaMAZU.lightning_wingman.NBC" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.module.Module</span></code></p>
<p>Naive Bayes Classifier.</p>
<p>Initializer takes optional training data for auto training.</p>
<dl class="py attribute">
<dt class="sig sig-object py" id="NaMAZU.lightning_wingman.NBC.offset">
<span class="sig-name descname"><span class="pre">offset</span></span><a class="headerlink" href="#NaMAZU.lightning_wingman.NBC.offset" title="Permalink to this definition">¶</a></dt>
<dd><p>An integer to increment the conditional probabilities
in order to smooth probabilities to avoid that a posterior
probability be 0.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>int</p>
</dd>
</dl>
</dd></dl>
<dl class="py attribute">
<dt class="sig sig-object py" id="NaMAZU.lightning_wingman.NBC.is_categorical">
<span class="sig-name descname"><span class="pre">is_categorical</span></span><a class="headerlink" href="#NaMAZU.lightning_wingman.NBC.is_categorical" title="Permalink to this definition">¶</a></dt>
<dd><p>A list of boolean for indicating if a feature is categorical or numerical.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>List[bool]</p>
</dd>
</dl>
</dd></dl>
<dl class="py attribute">
<dt class="sig sig-object py" id="NaMAZU.lightning_wingman.NBC.nb_features">
<span class="sig-name descname"><span class="pre">nb_features</span></span><a class="headerlink" href="#NaMAZU.lightning_wingman.NBC.nb_features" title="Permalink to this definition">¶</a></dt>
<dd><p>An integer for the numbers of feature of the data.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>int</p>
</dd>
</dl>
</dd></dl>
<dl class="py attribute">
<dt class="sig sig-object py" id="NaMAZU.lightning_wingman.NBC.nb_class">
<span class="sig-name descname"><span class="pre">nb_class</span></span><a class="headerlink" href="#NaMAZU.lightning_wingman.NBC.nb_class" title="Permalink to this definition">¶</a></dt>
<dd><p>An integer for the number of classes in the labels.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>int</p>
</dd>
</dl>
</dd></dl>
<dl class="py attribute">
<dt class="sig sig-object py" id="NaMAZU.lightning_wingman.NBC.class_probs">
<span class="sig-name descname"><span class="pre">class_probs</span></span><a class="headerlink" href="#NaMAZU.lightning_wingman.NBC.class_probs" title="Permalink to this definition">¶</a></dt>
<dd><p>A torch tensor for the proportion of each class.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>Tensor</p>
</dd>
</dl>
</dd></dl>
<dl class="py attribute">
<dt class="sig sig-object py" id="NaMAZU.lightning_wingman.NBC.cond_probs">
<span class="sig-name descname"><span class="pre">cond_probs</span></span><a class="headerlink" href="#NaMAZU.lightning_wingman.NBC.cond_probs" title="Permalink to this definition">¶</a></dt>
<dd><p>A torch tensor for the conditional probability of
having a given value on a certain feature in the population
of each class.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>Tensor</p>
</dd>
</dl>
</dd></dl>
<dl class="py attribute">
<dt class="sig sig-object py" id="NaMAZU.lightning_wingman.NBC.pi">
<span class="sig-name descname"><span class="pre">pi</span></span><a class="headerlink" href="#NaMAZU.lightning_wingman.NBC.pi" title="Permalink to this definition">¶</a></dt>
<dd><p>A torch tensor for the value of pi.</p>
</dd></dl>
<dl class="py method">
<dt class="sig sig-object py" id="NaMAZU.lightning_wingman.NBC.fit">
<span class="sig-name descname"><span class="pre">fit</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">torch.Tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">torch.Tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">is_categorical</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">bool</span><span class="p"><span class="pre">]</span></span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/NaMAZU/lightning_wingman/torch_nbc.html#NBC.fit"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#NaMAZU.lightning_wingman.NBC.fit" title="Permalink to this definition">¶</a></dt>
<dd><p>Fits the model given data and labels as input</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>X</strong> (<em>Tensor</em>) – A torch tensor containing a batch of data.</p></li>
<li><p><strong>y</strong> (<em>Tensor</em>) – A torch tensor containing a batch of labels.</p></li>
<li><p><strong>is_categorical</strong> (<em>Tensor</em>) – A list of boolean for indicating if a feature is categorical or numerical.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>0 if the model is fitted</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>int</p>
</dd>
<dt class="field-even">Raises</dt>
<dd class="field-even"><p><strong>ValueError</strong> – If the number of features is different from the number of features in X.</p>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt class="sig sig-object py" id="NaMAZU.lightning_wingman.NBC.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/NaMAZU/lightning_wingman/torch_nbc.html#NBC.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#NaMAZU.lightning_wingman.NBC.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Predicts labels given an input</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>X</strong> – A torch tensor containing a batch of data.</p>
</dd>
</dl>
</dd></dl>
<dl class="py attribute">
<dt class="sig sig-object py" id="id0">
<span class="sig-name descname"><span class="pre">offset</span></span><em class="property"><span class="pre">:</span> <span class="pre">int</span></em><a class="headerlink" href="#id0" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>
<dl class="py attribute">
<dt class="sig sig-object py" id="NaMAZU.lightning_wingman.NBC.training">
<span class="sig-name descname"><span class="pre">training</span></span><em class="property"><span class="pre">:</span> <span class="pre">bool</span></em><a class="headerlink" href="#NaMAZU.lightning_wingman.NBC.training" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>
</dd></dl>
<dl class="py class">
<dt class="sig sig-object py" id="NaMAZU.lightning_wingman.PredictionAssistant">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">NaMAZU.lightning_wingman.</span></span><span class="sig-name descname"><span class="pre">PredictionAssistant</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">models</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span> </span><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">pytorch_lightning.core.lightning.LightningModule</span><span class="p"><span class="pre">,</span> </span><span class="pre">str</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/NaMAZU/lightning_wingman/inference_helper.html#PredictionAssistant"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#NaMAZU.lightning_wingman.PredictionAssistant" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">abc.ABC</span></code></p>
<dl class="py method">
<dt class="sig sig-object py" id="NaMAZU.lightning_wingman.PredictionAssistant.predict">
<em class="property"><span class="pre">abstract</span> </em><span class="sig-name descname"><span class="pre">predict</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">query</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><a class="reference internal" href="#NaMAZU.lightning_wingman.instance.Query" title="NaMAZU.lightning_wingman.instance.Query"><span class="pre">NaMAZU.lightning_wingman.instance.Query</span></a></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">→</span> <span class="sig-return-typehint"><a class="reference internal" href="#NaMAZU.lightning_wingman.instance.Result" title="NaMAZU.lightning_wingman.instance.Result"><span class="pre">NaMAZU.lightning_wingman.instance.Result</span></a></span></span><a class="reference internal" href="_modules/NaMAZU/lightning_wingman/inference_helper.html#PredictionAssistant.predict"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#NaMAZU.lightning_wingman.PredictionAssistant.predict" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>
</dd></dl>
<dl class="py function">
<dt class="sig sig-object py" id="NaMAZU.lightning_wingman.self_supervised_training">
<span class="sig-prename descclassname"><span class="pre">NaMAZU.lightning_wingman.</span></span><span class="sig-name descname"><span class="pre">self_supervised_training</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">model_choice</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">image_dirs</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_size</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_threads</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">int</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">6</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_iterations</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">int</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">10000</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">save_dir</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">str</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">''</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">device</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">str</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">'cuda'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">simsiam</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">bool</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">→</span> <span class="sig-return-typehint"><span class="pre">torch.nn.modules.module.Module</span></span></span><a class="reference internal" href="_modules/NaMAZU/lightning_wingman/pretrainer.html#self_supervised_training"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#NaMAZU.lightning_wingman.self_supervised_training" title="Permalink to this definition">¶</a></dt>
<dd><p>Run self supervised training on given model with images given by image_dirs.</p>
<p>If len(image_dirs) is 1, then the model is trained on the single dataset.
Otherwise, mini-batch consists of images drown n // len(dataset) times from each dataset.
Argument simsiam is used to indicate whether to do SimSiam training or not.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>model_choice</strong> (<em>str</em>) – Model to train currently VGG, ResNet and DenseNet are supported.</p></li>
<li><p><strong>image_dirs</strong> (<em>List</em><em>[</em><em>str</em><em>]</em>) – List of pathes of datasets.</p></li>
<li><p><strong>batch_size</strong> (<em>int</em>) – Batch size.</p></li>
<li><p><strong>num_threads</strong> (<em>int</em>) – Number of cpu threads to use.</p></li>
<li><p><strong>num_iterations</strong> (<em>int</em>) – Number of epochs. Default is 10000.</p></li>
<li><p><strong>save_dir</strong> (<em>str</em><em>, </em><em>optional</em>) – Trained model is saved to the directory if given otherwise returned. Defaults to “”.</p></li>
<li><p><strong>device</strong> (<em>str</em><em>, </em><em>optional</em>) – Device to use. Defaults to “cuda”.</p></li>
<li><p><strong>simsiam</strong> (<em>bool</em><em>, </em><em>optional</em>) – If True, use SimSiam. Defaults to False.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Trained model.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>torch.nn.Module</p>
</dd>
</dl>
</dd></dl>




          </article>
        </div>
      </div>
    </main>
  </div>
  <footer class="md-footer">
    <div class="md-footer-nav">
      <nav class="md-footer-nav__inner md-grid">
          
          
        </a>
        
      </nav>
    </div>
    <div class="md-footer-meta md-typeset">
      <div class="md-footer-meta__inner md-grid">
        <div class="md-footer-copyright">
          <div class="md-footer-copyright__highlight">
              &#169; Copyright 2021, NMZ.
              
          </div>
            Created using
            <a href="http://www.sphinx-doc.org/">Sphinx</a> 4.1.2.
             and
            <a href="https://github.com/bashtage/sphinx-material/">Material for
              Sphinx</a>
        </div>
      </div>
    </div>
  </footer>
  <script src="_static/javascripts/application.js"></script>
  <script>app.initialize({version: "1.0.4", url: {base: ".."}})</script>
  </body>
</html>